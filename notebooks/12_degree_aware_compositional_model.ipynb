{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degree-Aware Compositional Model\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "1. **Does degree stratification improve compositional model fit?**\n",
    "2. **How much of the conditional structure is explained by degree heterogeneity alone?**\n",
    "3. **Can degree-aware priors reduce PMI and improve predictions?**\n",
    "\n",
    "## Approach\n",
    "\n",
    "### Naive Compositional Model\n",
    "$$P(C \\to P) = \\sum_{g} P(C \\to g) \\times P(g \\to P)$$\n",
    "\n",
    "Assumes independence regardless of degrees.\n",
    "\n",
    "### Degree-Aware Compositional Model\n",
    "$$P(C \\to P | \\text{deg}(C)=u, \\text{deg}(P)=v) = \\sum_{g: \\text{deg}(g)=w} P(C \\to g | u, w) \\times P(g \\to P | w, v)$$\n",
    "\n",
    "Stratifies by node degrees before applying compositional assumptions.\n",
    "\n",
    "## Key Hypothesis from Notebook 11\n",
    "\n",
    "Since Hetionet and null networks have identical PMI distributions (p=0.715), the conditional structure is **degree-driven**, not biology-driven. Therefore:\n",
    "\n",
    "✓ Degree-aware models should improve fit in **both** Hetionet and null networks  \n",
    "✓ Improvement should be **similar** in magnitude  \n",
    "✓ Remaining residuals may reflect biological dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd().parent\n",
    "src_dir = repo_dir / 'src'\n",
    "data_dir = repo_dir / 'data'\n",
    "results_dir = repo_dir / 'results' / 'degree_aware_compositionality'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "print(f\"Repository: {repo_dir}\")\n",
    "print(f\"Results: {results_dir}\")\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test metapath: CbGpPW (same as notebook 11)\nmetapath = ['CbG', 'GpPW']\nmetapath_name = 'CbGpPW'\n\n# Permutations to analyze\nPERMUTATION_IDS = [1, 2, 3, 4, 5]  # First 5 null networks\nHETIONET_ID = 0  # Real network\n\n# Continuous degree parameters (replaces binning)\nDEGREE_WINDOW_PCT = 0.2  # Use ±20% of degree as window\nMIN_WINDOW = 2           # Minimum window size (for low degrees)\nMIN_EDGE_COUNT = 10      # Minimum edges for reliable estimate\n\nprint(f\"Testing metapath: {metapath_name}\")\nprint(f\"  Edge 1: {metapath[0]} (Compound → Gene)\")\nprint(f\"  Edge 2: {metapath[1]} (Gene → Pathway)\")\nprint(f\"\\nAnalyzing Hetionet ({HETIONET_ID:03d}) + {len(PERMUTATION_IDS)} permutations\")\nprint(f\"\\nContinuous degree parameters:\")\nprint(f\"  Degree window: ±{DEGREE_WINDOW_PCT*100:.0f}% (min {MIN_WINDOW})\")\nprint(f\"  Minimum edge count: {MIN_EDGE_COUNT}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_edge_matrix(edge_type: str, perm_id: int = 0) -> sp.csr_matrix:\n    \"\"\"Load edge matrix for given edge type and permutation.\"\"\"\n    edge_file = data_dir / 'permutations' / f'{perm_id:03d}.hetmat' / 'edges' / f'{edge_type}.sparse.npz'\n    return sp.load_npz(edge_file)\n\ndef analytical_prior(u: float, v: float, m: float) -> float:\n    \"\"\"Current analytical formula for edge probability.\"\"\"\n    uv = u * v\n    denominator = np.sqrt(uv**2 + (m - u - v + 1)**2)\n    return uv / denominator if denominator > 0 else 0.0\n\ndef compute_degree_window(degree: int, window_pct: float = DEGREE_WINDOW_PCT, \n                          min_window: int = MIN_WINDOW) -> tuple:\n    \"\"\"\n    Compute degree window for continuous degree-aware model.\n    \n    Returns (lower, upper) bounds for degree window.\n    Window size adapts to degree magnitude: larger degrees get larger windows.\n    \"\"\"\n    window_size = max(min_window, int(degree * window_pct))\n    lower = max(1, degree - window_size)  # Degrees start at 1\n    upper = degree + window_size\n    return lower, upper\n\ndef build_degree_pair_index(edge_matrix: sp.csr_matrix) -> dict:\n    \"\"\"\n    Build index of edge counts by degree pairs for efficient lookup.\n    \n    Returns:\n        dict: {(source_deg, target_deg): edge_count}\n    \"\"\"\n    source_degrees = np.array(edge_matrix.sum(axis=1)).flatten()\n    target_degrees = np.array(edge_matrix.sum(axis=0)).flatten()\n    \n    degree_pair_counts = defaultdict(int)\n    \n    for i, j in zip(*edge_matrix.nonzero()):\n        u = int(source_degrees[i])\n        v = int(target_degrees[j])\n        degree_pair_counts[(u, v)] += 1\n    \n    return dict(degree_pair_counts)\n\ndef get_effective_m(u: int, v: int, degree_pair_index: dict, \n                   min_count: int = MIN_EDGE_COUNT) -> float:\n    \"\"\"\n    Compute effective edge count m for degree pair (u, v).\n    \n    Uses adaptive window to include edges with similar degrees.\n    Expands window if insufficient edges found.\n    \"\"\"\n    # Start with degree windows\n    u_lower, u_upper = compute_degree_window(u)\n    v_lower, v_upper = compute_degree_window(v)\n    \n    # Count edges in window\n    edge_count = 0\n    for (deg_u, deg_v), count in degree_pair_index.items():\n        if u_lower <= deg_u <= u_upper and v_lower <= deg_v <= v_upper:\n            edge_count += count\n    \n    # If too few edges, expand window (double the percentage)\n    if edge_count < min_count:\n        u_lower, u_upper = compute_degree_window(u, window_pct=DEGREE_WINDOW_PCT * 2)\n        v_lower, v_upper = compute_degree_window(v, window_pct=DEGREE_WINDOW_PCT * 2)\n        \n        edge_count = 0\n        for (deg_u, deg_v), count in degree_pair_index.items():\n            if u_lower <= deg_u <= u_upper and v_lower <= deg_v <= v_upper:\n                edge_count += count\n    \n    # Still too few? Use global count\n    if edge_count < min_count:\n        edge_count = sum(degree_pair_index.values())\n    \n    return float(edge_count)\n\ndef compute_observed_metapath_frequencies(edge1: sp.csr_matrix, edge2: sp.csr_matrix) -> dict:\n    \"\"\"Compute observed metapath frequencies.\"\"\"\n    # Filter zero-degree nodes\n    compound_degrees = np.array(edge1.sum(axis=1)).flatten()\n    pathway_degrees = np.array(edge2.sum(axis=0)).flatten()\n    \n    compound_nonzero = np.where(compound_degrees > 0)[0]\n    pathway_nonzero = np.where(pathway_degrees > 0)[0]\n    \n    edge1_filt = edge1[compound_nonzero, :]\n    edge2_filt = edge2[:, pathway_nonzero]\n    \n    # Compute metapath matrix\n    metapath_matrix = edge1_filt @ edge2_filt\n    \n    # Compute observed frequencies\n    observed_freq = {}\n    for i, j in zip(*metapath_matrix.nonzero()):\n        compound_genes = set(edge1_filt.getrow(i).nonzero()[1])\n        pathway_genes = set(edge2_filt.getcol(j).nonzero()[0])\n        shared_genes = compound_genes & pathway_genes\n        n_paths = len(shared_genes)\n        n_possible = len(compound_genes)\n        if n_possible > 0:\n            observed_freq[(compound_nonzero[i], pathway_nonzero[j])] = n_paths / n_possible\n    \n    return observed_freq, edge1_filt, edge2_filt, compound_nonzero, pathway_nonzero\n\nprint(\"Helper functions defined (continuous degree version)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Compositional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_naive_compositional_model(edge1: sp.csr_matrix, edge2: sp.csr_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    Compute naive compositional predictions.\n",
    "    P(C→P) = sum_g P(C→g) × P(g→P)\n",
    "    \"\"\"\n",
    "    # Get degrees\n",
    "    compound_degrees = np.array(edge1.sum(axis=1)).flatten()\n",
    "    pathway_degrees = np.array(edge2.sum(axis=0)).flatten()\n",
    "    gene_degrees_1 = np.array(edge1.sum(axis=0)).flatten()\n",
    "    gene_degrees_2 = np.array(edge2.sum(axis=1)).flatten()\n",
    "    \n",
    "    m1 = edge1.nnz\n",
    "    m2 = edge2.nnz\n",
    "    \n",
    "    # Compute edge priors\n",
    "    edge1_priors = {}\n",
    "    for i, j in zip(*edge1.nonzero()):\n",
    "        u, v = compound_degrees[i], gene_degrees_1[j]\n",
    "        if u > 0 and v > 0:\n",
    "            edge1_priors[(i, j)] = analytical_prior(u, v, m1)\n",
    "    \n",
    "    edge2_priors = {}\n",
    "    for i, j in zip(*edge2.nonzero()):\n",
    "        u, v = gene_degrees_2[i], pathway_degrees[j]\n",
    "        if u > 0 and v > 0:\n",
    "            edge2_priors[(i, j)] = analytical_prior(u, v, m2)\n",
    "    \n",
    "    # Compute compositional probabilities\n",
    "    compositional_prob = {}\n",
    "    n_compounds = edge1.shape[0]\n",
    "    n_pathways = edge2.shape[1]\n",
    "    \n",
    "    for i in range(n_compounds):\n",
    "        if compound_degrees[i] == 0:\n",
    "            continue\n",
    "        compound_genes = edge1.getrow(i).nonzero()[1]\n",
    "        \n",
    "        for j in range(n_pathways):\n",
    "            if pathway_degrees[j] == 0:\n",
    "                continue\n",
    "            pathway_genes = edge2.getcol(j).nonzero()[0]\n",
    "            \n",
    "            total_prob = 0.0\n",
    "            for gene in set(compound_genes) & set(pathway_genes):\n",
    "                p1 = edge1_priors.get((i, gene), 0.0)\n",
    "                p2 = edge2_priors.get((gene, j), 0.0)\n",
    "                total_prob += p1 * p2\n",
    "            \n",
    "            if total_prob > 0:\n",
    "                compositional_prob[(i, j)] = total_prob\n",
    "    \n",
    "    return compositional_prob\n",
    "\n",
    "print(\"Naive compositional model function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Degree-Aware Compositional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_degree_aware_compositional_model(edge1: sp.csr_matrix, edge2: sp.csr_matrix, \n                                             return_diagnostics: bool = False) -> dict:\n    \"\"\"\n    Compute continuous degree-aware compositional predictions.\n    \n    Key difference from naive model:\n    - Edge priors use degree-specific normalization\n    - For each edge (u,v), compute effective m based on edges with similar degrees\n    - No binning - uses continuous degree windows\n    \n    Args:\n        edge1: First edge matrix\n        edge2: Second edge matrix\n        return_diagnostics: If True, returns (predictions, diagnostics) tuple\n    \n    Returns:\n        If return_diagnostics=False: dict of predictions\n        If return_diagnostics=True: (dict of predictions, dict of diagnostics)\n    \"\"\"\n    print(\"  Building degree pair indices...\")\n    \n    # Get degrees\n    compound_degrees = np.array(edge1.sum(axis=1)).flatten()\n    pathway_degrees = np.array(edge2.sum(axis=0)).flatten()\n    gene_degrees_1 = np.array(edge1.sum(axis=0)).flatten()\n    gene_degrees_2 = np.array(edge2.sum(axis=1)).flatten()\n    \n    # Build degree pair indices for efficient lookup\n    edge1_degree_index = build_degree_pair_index(edge1)\n    edge2_degree_index = build_degree_pair_index(edge2)\n    \n    print(f\"    Edge1 unique degree pairs: {len(edge1_degree_index)}\")\n    print(f\"    Edge2 unique degree pairs: {len(edge2_degree_index)}\")\n    \n    # Compute degree-aware edge priors\n    print(\"  Computing degree-aware edge priors...\")\n    edge1_priors = {}\n    edge1_effective_m = {}  # Store for diagnostics\n    \n    for i, j in zip(*edge1.nonzero()):\n        u = int(compound_degrees[i])\n        v = int(gene_degrees_1[j])\n        \n        if u > 0 and v > 0:\n            # Compute effective m for this degree pair\n            m_eff = get_effective_m(u, v, edge1_degree_index)\n            edge1_priors[(i, j)] = analytical_prior(u, v, m_eff)\n            edge1_effective_m[(u, v)] = m_eff\n    \n    edge2_priors = {}\n    edge2_effective_m = {}\n    \n    for i, j in zip(*edge2.nonzero()):\n        u = int(gene_degrees_2[i])\n        v = int(pathway_degrees[j])\n        \n        if u > 0 and v > 0:\n            m_eff = get_effective_m(u, v, edge2_degree_index)\n            edge2_priors[(i, j)] = analytical_prior(u, v, m_eff)\n            edge2_effective_m[(u, v)] = m_eff\n    \n    print(f\"    Edge1 priors computed: {len(edge1_priors)}\")\n    print(f\"    Edge2 priors computed: {len(edge2_priors)}\")\n    \n    # Compute compositional probabilities (same as naive)\n    print(\"  Computing compositional probabilities...\")\n    compositional_prob = {}\n    n_compounds = edge1.shape[0]\n    n_pathways = edge2.shape[1]\n    \n    for i in range(n_compounds):\n        if compound_degrees[i] == 0:\n            continue\n        compound_genes = edge1.getrow(i).nonzero()[1]\n        \n        for j in range(n_pathways):\n            if pathway_degrees[j] == 0:\n                continue\n            pathway_genes = edge2.getcol(j).nonzero()[0]\n            \n            total_prob = 0.0\n            for gene in set(compound_genes) & set(pathway_genes):\n                p1 = edge1_priors.get((i, gene), 0.0)\n                p2 = edge2_priors.get((gene, j), 0.0)\n                total_prob += p1 * p2\n            \n            if total_prob > 0:\n                compositional_prob[(i, j)] = total_prob\n    \n    if return_diagnostics:\n        diagnostics = {\n            'edge1_effective_m': edge1_effective_m,\n            'edge2_effective_m': edge2_effective_m\n        }\n        return compositional_prob, diagnostics\n    else:\n        return compositional_prob\n\nprint(\"Continuous degree-aware compositional model function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Hetionet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Analyzing Hetionet...\\n\")\n\n# Load edges\nedge1_het = load_edge_matrix(metapath[0], HETIONET_ID)\nedge2_het = load_edge_matrix(metapath[1], HETIONET_ID)\n\nprint(f\"Edge matrices loaded:\")\nprint(f\"  {metapath[0]}: {edge1_het.shape}, {edge1_het.nnz} edges\")\nprint(f\"  {metapath[1]}: {edge2_het.shape}, {edge2_het.nnz} edges\")\n\n# Compute observed frequencies\nprint(\"\\nComputing observed metapath frequencies...\")\nobserved_freq, _, _, _, _ = compute_observed_metapath_frequencies(edge1_het, edge2_het)\nprint(f\"  Found {len(observed_freq)} metapath pairs\")\n\n# Compute naive compositional predictions\nprint(\"\\nComputing naive compositional model...\")\nnaive_pred = compute_naive_compositional_model(edge1_het, edge2_het)\nprint(f\"  Generated {len(naive_pred)} predictions\")\n\n# Compute degree-aware compositional predictions (with diagnostics)\nprint(\"\\nComputing degree-aware compositional model...\")\ndegree_aware_pred, diagnostics = compute_degree_aware_compositional_model(edge1_het, edge2_het, return_diagnostics=True)\nprint(f\"  Generated {len(degree_aware_pred)} predictions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common pairs for fair comparison\n",
    "common_pairs = set(observed_freq.keys()) & set(naive_pred.keys()) & set(degree_aware_pred.keys())\n",
    "print(f\"Common pairs across all three: {len(common_pairs)}\")\n",
    "\n",
    "# Extract values for common pairs\n",
    "y_true = np.array([observed_freq[pair] for pair in common_pairs])\n",
    "y_naive = np.array([naive_pred[pair] for pair in common_pairs])\n",
    "y_degree_aware = np.array([degree_aware_pred[pair] for pair in common_pairs])\n",
    "\n",
    "# Compute metrics\n",
    "naive_corr = pearsonr(y_true, y_naive)[0]\n",
    "degree_aware_corr = pearsonr(y_true, y_degree_aware)[0]\n",
    "\n",
    "naive_rmse = np.sqrt(mean_squared_error(y_true, y_naive))\n",
    "degree_aware_rmse = np.sqrt(mean_squared_error(y_true, y_degree_aware))\n",
    "\n",
    "naive_mae = mean_absolute_error(y_true, y_naive)\n",
    "degree_aware_mae = mean_absolute_error(y_true, y_degree_aware)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - HETIONET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nNaive Compositional Model:\")\n",
    "print(f\"  Correlation: {naive_corr:.4f}\")\n",
    "print(f\"  RMSE:        {naive_rmse:.4f}\")\n",
    "print(f\"  MAE:         {naive_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nDegree-Aware Compositional Model:\")\n",
    "print(f\"  Correlation: {degree_aware_corr:.4f}\")\n",
    "print(f\"  RMSE:        {degree_aware_rmse:.4f}\")\n",
    "print(f\"  MAE:         {degree_aware_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Δ Correlation: {degree_aware_corr - naive_corr:+.4f} ({(degree_aware_corr - naive_corr) / abs(naive_corr) * 100:+.1f}%)\")\n",
    "print(f\"  Δ RMSE:        {degree_aware_rmse - naive_rmse:+.4f} ({(degree_aware_rmse - naive_rmse) / naive_rmse * 100:+.1f}%)\")\n",
    "print(f\"  Δ MAE:         {degree_aware_mae - naive_mae:+.4f} ({(degree_aware_mae - naive_mae) / naive_mae * 100:+.1f}%)\")\n",
    "\n",
    "if degree_aware_corr > naive_corr:\n",
    "    print(f\"\\n✓ DEGREE-AWARE MODEL IMPROVES FIT\")\n",
    "else:\n",
    "    print(f\"\\n✗ No improvement from degree stratification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PMI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PMI for both models\n",
    "def compute_pmi(observed, predicted):\n",
    "    \"\"\"Compute PMI = log2(observed / predicted)\"\"\"\n",
    "    pmi = []\n",
    "    for obs, pred in zip(observed, predicted):\n",
    "        if obs > 0 and pred > 0:\n",
    "            pmi.append(np.log2(obs / pred))\n",
    "    return np.array(pmi)\n",
    "\n",
    "pmi_naive = compute_pmi(y_true, y_naive)\n",
    "pmi_degree_aware = compute_pmi(y_true, y_degree_aware)\n",
    "\n",
    "print(\"\\nPMI Analysis:\")\n",
    "print(f\"\\nNaive Model:\")\n",
    "print(f\"  Mean PMI:   {pmi_naive.mean():.4f}\")\n",
    "print(f\"  Median PMI: {np.median(pmi_naive):.4f}\")\n",
    "print(f\"  Std PMI:    {pmi_naive.std():.4f}\")\n",
    "\n",
    "print(f\"\\nDegree-Aware Model:\")\n",
    "print(f\"  Mean PMI:   {pmi_degree_aware.mean():.4f}\")\n",
    "print(f\"  Median PMI: {np.median(pmi_degree_aware):.4f}\")\n",
    "print(f\"  Std PMI:    {pmi_degree_aware.std():.4f}\")\n",
    "\n",
    "print(f\"\\nPMI Reduction:\")\n",
    "print(f\"  Δ Mean PMI: {pmi_degree_aware.mean() - pmi_naive.mean():.4f}\")\n",
    "\n",
    "if abs(pmi_degree_aware.mean()) < abs(pmi_naive.mean()):\n",
    "    print(f\"\\n✓ DEGREE-AWARE MODEL REDUCES PMI (closer to 0 = better compositional fit)\")\n",
    "else:\n",
    "    print(f\"\\n→ No PMI reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stratified Analysis by Degree Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get degrees for stratification\ncompound_degrees = np.array(edge1_het.sum(axis=1)).flatten()\npathway_degrees = np.array(edge2_het.sum(axis=0)).flatten()\n\n# Create results dataframe\nresults_df = pd.DataFrame({\n    'compound_idx': [pair[0] for pair in common_pairs],\n    'pathway_idx': [pair[1] for pair in common_pairs],\n    'observed': y_true,\n    'naive_pred': y_naive,\n    'degree_aware_pred': y_degree_aware\n})\n\nresults_df['compound_degree'] = results_df['compound_idx'].map(lambda i: compound_degrees[i])\nresults_df['pathway_degree'] = results_df['pathway_idx'].map(lambda i: pathway_degrees[i])\n\nresults_df['pmi_naive'] = np.log2(results_df['observed'] / results_df['naive_pred'])\nresults_df['pmi_degree_aware'] = np.log2(results_df['observed'] / results_df['degree_aware_pred'])\n\n# Create log-scale bins for visualization (not used in model)\nDEGREE_BINS_VIZ = [0, 2, 4, 8, 16, 32, 64, 128, np.inf]\nDEGREE_LABELS_VIZ = ['1-2', '3-4', '5-8', '9-16', '17-32', '33-64', '65-128', '>128']\n\nresults_df['compound_bin'] = pd.cut(results_df['compound_degree'], bins=DEGREE_BINS_VIZ, labels=DEGREE_LABELS_VIZ)\nresults_df['pathway_bin'] = pd.cut(results_df['pathway_degree'], bins=DEGREE_BINS_VIZ, labels=DEGREE_LABELS_VIZ)\n\nprint(\"\\nPerformance by Compound Degree Bin (for visualization only):\")\nprint(\"=\"*80)\n\nfor bin_label in DEGREE_LABELS_VIZ:\n    subset = results_df[results_df['compound_bin'] == bin_label]\n    if len(subset) > 10:  # Require at least 10 samples for reliable correlation\n        naive_corr_bin = pearsonr(subset['observed'], subset['naive_pred'])[0]\n        da_corr_bin = pearsonr(subset['observed'], subset['degree_aware_pred'])[0]\n        \n        print(f\"\\n{bin_label} (n={len(subset)}):\")\n        print(f\"  Naive correlation:        {naive_corr_bin:.4f}\")\n        print(f\"  Degree-aware correlation: {da_corr_bin:.4f}\")\n        print(f\"  Improvement:              {da_corr_bin - naive_corr_bin:+.4f}\")\n        print(f\"  Mean compound degree:     {subset['compound_degree'].mean():.1f}\")\n        print(f\"  Mean PMI (naive):         {subset['pmi_naive'].mean():.4f}\")\n        print(f\"  Mean PMI (degree-aware):  {subset['pmi_degree_aware'].mean():.4f}\")\n    elif len(subset) > 0:\n        print(f\"\\n{bin_label} (n={len(subset)}): Too few samples for correlation\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 6b. Degree Window Diagnostics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze effective m values used in degree-aware model\nif diagnostics:\n    edge1_m_values = diagnostics['edge1_effective_m']\n    edge2_m_values = diagnostics['edge2_effective_m']\n    \n    print(\"\\nEffective m Diagnostics:\")\n    print(\"=\"*80)\n    \n    # Edge1 statistics\n    edge1_degrees = sorted(set(u for u, v in edge1_m_values.keys()))\n    edge1_m_by_degree = defaultdict(list)\n    for (u, v), m in edge1_m_values.items():\n        edge1_m_by_degree[u].append(m)\n    \n    print(f\"\\nEdge1 ({metapath[0]}) - Compound→Gene:\")\n    print(f\"  Unique degree pairs: {len(edge1_m_values)}\")\n    print(f\"  Effective m range: {min(edge1_m_values.values()):.0f} - {max(edge1_m_values.values()):.0f}\")\n    print(f\"  Mean effective m: {np.mean(list(edge1_m_values.values())):.1f}\")\n    \n    # Sample low degrees\n    print(f\"\\n  Sample effective m for low compound degrees:\")\n    for deg in sorted(edge1_degrees)[:10]:\n        m_vals = edge1_m_by_degree[deg]\n        print(f\"    Degree {deg:2d}: m = {np.mean(m_vals):6.1f} ± {np.std(m_vals):5.1f} (n={len(m_vals)})\")\n    \n    # Edge2 statistics\n    edge2_degrees = sorted(set(u for u, v in edge2_m_values.keys()))\n    edge2_m_by_degree = defaultdict(list)\n    for (u, v), m in edge2_m_values.items():\n        edge2_m_by_degree[u].append(m)\n    \n    print(f\"\\nEdge2 ({metapath[1]}) - Gene→Pathway:\")\n    print(f\"  Unique degree pairs: {len(edge2_m_values)}\")\n    print(f\"  Effective m range: {min(edge2_m_values.values()):.0f} - {max(edge2_m_values.values()):.0f}\")\n    print(f\"  Mean effective m: {np.mean(list(edge2_m_values.values())):.1f}\")\n    \n    # Visualize effective m vs degree\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Edge1\n    ax = axes[0]\n    degrees = []\n    mean_m = []\n    for deg in sorted(edge1_m_by_degree.keys()):\n        degrees.append(deg)\n        mean_m.append(np.mean(edge1_m_by_degree[deg]))\n    \n    ax.scatter(degrees, mean_m, alpha=0.6, s=50, edgecolors='black', linewidths=0.5)\n    ax.set_xlabel('Source Degree (Compound)', fontsize=12)\n    ax.set_ylabel('Mean Effective m', fontsize=12)\n    ax.set_title(f'Effective m vs Degree: {metapath[0]}', fontsize=14, fontweight='bold')\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n    ax.grid(alpha=0.3)\n    ax.axhline(edge1_het.nnz, color='red', linestyle='--', alpha=0.5, label=f'Global m = {edge1_het.nnz}')\n    ax.legend()\n    \n    # Edge2\n    ax = axes[1]\n    degrees = []\n    mean_m = []\n    for deg in sorted(edge2_m_by_degree.keys()):\n        degrees.append(deg)\n        mean_m.append(np.mean(edge2_m_by_degree[deg]))\n    \n    ax.scatter(degrees, mean_m, alpha=0.6, s=50, edgecolors='black', linewidths=0.5, color='green')\n    ax.set_xlabel('Source Degree (Gene)', fontsize=12)\n    ax.set_ylabel('Mean Effective m', fontsize=12)\n    ax.set_title(f'Effective m vs Degree: {metapath[1]}', fontsize=14, fontweight='bold')\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n    ax.grid(alpha=0.3)\n    ax.axhline(edge2_het.nnz, color='red', linestyle='--', alpha=0.5, label=f'Global m = {edge2_het.nnz}')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig(results_dir / f'{metapath_name}_effective_m_diagnostics.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"\\nEffective m diagnostic plot saved\")\n    \n    # Check what fraction use global m (fallback)\n    global_m_edge1 = sum(1 for m in edge1_m_values.values() if m == edge1_het.nnz)\n    global_m_edge2 = sum(1 for m in edge2_m_values.values() if m == edge2_het.nnz)\n    \n    print(f\"\\nFallback to global m:\")\n    print(f\"  Edge1: {global_m_edge1}/{len(edge1_m_values)} ({global_m_edge1/len(edge1_m_values)*100:.1f}%)\")\n    print(f\"  Edge2: {global_m_edge2}/{len(edge2_m_values)} ({global_m_edge2/len(edge2_m_values)*100:.1f}%)\")\n    \nelse:\n    print(\"\\nWarning: Diagnostics not available\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Null Network Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nAnalyzing {len(PERMUTATION_IDS)} null networks...\\n\")\n",
    "\n",
    "null_results = []\n",
    "\n",
    "for perm_id in tqdm(PERMUTATION_IDS, desc=\"Processing permutations\"):\n",
    "    # Load edges\n",
    "    edge1_perm = load_edge_matrix(metapath[0], perm_id)\n",
    "    edge2_perm = load_edge_matrix(metapath[1], perm_id)\n",
    "    \n",
    "    # Compute observed\n",
    "    obs_freq, _, _, _, _ = compute_observed_metapath_frequencies(edge1_perm, edge2_perm)\n",
    "    \n",
    "    # Compute predictions\n",
    "    naive_perm = compute_naive_compositional_model(edge1_perm, edge2_perm)\n",
    "    da_perm = compute_degree_aware_compositional_model(edge1_perm, edge2_perm)\n",
    "    \n",
    "    # Common pairs\n",
    "    common = set(obs_freq.keys()) & set(naive_perm.keys()) & set(da_perm.keys())\n",
    "    \n",
    "    if len(common) > 1:\n",
    "        y_obs = np.array([obs_freq[p] for p in common])\n",
    "        y_naive_p = np.array([naive_perm[p] for p in common])\n",
    "        y_da_p = np.array([da_perm[p] for p in common])\n",
    "        \n",
    "        naive_corr_p = pearsonr(y_obs, y_naive_p)[0]\n",
    "        da_corr_p = pearsonr(y_obs, y_da_p)[0]\n",
    "        \n",
    "        null_results.append({\n",
    "            'perm_id': perm_id,\n",
    "            'naive_corr': naive_corr_p,\n",
    "            'degree_aware_corr': da_corr_p,\n",
    "            'improvement': da_corr_p - naive_corr_p,\n",
    "            'n_pairs': len(common)\n",
    "        })\n",
    "\n",
    "null_df = pd.DataFrame(null_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NULL NETWORK RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nNull Networks (n={len(null_df)}):\")\n",
    "print(f\"  Naive correlation:        {null_df['naive_corr'].mean():.4f} ± {null_df['naive_corr'].std():.4f}\")\n",
    "print(f\"  Degree-aware correlation: {null_df['degree_aware_corr'].mean():.4f} ± {null_df['degree_aware_corr'].std():.4f}\")\n",
    "print(f\"  Mean improvement:         {null_df['improvement'].mean():+.4f} ± {null_df['improvement'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nHetionet vs Null:\")\n",
    "het_improvement = degree_aware_corr - naive_corr\n",
    "null_improvement = null_df['improvement'].mean()\n",
    "print(f\"  Hetionet improvement:     {het_improvement:+.4f}\")\n",
    "print(f\"  Null improvement:         {null_improvement:+.4f}\")\n",
    "print(f\"  Difference:               {het_improvement - null_improvement:+.4f}\")\n",
    "\n",
    "if abs(het_improvement - null_improvement) < 0.01:\n",
    "    print(f\"\\n✓ SIMILAR IMPROVEMENT IN HETIONET AND NULL\")\n",
    "    print(f\"  → Confirms degree structure is main driver of conditional dependencies\")\n",
    "    print(f\"  → Biology adds minimal signal beyond degrees\")\n",
    "else:\n",
    "    print(f\"\\n→ DIFFERENT IMPROVEMENT PATTERNS\")\n",
    "    print(f\"  → May indicate biological signal beyond degree structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comprehensive visualization\nfig, axes = plt.subplots(2, 3, figsize=(20, 13))\n\n# 1. Scatter: Observed vs Naive\nax = axes[0, 0]\nax.scatter(y_true, y_naive, alpha=0.5, s=10, edgecolors='none')\nax.plot([0, 1], [0, 1], 'r--', alpha=0.8, linewidth=2)\nax.set_xlabel('Observed Frequency', fontsize=12)\nax.set_ylabel('Naive Compositional Prediction', fontsize=12)\nax.set_title(f'Naive Model (r={naive_corr:.3f})', fontsize=14, fontweight='bold')\nax.grid(alpha=0.3)\nax.set_xlim(0, 1)\nax.set_ylim(0, max(y_naive.max(), 1))\n\n# 2. Scatter: Observed vs Degree-Aware\nax = axes[0, 1]\nax.scatter(y_true, y_degree_aware, alpha=0.5, s=10, edgecolors='none', color='green')\nax.plot([0, 1], [0, 1], 'r--', alpha=0.8, linewidth=2)\nax.set_xlabel('Observed Frequency', fontsize=12)\nax.set_ylabel('Degree-Aware Prediction', fontsize=12)\nax.set_title(f'Continuous Degree-Aware Model (r={degree_aware_corr:.3f})', fontsize=14, fontweight='bold')\nax.grid(alpha=0.3)\nax.set_xlim(0, 1)\nax.set_ylim(0, max(y_degree_aware.max(), 1))\n\n# 3. PMI distributions\nax = axes[0, 2]\nax.hist(pmi_naive, bins=50, alpha=0.6, label='Naive', edgecolor='black')\nax.hist(pmi_degree_aware, bins=50, alpha=0.6, label='Degree-Aware', edgecolor='black', color='green')\nax.axvline(0, color='red', linestyle='--', linewidth=2, label='Perfect Fit (PMI=0)')\nax.set_xlabel('PMI', fontsize=12)\nax.set_ylabel('Frequency', fontsize=12)\nax.set_title('PMI Distribution', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(alpha=0.3)\n\n# 4. Correlation by compound degree (using visualization bins)\nax = axes[1, 0]\ncorr_by_compound = []\nfor bin_label in DEGREE_LABELS_VIZ:\n    subset = results_df[results_df['compound_bin'] == bin_label]\n    if len(subset) > 10:\n        naive_c = pearsonr(subset['observed'], subset['naive_pred'])[0]\n        da_c = pearsonr(subset['observed'], subset['degree_aware_pred'])[0]\n        corr_by_compound.append({\n            'bin': bin_label, \n            'naive': naive_c, \n            'degree_aware': da_c,\n            'n': len(subset)\n        })\n\nif corr_by_compound:\n    corr_compound_df = pd.DataFrame(corr_by_compound)\n    x = np.arange(len(corr_compound_df))\n    width = 0.35\n    bars1 = ax.bar(x - width/2, corr_compound_df['naive'], width, label='Naive', alpha=0.7)\n    bars2 = ax.bar(x + width/2, corr_compound_df['degree_aware'], width, label='Continuous Degree-Aware', alpha=0.7, color='green')\n    \n    # Add sample size annotations\n    for i, row in corr_compound_df.iterrows():\n        ax.text(i, -0.15, f\"n={row['n']}\", ha='center', va='top', fontsize=8, rotation=0)\n    \n    ax.set_xlabel('Compound Degree Bin', fontsize=12)\n    ax.set_ylabel('Correlation', fontsize=12)\n    ax.set_title('Correlation by Compound Degree', fontsize=14, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(corr_compound_df['bin'], rotation=45, ha='right')\n    ax.legend()\n    ax.grid(alpha=0.3, axis='y')\n    ax.set_ylim(bottom=-0.2)\n\n# 5. Null network improvement distribution\nax = axes[1, 1]\nax.hist(null_df['improvement'], bins=15, alpha=0.7, edgecolor='black', color='blue')\nax.axvline(het_improvement, color='red', linestyle='--', linewidth=3, label=f'Hetionet ({het_improvement:+.3f})')\nax.axvline(null_improvement, color='blue', linestyle='--', linewidth=2, label=f'Null mean ({null_improvement:+.3f})')\nax.set_xlabel('Improvement in Correlation', fontsize=12)\nax.set_ylabel('Frequency', fontsize=12)\nax.set_title('Continuous Degree-Aware Improvement: Hetionet vs Null', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(alpha=0.3)\n\n# 6. Performance vs compound degree (scatter)\nax = axes[1, 2]\n# Sample if too many points\nif len(results_df) > 5000:\n    sample_df = results_df.sample(5000, random_state=42)\nelse:\n    sample_df = results_df\n\n# Color by improvement\nimprovement = sample_df['degree_aware_pred'] - sample_df['naive_pred']\nscatter = ax.scatter(sample_df['compound_degree'], sample_df['pathway_degree'], \n                     c=improvement, cmap='RdYlGn', alpha=0.6, s=10, \n                     vmin=-0.1, vmax=0.1, edgecolors='none')\nax.set_xlabel('Compound Degree', fontsize=12)\nax.set_ylabel('Pathway Degree', fontsize=12)\nax.set_title('Improvement by Degree Pair', fontsize=14, fontweight='bold')\nax.set_xscale('log')\nax.set_yscale('log')\ncbar = plt.colorbar(scatter, ax=ax)\ncbar.set_label('Prediction Improvement', fontsize=10)\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(results_dir / f'{metapath_name}_degree_aware_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nVisualization saved to: {results_dir / f'{metapath_name}_degree_aware_analysis.png'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Hetionet results\n",
    "results_df.to_csv(results_dir / f'{metapath_name}_hetionet_results.csv', index=False)\n",
    "\n",
    "# Save null results\n",
    "null_df.to_csv(results_dir / f'{metapath_name}_null_results.csv', index=False)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'metapath': metapath_name,\n",
    "    'hetionet_naive_corr': naive_corr,\n",
    "    'hetionet_degree_aware_corr': degree_aware_corr,\n",
    "    'hetionet_improvement': het_improvement,\n",
    "    'hetionet_naive_rmse': naive_rmse,\n",
    "    'hetionet_degree_aware_rmse': degree_aware_rmse,\n",
    "    'hetionet_naive_pmi_mean': pmi_naive.mean(),\n",
    "    'hetionet_degree_aware_pmi_mean': pmi_degree_aware.mean(),\n",
    "    'null_naive_corr_mean': null_df['naive_corr'].mean(),\n",
    "    'null_degree_aware_corr_mean': null_df['degree_aware_corr'].mean(),\n",
    "    'null_improvement_mean': null_improvement,\n",
    "    'n_hetionet_pairs': len(common_pairs),\n",
    "    'n_null_networks': len(null_df)\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(results_dir / f'{metapath_name}_summary.csv', index=False)\n",
    "\n",
    "print(f\"\\nResults saved to:\")\n",
    "print(f\"  - {results_dir / f'{metapath_name}_hetionet_results.csv'}\")\n",
    "print(f\"  - {results_dir / f'{metapath_name}_null_results.csv'}\")\n",
    "print(f\"  - {results_dir / f'{metapath_name}_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSIONS: DEGREE-AWARE COMPOSITIONAL MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. MODEL PERFORMANCE:\")\n",
    "print(f\"   Naive compositional:      r = {naive_corr:.4f}\")\n",
    "print(f\"   Degree-aware:             r = {degree_aware_corr:.4f}\")\n",
    "print(f\"   Improvement:              Δr = {het_improvement:+.4f} ({het_improvement / abs(naive_corr) * 100:+.1f}%)\")\n",
    "\n",
    "if het_improvement > 0.01:\n",
    "    print(f\"\\n   ✓ DEGREE-AWARE MODEL SIGNIFICANTLY IMPROVES FIT\")\n",
    "elif het_improvement > 0:\n",
    "    print(f\"\\n   → Modest improvement from degree stratification\")\n",
    "else:\n",
    "    print(f\"\\n   ✗ No improvement or degradation\")\n",
    "\n",
    "print(f\"\\n2. PMI REDUCTION:\")\n",
    "print(f\"   Naive PMI:                {pmi_naive.mean():.4f}\")\n",
    "print(f\"   Degree-aware PMI:         {pmi_degree_aware.mean():.4f}\")\n",
    "print(f\"   Reduction:                {pmi_naive.mean() - pmi_degree_aware.mean():.4f}\")\n",
    "\n",
    "if abs(pmi_degree_aware.mean()) < abs(pmi_naive.mean()):\n",
    "    print(f\"\\n   ✓ PMI closer to 0 → better compositional fit\")\n",
    "\n",
    "print(f\"\\n3. HETIONET VS NULL:\")\n",
    "print(f\"   Hetionet improvement:     {het_improvement:+.4f}\")\n",
    "print(f\"   Null improvement (mean):  {null_improvement:+.4f}\")\n",
    "print(f\"   Difference:               {abs(het_improvement - null_improvement):.4f}\")\n",
    "\n",
    "if abs(het_improvement - null_improvement) < 0.01:\n",
    "    print(f\"\\n   ✓ SIMILAR IMPROVEMENT (difference < 0.01)\")\n",
    "    print(f\"     → Degree structure is primary driver\")\n",
    "    print(f\"     → Biological dependencies are minimal\")\n",
    "else:\n",
    "    print(f\"\\n   → DIFFERENT IMPROVEMENT PATTERNS\")\n",
    "    print(f\"     → Potential biological signal beyond degrees\")\n",
    "\n",
    "print(f\"\\n4. RECOMMENDATIONS:\")\n",
    "\n",
    "if het_improvement > 0.05 and abs(het_improvement - null_improvement) < 0.02:\n",
    "    print(f\"   → USE DEGREE-AWARE COMPOSITIONAL MODELS\")\n",
    "    print(f\"   → Substantial improvement from degree stratification\")\n",
    "    print(f\"   → Apply to both Hetionet and null networks\")\n",
    "    print(f\"   → Stratify by {len(DEGREE_BINS)-1} degree bins: {DEGREE_LABELS}\")\n",
    "elif het_improvement > 0:\n",
    "    print(f\"   → DEGREE-AWARE MODELS PROVIDE MODEST BENEFIT\")\n",
    "    print(f\"   → Consider simpler degree-correction methods\")\n",
    "    print(f\"   → May not be worth added complexity\")\n",
    "else:\n",
    "    print(f\"   → DEGREE STRATIFICATION NOT HELPFUL\")\n",
    "    print(f\"   → Use naive compositional or ML approaches\")\n",
    "\n",
    "if abs(het_improvement - null_improvement) > 0.02:\n",
    "    print(f\"\\n   → BIOLOGICAL SIGNAL DETECTED\")\n",
    "    print(f\"   → Consider hybrid: degree-aware + biological corrections\")\n",
    "    print(f\"   → ML models may capture additional structure\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}