{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fc09b7",
   "metadata": {},
   "source": [
    "# Model Aggregation: Weighted Ensemble of Edge Prediction Models\n",
    "\n",
    "This notebook creates ensemble models by taking weighted averages of trained models across different permutations. The goal is to create a single robust model for each model type (Neural Network, Logistic Regression, Polynomial Logistic Regression, Random Forest) that combines the knowledge learned from multiple permutations of the heterogeneous network.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Input**: Multiple trained models from different permutations\n",
    "- **Process**: Weight models by their performance (AUC scores) and create ensemble predictions\n",
    "- **Output**: Single aggregated model for each model type\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Model Discovery**: Scan the models directory for all trained models\n",
    "2. **Performance Extraction**: Extract AUC scores for weighting\n",
    "3. **Weighted Averaging**: Create ensemble predictions weighted by performance\n",
    "4. **Model Persistence**: Save the aggregated models and their metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "import pathlib\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up paths\n",
    "repo_dir = pathlib.Path().cwd().parent\n",
    "src_dir = repo_dir / \"src\"\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Import custom modules\n",
    "from models import EdgePredictionNN\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Repository directory: {repo_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47cd23",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Configure the aggregation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d103bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model aggregation\n",
    "models_dir = repo_dir / \"models\"\n",
    "output_dir = repo_dir / \"aggregated_models\"\n",
    "edge_type = \"AeG\"  # Edge type to aggregate models for\n",
    "\n",
    "# Weighting strategy: 'auc' (performance-based) or 'equal' (equal weights)\n",
    "weighting_strategy = \"auc\"\n",
    "\n",
    "# Minimum number of models required for aggregation\n",
    "min_models_threshold = 2\n",
    "\n",
    "# Create output directory\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Edge type: {edge_type}\")\n",
    "print(f\"Weighting strategy: {weighting_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c64ff4",
   "metadata": {},
   "source": [
    "## Model Discovery and Loading\n",
    "\n",
    "Discover all trained models for the specified edge type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_models(models_dir, edge_type=None):\n",
    "    \"\"\"\n",
    "    Discover all trained models in the models directory.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Organized by model type and permutation\n",
    "    \"\"\"\n",
    "    model_files = {\n",
    "        'neural_network': [],\n",
    "        'logistic_regression': [],\n",
    "        'polynomial_logistic': [],\n",
    "        'random_forest': []\n",
    "    }\n",
    "    \n",
    "    # Scan for different model types\n",
    "    patterns = {\n",
    "        'neural_network': 'edge_prediction_model_*.pt',\n",
    "        'logistic_regression': 'logistic_regression_model_*.pkl',\n",
    "        'polynomial_logistic': 'polynomial_logistic_model_*.pkl',\n",
    "        'random_forest': 'random_forest_model_*.pkl'\n",
    "    }\n",
    "    \n",
    "    for model_type, pattern in patterns.items():\n",
    "        files = list(models_dir.glob(pattern))\n",
    "        \n",
    "        # Filter by edge type if specified\n",
    "        if edge_type:\n",
    "            files = [f for f in files if edge_type in f.name]\n",
    "        \n",
    "        model_files[model_type] = files\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "# Discover models\n",
    "discovered_models = discover_models(models_dir, edge_type)\n",
    "\n",
    "print(\"Discovered models:\")\n",
    "for model_type, files in discovered_models.items():\n",
    "    print(f\"  {model_type}: {len(files)} models\")\n",
    "    for f in files:\n",
    "        print(f\"    - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3407a52",
   "metadata": {},
   "source": [
    "## Performance Extraction\n",
    "\n",
    "Extract performance metrics for weighting models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_performance(model_file, model_type):\n",
    "    \"\"\"\n",
    "    Extract performance metrics from model files.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Performance metrics including AUC\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_type == 'neural_network':\n",
    "            # Load PyTorch model\n",
    "            checkpoint = torch.load(model_file, map_location='cpu')\n",
    "            return {\n",
    "                'auc': checkpoint['test_metrics']['auc'],\n",
    "                'ap': checkpoint['test_metrics']['average_precision'],\n",
    "                'permutation': checkpoint.get('permutation_name', 'unknown'),\n",
    "                'model_data': checkpoint\n",
    "            }\n",
    "        else:\n",
    "            # Load scikit-learn model\n",
    "            with open(model_file, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            return {\n",
    "                'auc': model_data['test_metrics']['auc'],\n",
    "                'ap': model_data['test_metrics']['ap'],\n",
    "                'permutation': model_data.get('permutation_name', 'unknown'),\n",
    "                'model_data': model_data\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract performance for all models\n",
    "model_performance = {}\n",
    "\n",
    "for model_type, files in discovered_models.items():\n",
    "    model_performance[model_type] = []\n",
    "    \n",
    "    for model_file in files:\n",
    "        perf = extract_model_performance(model_file, model_type)\n",
    "        if perf:\n",
    "            perf['file'] = model_file\n",
    "            model_performance[model_type].append(perf)\n",
    "\n",
    "# Display performance summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "for model_type, perfs in model_performance.items():\n",
    "    if perfs:\n",
    "        aucs = [p['auc'] for p in perfs]\n",
    "        print(f\"\\n{model_type.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Models: {len(perfs)}\")\n",
    "        print(f\"  AUC range: {min(aucs):.4f} - {max(aucs):.4f}\")\n",
    "        print(f\"  Mean AUC: {np.mean(aucs):.4f}\")\n",
    "        for p in perfs:\n",
    "            print(f\"    {p['permutation']}: AUC={p['auc']:.4f}, AP={p['ap']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6dd52a",
   "metadata": {},
   "source": [
    "## Weighted Ensemble Models\n",
    "\n",
    "Create ensemble models using performance-based weighting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    A weighted ensemble classifier that combines predictions from multiple models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models, weights, model_type, scalers=None):\n",
    "        self.models = models\n",
    "        self.weights = np.array(weights)\n",
    "        self.weights = self.weights / self.weights.sum()  # Normalize weights\n",
    "        self.model_type = model_type\n",
    "        self.scalers = scalers or [None] * len(models)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities using weighted ensemble.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for i, (model, scaler) in enumerate(zip(self.models, self.scalers)):\n",
    "            X_scaled = X if scaler is None else scaler.transform(X)\n",
    "            \n",
    "            if self.model_type == 'neural_network':\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    X_tensor = torch.FloatTensor(X_scaled)\n",
    "                    pred = model(X_tensor).cpu().numpy()\n",
    "                    # Convert single output to probability format\n",
    "                    pred_proba = np.column_stack([1 - pred, pred])\n",
    "            else:\n",
    "                pred_proba = model.predict_proba(X_scaled)\n",
    "            \n",
    "            predictions.append(pred_proba)\n",
    "        \n",
    "        # Weighted average of predictions\n",
    "        weighted_pred = np.average(predictions, axis=0, weights=self.weights)\n",
    "        return weighted_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] > 0.5).astype(int)\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"\n",
    "        Get information about the ensemble.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'model_type': self.model_type,\n",
    "            'num_models': len(self.models),\n",
    "            'weights': self.weights.tolist(),\n",
    "            'ensemble_type': 'weighted_average'\n",
    "        }\n",
    "\n",
    "print(\"Weighted ensemble classifier defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ddfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble_models(model_performance, weighting_strategy='auc', min_models=2):\n",
    "    \"\"\"\n",
    "    Create ensemble models for each model type.\n",
    "    \"\"\"\n",
    "    ensemble_models = {}\n",
    "    \n",
    "    for model_type, perfs in model_performance.items():\n",
    "        if len(perfs) < min_models:\n",
    "            print(f\"Skipping {model_type}: insufficient models ({len(perfs)} < {min_models})\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nCreating ensemble for {model_type}...\")\n",
    "        \n",
    "        # Extract models and performance\n",
    "        models = []\n",
    "        scalers = []\n",
    "        weights = []\n",
    "        \n",
    "        for perf in perfs:\n",
    "            if model_type == 'neural_network':\n",
    "                # Load neural network\n",
    "                model = EdgePredictionNN(input_size=2, hidden_size=64, dropout_rate=0.3)\n",
    "                model.load_state_dict(perf['model_data']['model_state_dict'])\n",
    "                model.eval()\n",
    "                models.append(model)\n",
    "                \n",
    "                # Neural networks typically have their own scaling\n",
    "                scalers.append(None)\n",
    "            else:\n",
    "                # Load scikit-learn model\n",
    "                models.append(perf['model_data']['model'])\n",
    "                scalers.append(perf['model_data'].get('scaler'))\n",
    "            \n",
    "            # Calculate weights based on strategy\n",
    "            if weighting_strategy == 'auc':\n",
    "                weights.append(perf['auc'])\n",
    "            else:  # equal weighting\n",
    "                weights.append(1.0)\n",
    "        \n",
    "        # Create ensemble\n",
    "        ensemble = WeightedEnsembleClassifier(\n",
    "            models=models,\n",
    "            weights=weights,\n",
    "            model_type=model_type,\n",
    "            scalers=scalers\n",
    "        )\n",
    "        \n",
    "        ensemble_models[model_type] = {\n",
    "            'ensemble': ensemble,\n",
    "            'component_performance': perfs,\n",
    "            'weights': ensemble.weights,\n",
    "            'metadata': {\n",
    "                'edge_type': edge_type,\n",
    "                'weighting_strategy': weighting_strategy,\n",
    "                'num_components': len(models),\n",
    "                'component_aucs': [p['auc'] for p in perfs],\n",
    "                'weighted_mean_auc': np.average([p['auc'] for p in perfs], weights=weights)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"  Created ensemble with {len(models)} models\")\n",
    "        print(f\"  Weights: {ensemble.weights}\")\n",
    "        print(f\"  Weighted mean AUC: {ensemble_models[model_type]['metadata']['weighted_mean_auc']:.4f}\")\n",
    "    \n",
    "    return ensemble_models\n",
    "\n",
    "# Create ensemble models\n",
    "ensemble_models = create_ensemble_models(\n",
    "    model_performance, \n",
    "    weighting_strategy=weighting_strategy,\n",
    "    min_models=min_models_threshold\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(ensemble_models)} ensemble models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1616583",
   "metadata": {},
   "source": [
    "## Save Ensemble Models\n",
    "\n",
    "Persist the ensemble models and their metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f29362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ensemble_models(ensemble_models, output_dir, edge_type):\n",
    "    \"\"\"\n",
    "    Save ensemble models and their metadata.\n",
    "    \"\"\"\n",
    "    saved_models = {}\n",
    "    \n",
    "    for model_type, ensemble_data in ensemble_models.items():\n",
    "        # Define filenames\n",
    "        model_filename = f\"ensemble_{model_type}_{edge_type}.pkl\"\n",
    "        metadata_filename = f\"ensemble_{model_type}_{edge_type}_metadata.json\"\n",
    "        \n",
    "        model_path = output_dir / model_filename\n",
    "        metadata_path = output_dir / metadata_filename\n",
    "        \n",
    "        # Save ensemble model\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(ensemble_data['ensemble'], f)\n",
    "        \n",
    "        # Prepare metadata for JSON serialization\n",
    "        metadata = ensemble_data['metadata'].copy()\n",
    "        metadata['weights'] = ensemble_data['weights'].tolist()\n",
    "        metadata['component_files'] = [str(p['file'].name) for p in ensemble_data['component_performance']]\n",
    "        metadata['component_permutations'] = [p['permutation'] for p in ensemble_data['component_performance']]\n",
    "        \n",
    "        # Save metadata\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        saved_models[model_type] = {\n",
    "            'model_path': model_path,\n",
    "            'metadata_path': metadata_path,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        \n",
    "        print(f\"Saved {model_type} ensemble:\")\n",
    "        print(f\"  Model: {model_path}\")\n",
    "        print(f\"  Metadata: {metadata_path}\")\n",
    "    \n",
    "    return saved_models\n",
    "\n",
    "# Save ensemble models\n",
    "saved_models = save_ensemble_models(ensemble_models, output_dir, edge_type)\n",
    "\n",
    "print(f\"\\nAll ensemble models saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37327e0",
   "metadata": {},
   "source": [
    "## Ensemble Performance Summary\n",
    "\n",
    "Create a comprehensive summary of the ensemble models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary visualization\n",
    "if ensemble_models:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Component AUCs and weights\n",
    "    model_types = list(ensemble_models.keys())\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(model_types)))\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    y_pos = 0\n",
    "    \n",
    "    for i, (model_type, ensemble_data) in enumerate(ensemble_models.items()):\n",
    "        aucs = ensemble_data['metadata']['component_aucs']\n",
    "        weights = ensemble_data['weights']\n",
    "        \n",
    "        # Plot AUCs as bars with weight-based alpha\n",
    "        for j, (auc, weight) in enumerate(zip(aucs, weights)):\n",
    "            ax1.barh(y_pos + j, auc, alpha=0.3 + 0.7 * weight, \n",
    "                    color=colors[i], label=model_type if j == 0 else \"\")\n",
    "            ax1.text(auc + 0.01, y_pos + j, f'{weight:.3f}', \n",
    "                    va='center', fontsize=8)\n",
    "        \n",
    "        y_pos += len(aucs) + 1\n",
    "    \n",
    "    ax1.set_xlabel('AUC Score')\n",
    "    ax1.set_title('Component Model AUCs\\n(Bar opacity = weight)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Weighted mean AUCs\n",
    "    ax2 = axes[1]\n",
    "    model_names = [mt.replace('_', ' ').title() for mt in model_types]\n",
    "    weighted_aucs = [ensemble_data['metadata']['weighted_mean_auc'] \n",
    "                    for ensemble_data in ensemble_models.values()]\n",
    "    \n",
    "    bars = ax2.bar(model_names, weighted_aucs, color=colors[:len(model_types)])\n",
    "    ax2.set_ylabel('Weighted Mean AUC')\n",
    "    ax2.set_title('Ensemble Model Performance')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, auc in zip(bars, weighted_aucs):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{auc:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = output_dir / f\"ensemble_summary_{edge_type}.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Summary plot saved to: {plot_path}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = {\n",
    "    'edge_type': edge_type,\n",
    "    'weighting_strategy': weighting_strategy,\n",
    "    'total_ensembles_created': len(ensemble_models),\n",
    "    'ensemble_details': {}\n",
    "}\n",
    "\n",
    "for model_type, ensemble_data in ensemble_models.items():\n",
    "    summary_report['ensemble_details'][model_type] = ensemble_data['metadata']\n",
    "\n",
    "# Save summary report\n",
    "summary_path = output_dir / f\"ensemble_summary_{edge_type}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary report saved to: {summary_path}\")\n",
    "print(\"\\nEnsemble model creation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa11ed0",
   "metadata": {},
   "source": [
    "## Test Ensemble Models\n",
    "\n",
    "Quick test to verify the ensemble models work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a79c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ensemble models with dummy data\n",
    "if ensemble_models:\n",
    "    print(\"Testing ensemble models with dummy data...\")\n",
    "    \n",
    "    # Create test data (2 features as expected by the models)\n",
    "    X_test = np.random.rand(10, 2)\n",
    "    \n",
    "    for model_type, ensemble_data in ensemble_models.items():\n",
    "        try:\n",
    "            ensemble = ensemble_data['ensemble']\n",
    "            \n",
    "            # Test prediction\n",
    "            proba = ensemble.predict_proba(X_test)\n",
    "            pred = ensemble.predict(X_test)\n",
    "            \n",
    "            print(f\"\\n{model_type} ensemble test:\")\n",
    "            print(f\"  Probability shape: {proba.shape}\")\n",
    "            print(f\"  Prediction shape: {pred.shape}\")\n",
    "            print(f\"  Sample probabilities: {proba[0]}\")\n",
    "            print(f\"  Sample prediction: {pred[0]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error testing {model_type}: {e}\")\n",
    "\n",
    "print(\"\\nTesting completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
