{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learned Analytical Formula Diagnostics\n",
    "\n",
    "This notebook diagnoses why the learned analytical formula is underperforming compared to the current analytical baseline.\n",
    "\n",
    "## Investigation Goals\n",
    "\n",
    "1. **Load and analyze learned formulas** from notebook 8/8b results\n",
    "2. **Compare predictions**: Learned vs Analytical vs Empirical\n",
    "3. **Identify failure modes**: Where does the learned formula perform poorly?\n",
    "4. **Test different configurations** with fewer permutations (<50)\n",
    "5. **Recommend improvements**\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "- What correlation does the learned formula achieve vs empirical?\n",
    "- Are the learned parameters reasonable?\n",
    "- Is regularization (λ=0.001) too strong?\n",
    "- Is training data quantity insufficient?\n",
    "- Does the formula structure have fundamental limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd().parent\n",
    "src_dir = repo_dir / 'src'\n",
    "data_dir = repo_dir / 'data'\n",
    "results_dir = repo_dir / 'results'\n",
    "\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "from learned_analytical import LearnedAnalyticalFormula\n",
    "\n",
    "print(f\"Repository: {repo_dir}\")\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge types\n",
    "test_edge_types = ['CtD', 'AeG']  # Sparse and dense\n",
    "\n",
    "# Test fewer permutations (<50)\n",
    "test_N_values = [5, 10, 20, 30, 40]\n",
    "\n",
    "# Test configurations\n",
    "test_configs = [\n",
    "    {'name': 'baseline', 'regularization': 0.001, 'n_starts': 10, 'formula': 'original'},\n",
    "    {'name': 'no_reg', 'regularization': 0.0, 'n_starts': 10, 'formula': 'original'},\n",
    "    {'name': 'high_reg', 'regularization': 0.01, 'n_starts': 10, 'formula': 'original'},\n",
    "    {'name': 'more_starts', 'regularization': 0.001, 'n_starts': 20, 'formula': 'original'},\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(test_edge_types)} edge types\")\n",
    "print(f\"Testing {len(test_N_values)} N values: {test_N_values}\")\n",
    "print(f\"Testing {len(test_configs)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_empirical_200(edge_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Load 200-permutation empirical frequencies.\"\"\"\n",
    "    file_path = results_dir / 'empirical_edge_frequencies' / f'edge_frequency_by_degree_{edge_type}.csv'\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def compute_analytical_predictions(empirical_df: pd.DataFrame, m: int) -> np.ndarray:\n",
    "    \"\"\"Compute current analytical formula predictions.\"\"\"\n",
    "    predictions = []\n",
    "    for _, row in empirical_df.iterrows():\n",
    "        u, v = row['source_degree'], row['target_degree']\n",
    "        uv = u * v\n",
    "        denom = np.sqrt(uv**2 + (m - u - v + 1)**2)\n",
    "        p = uv / denom if denom > 0 else 0.0\n",
    "        predictions.append(p)\n",
    "    return np.array(predictions)\n",
    "\n",
    "def get_graph_stats(edge_type: str) -> dict:\n",
    "    \"\"\"Get graph statistics.\"\"\"\n",
    "    edge_file = data_dir / 'permutations' / '000.hetmat' / 'edges' / f'{edge_type}.sparse.npz'\n",
    "    edge_matrix = sp.load_npz(edge_file)\n",
    "    n_sources, n_targets = edge_matrix.shape\n",
    "    m = edge_matrix.nnz\n",
    "    density = m / (n_sources * n_targets)\n",
    "    return {'m': m, 'density': density, 'n_sources': n_sources, 'n_targets': n_targets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_diagnostic_results = []\n",
    "\n",
    "for edge_type in test_edge_types:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TESTING {edge_type}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load empirical and graph stats\n",
    "    empirical_df = load_empirical_200(edge_type)\n",
    "    stats = get_graph_stats(edge_type)\n",
    "    m, density = stats['m'], stats['density']\n",
    "    \n",
    "    # Compute analytical baseline\n",
    "    analytical_preds = compute_analytical_predictions(empirical_df, m)\n",
    "    empirical_vals = empirical_df['frequency'].values\n",
    "    \n",
    "    baseline_corr = pearsonr(analytical_preds, empirical_vals)[0]\n",
    "    baseline_mae = np.mean(np.abs(analytical_preds - empirical_vals))\n",
    "    \n",
    "    print(f\"Baseline (Current Analytical):\")\n",
    "    print(f\"  Correlation: {baseline_corr:.6f}\")\n",
    "    print(f\"  MAE: {baseline_mae:.6f}\\n\")\n",
    "    \n",
    "    for config in test_configs:\n",
    "        print(f\"{'-'*60}\")\n",
    "        print(f\"Config: {config['name']}\")\n",
    "        print(f\"  Regularization λ={config['regularization']}, Starts={config['n_starts']}, Formula={config['formula']}\")\n",
    "        print(f\"{'-'*60}\\n\")\n",
    "        \n",
    "        # Train learner with different N values\n",
    "        for N in test_N_values:\n",
    "            learner = LearnedAnalyticalFormula(\n",
    "                n_random_starts=config['n_starts'],\n",
    "                regularization_lambda=config['regularization'],\n",
    "                formula_type=config['formula']\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # Train on N permutations\n",
    "                results = learner.find_minimum_permutations(\n",
    "                    graph_name=edge_type,\n",
    "                    data_dir=data_dir,\n",
    "                    results_dir=results_dir,\n",
    "                    N_candidates=[N],\n",
    "                    convergence_threshold=0.02,\n",
    "                    target_metric='correlation',\n",
    "                    min_metric_value=0.95\n",
    "                )\n",
    "                \n",
    "                learned_corr = results['final_metrics']['correlation']\n",
    "                learned_mae = results['final_metrics']['mae']\n",
    "                \n",
    "                # Store results\n",
    "                all_diagnostic_results.append({\n",
    "                    'edge_type': edge_type,\n",
    "                    'config': config['name'],\n",
    "                    'N': N,\n",
    "                    'regularization': config['regularization'],\n",
    "                    'n_starts': config['n_starts'],\n",
    "                    'formula': config['formula'],\n",
    "                    'learned_corr': learned_corr,\n",
    "                    'learned_mae': learned_mae,\n",
    "                    'baseline_corr': baseline_corr,\n",
    "                    'baseline_mae': baseline_mae,\n",
    "                    'improvement_corr': learned_corr - baseline_corr,\n",
    "                    'improvement_mae': baseline_mae - learned_mae,\n",
    "                    'params': learner.params.tolist()\n",
    "                })\n",
    "                \n",
    "                improvement = learned_corr - baseline_corr\n",
    "                status = \"✓\" if improvement > 0 else \"✗\"\n",
    "                print(f\"  N={N:2d}: r={learned_corr:.4f} (baseline={baseline_corr:.4f}, Δ={improvement:+.4f}) {status}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  N={N:2d}: FAILED - {e}\")\n",
    "                all_diagnostic_results.append({\n",
    "                    'edge_type': edge_type,\n",
    "                    'config': config['name'],\n",
    "                    'N': N,\n",
    "                    'regularization': config['regularization'],\n",
    "                    'n_starts': config['n_starts'],\n",
    "                    'formula': config['formula'],\n",
    "                    'learned_corr': None,\n",
    "                    'learned_mae': None,\n",
    "                    'baseline_corr': baseline_corr,\n",
    "                    'baseline_mae': baseline_mae,\n",
    "                    'improvement_corr': None,\n",
    "                    'improvement_mae': None,\n",
    "                    'params': None\n",
    "                })\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Convert to DataFrame\n",
    "diagnostic_df = pd.DataFrame(all_diagnostic_results)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL DIAGNOSTIC TESTS COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for edge_type in test_edge_types:\n",
    "    print(f\"\\n{edge_type}:\")\n",
    "    edge_results = diagnostic_df[diagnostic_df['edge_type'] == edge_type]\n",
    "    \n",
    "    # Find best configuration\n",
    "    valid_results = edge_results[edge_results['learned_corr'].notna()]\n",
    "    if len(valid_results) > 0:\n",
    "        best_idx = valid_results['improvement_corr'].idxmax()\n",
    "        best = valid_results.loc[best_idx]\n",
    "        \n",
    "        print(f\"  Best: {best['config']} with N={best['N']}\")\n",
    "        print(f\"    Correlation: {best['learned_corr']:.4f} (baseline: {best['baseline_corr']:.4f})\")\n",
    "        print(f\"    Improvement: {best['improvement_corr']:+.4f} ({best['improvement_corr']/best['baseline_corr']*100:+.1f}%)\")\n",
    "        \n",
    "        if best['improvement_corr'] > 0:\n",
    "            print(f\"    ✓ OUTPERFORMS BASELINE!\")\n",
    "        else:\n",
    "            print(f\"    ✗ Still underperforming\")\n",
    "    else:\n",
    "        print(f\"  No valid results\")\n",
    "\n",
    "# Display full results table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Full Results Table:\")\n",
    "display(diagnostic_df[['edge_type', 'config', 'N', 'learned_corr', 'baseline_corr', 'improvement_corr']].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Performance by Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot improvement vs N for each configuration\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax_idx, edge_type in enumerate(test_edge_types):\n",
    "    ax = axes[ax_idx]\n",
    "    edge_results = diagnostic_df[diagnostic_df['edge_type'] == edge_type]\n",
    "    \n",
    "    for config_name in diagnostic_df['config'].unique():\n",
    "        config_results = edge_results[edge_results['config'] == config_name]\n",
    "        valid = config_results[config_results['learned_corr'].notna()]\n",
    "        \n",
    "        if len(valid) > 0:\n",
    "            ax.plot(valid['N'], valid['improvement_corr'], marker='o', label=config_name, linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Baseline')\n",
    "    ax.set_xlabel('Number of Training Permutations (N)', fontsize=12)\n",
    "    ax.set_ylabel('Improvement over Baseline\\n(Δ Correlation)', fontsize=12)\n",
    "    ax.set_title(f'{edge_type} - Learned Formula Performance', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'learned_formula_diagnostics_improvement.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved improvement plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze learned parameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LEARNED PARAMETERS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "param_names = ['α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'κ']\n",
    "\n",
    "for edge_type in test_edge_types:\n",
    "    print(f\"\\n{edge_type}:\")\n",
    "    edge_results = diagnostic_df[(diagnostic_df['edge_type'] == edge_type) & (diagnostic_df['params'].notna())]\n",
    "    \n",
    "    if len(edge_results) > 0:\n",
    "        # Get best performing configuration\n",
    "        best_idx = edge_results['improvement_corr'].idxmax()\n",
    "        best = edge_results.loc[best_idx]\n",
    "        \n",
    "        print(f\"  Best config: {best['config']} (N={best['N']})\")\n",
    "        print(f\"  Learned parameters:\")\n",
    "        for pname, pval in zip(param_names, best['params']):\n",
    "            print(f\"    {pname}: {pval:10.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze which configuration works best\n",
    "valid_results = diagnostic_df[diagnostic_df['learned_corr'].notna()]\n",
    "\n",
    "if len(valid_results) > 0:\n",
    "    # Best configuration overall\n",
    "    best_config = valid_results.groupby('config')['improvement_corr'].mean().idxmax()\n",
    "    best_avg_improvement = valid_results.groupby('config')['improvement_corr'].mean()[best_config]\n",
    "    \n",
    "    print(f\"\\n1. BEST CONFIGURATION: {best_config}\")\n",
    "    print(f\"   Average improvement: {best_avg_improvement:+.4f}\")\n",
    "    \n",
    "    # Best N value\n",
    "    best_N = valid_results.groupby('N')['improvement_corr'].mean().idxmax()\n",
    "    best_N_improvement = valid_results.groupby('N')['improvement_corr'].mean()[best_N]\n",
    "    \n",
    "    print(f\"\\n2. BEST N VALUE: {best_N} permutations\")\n",
    "    print(f\"   Average improvement: {best_N_improvement:+.4f}\")\n",
    "    \n",
    "    # Check if any configuration beats baseline\n",
    "    any_beat_baseline = (valid_results['improvement_corr'] > 0).any()\n",
    "    \n",
    "    if any_beat_baseline:\n",
    "        print(f\"\\n3. ✓ SOME CONFIGURATIONS BEAT BASELINE!\")\n",
    "        winners = valid_results[valid_results['improvement_corr'] > 0]\n",
    "        print(f\"   {len(winners)} out of {len(valid_results)} tests successful\")\n",
    "    else:\n",
    "        print(f\"\\n3. ✗ NO CONFIGURATION BEATS BASELINE\")\n",
    "        print(f\"   Possible reasons:\")\n",
    "        print(f\"   - Formula structure is too simple\")\n",
    "        print(f\"   - Training data is insufficient (<50 permutations)\")\n",
    "        print(f\"   - Optimization is getting stuck in local minima\")\n",
    "        print(f\"   - Regularization is too strong/weak\")\n",
    "\n",
    "# Save diagnostic results\n",
    "diagnostic_df.to_csv(results_dir / 'learned_formula_diagnostics.csv', index=False)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Results saved to: {results_dir / 'learned_formula_diagnostics.csv'}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
