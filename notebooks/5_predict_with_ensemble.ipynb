{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3004a405",
   "metadata": {},
   "source": [
    "# Edge Prediction Using Ensemble Models\n",
    "\n",
    "This notebook applies the aggregated ensemble models to make predictions on permuted networks and provides comprehensive reporting of predictions both overall and by edge type.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Input**: Aggregated ensemble models and permuted network data\n",
    "- **Process**: Generate predictions for all edge types across all permutations\n",
    "- **Output**: Comprehensive prediction reports with statistical analysis\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Model Loading**: Load pre-trained ensemble models for each model type\n",
    "2. **Data Preparation**: Process permuted networks to extract features\n",
    "3. **Prediction Generation**: Apply ensemble models to generate edge probabilities\n",
    "4. **Analysis & Reporting**: Create detailed reports by edge type and overall statistics\n",
    "5. **Visualization**: Generate comparative plots and heatmaps of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "import pathlib\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up paths\n",
    "repo_dir = pathlib.Path().cwd().parent\n",
    "src_dir = repo_dir / \"src\"\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Import custom modules\n",
    "from models import EdgePredictionNN\n",
    "from data_processing import load_permutation_data, prepare_edge_prediction_data\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Repository directory: {repo_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27d0af",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Configure the prediction process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for prediction\n",
    "aggregated_models_dir = repo_dir / \"aggregated_models\"\n",
    "permutations_dir = repo_dir / \"data\" / \"permutations\"\n",
    "output_dir = repo_dir / \"prediction_results\"\n",
    "\n",
    "# Edge types to analyze (if None, will discover all available)\n",
    "target_edge_types = None  # or specify like [\"AeG\", \"CbG\", \"DaG\"]\n",
    "\n",
    "# Permutations to analyze (if None, will use all available)\n",
    "target_permutations = None  # or specify like [\"000.hetmat\", \"001.hetmat\"]\n",
    "\n",
    "# Prediction threshold for binary classification\n",
    "prediction_threshold = 0.5\n",
    "\n",
    "# Create output directory\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Aggregated models directory: {aggregated_models_dir}\")\n",
    "print(f\"Permutations directory: {permutations_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Prediction threshold: {prediction_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00cc8b7",
   "metadata": {},
   "source": [
    "## Discover Available Models and Data\n",
    "\n",
    "Find all available ensemble models and permuted network data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7982e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_ensemble_models(models_dir):\n",
    "    \"\"\"\n",
    "    Discover all available ensemble models.\n",
    "    \"\"\"\n",
    "    ensemble_files = {}\n",
    "    \n",
    "    if not models_dir.exists():\n",
    "        print(f\"Warning: Models directory {models_dir} does not exist\")\n",
    "        return ensemble_files\n",
    "    \n",
    "    # Find ensemble model files\n",
    "    for model_file in models_dir.glob(\"ensemble_*.pkl\"):\n",
    "        # Parse filename: ensemble_{model_type}_{edge_type}.pkl\n",
    "        parts = model_file.stem.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            model_type = parts[1]\n",
    "            edge_type = '_'.join(parts[2:])  # Handle edge types with underscores\n",
    "            \n",
    "            if edge_type not in ensemble_files:\n",
    "                ensemble_files[edge_type] = {}\n",
    "            \n",
    "            ensemble_files[edge_type][model_type] = model_file\n",
    "    \n",
    "    return ensemble_files\n",
    "\n",
    "def discover_permutations(permutations_dir):\n",
    "    \"\"\"\n",
    "    Discover all available permutations.\n",
    "    \"\"\"\n",
    "    if not permutations_dir.exists():\n",
    "        print(f\"Warning: Permutations directory {permutations_dir} does not exist\")\n",
    "        return []\n",
    "    \n",
    "    permutations = [p.name for p in permutations_dir.iterdir() \n",
    "                   if p.is_dir() and p.name.endswith('.hetmat')]\n",
    "    return sorted(permutations)\n",
    "\n",
    "# Discover available models and data\n",
    "available_models = discover_ensemble_models(aggregated_models_dir)\n",
    "available_permutations = discover_permutations(permutations_dir)\n",
    "\n",
    "print(\"Available ensemble models:\")\n",
    "for edge_type, models in available_models.items():\n",
    "    print(f\"  {edge_type}: {list(models.keys())}\")\n",
    "\n",
    "print(f\"\\nAvailable permutations: {len(available_permutations)}\")\n",
    "for perm in available_permutations[:5]:  # Show first 5\n",
    "    print(f\"  {perm}\")\n",
    "if len(available_permutations) > 5:\n",
    "    print(f\"  ... and {len(available_permutations) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3207d3",
   "metadata": {},
   "source": [
    "## Load Ensemble Models\n",
    "\n",
    "Load the ensemble models for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08bc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ensemble_models(available_models, target_edge_types=None):\n",
    "    \"\"\"\n",
    "    Load ensemble models from disk.\n",
    "    \"\"\"\n",
    "    loaded_models = {}\n",
    "    \n",
    "    edge_types_to_load = target_edge_types if target_edge_types else list(available_models.keys())\n",
    "    \n",
    "    for edge_type in edge_types_to_load:\n",
    "        if edge_type not in available_models:\n",
    "            print(f\"Warning: No models found for edge type {edge_type}\")\n",
    "            continue\n",
    "        \n",
    "        loaded_models[edge_type] = {}\n",
    "        \n",
    "        for model_type, model_file in available_models[edge_type].items():\n",
    "            try:\n",
    "                with open(model_file, 'rb') as f:\n",
    "                    model = pickle.load(f)\n",
    "                \n",
    "                loaded_models[edge_type][model_type] = model\n",
    "                print(f\"Loaded {model_type} ensemble for {edge_type}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {model_type} for {edge_type}: {e}\")\n",
    "    \n",
    "    return loaded_models\n",
    "\n",
    "# Load ensemble models\n",
    "ensemble_models = load_ensemble_models(available_models, target_edge_types)\n",
    "\n",
    "print(f\"\\nLoaded ensemble models for {len(ensemble_models)} edge types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de85bf",
   "metadata": {},
   "source": [
    "## Define Edge Type Mappings\n",
    "\n",
    "Map edge types to their corresponding node types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define edge type to node type mappings\n",
    "edge_type_mappings = {\n",
    "    # Anatomy edges\n",
    "    \"AdG\": (\"Anatomy\", \"Gene\"),\n",
    "    \"AeG\": (\"Anatomy\", \"Gene\"),\n",
    "    \"AuG\": (\"Anatomy\", \"Gene\"),\n",
    "    \n",
    "    # Compound edges\n",
    "    \"CbG\": (\"Compound\", \"Gene\"),\n",
    "    \"CcSE\": (\"Compound\", \"Side Effect\"),\n",
    "    \"CdG\": (\"Compound\", \"Gene\"),\n",
    "    \"CpD\": (\"Compound\", \"Disease\"),\n",
    "    \"CrC\": (\"Compound\", \"Compound\"),\n",
    "    \"CtD\": (\"Compound\", \"Disease\"),\n",
    "    \"CuG\": (\"Compound\", \"Gene\"),\n",
    "    \n",
    "    # Disease edges\n",
    "    \"DaG\": (\"Disease\", \"Gene\"),\n",
    "    \"DdG\": (\"Disease\", \"Gene\"),\n",
    "    \"DlA\": (\"Disease\", \"Anatomy\"),\n",
    "    \"DpS\": (\"Disease\", \"Symptom\"),\n",
    "    \"DrD\": (\"Disease\", \"Disease\"),\n",
    "    \"DuG\": (\"Disease\", \"Gene\"),\n",
    "    \n",
    "    # Gene edges\n",
    "    \"GcG\": (\"Gene\", \"Gene\"),\n",
    "    \"GiG\": (\"Gene\", \"Gene\"),\n",
    "    \"GpBP\": (\"Gene\", \"Biological Process\"),\n",
    "    \"GpCC\": (\"Gene\", \"Cellular Component\"),\n",
    "    \"GpMF\": (\"Gene\", \"Molecular Function\"),\n",
    "    \"GpPW\": (\"Gene\", \"Pathway\"),\n",
    "    \"Gr>G\": (\"Gene\", \"Gene\"),\n",
    "    \n",
    "    # Pharmacologic Class edges\n",
    "    \"PCiC\": (\"Pharmacologic Class\", \"Compound\")\n",
    "}\n",
    "\n",
    "print(f\"Defined mappings for {len(edge_type_mappings)} edge types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbfd8b",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "\n",
    "Apply ensemble models to generate predictions on permuted networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a695d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_for_permutation(permutation_name, ensemble_models, edge_type_mappings):\n",
    "    \"\"\"\n",
    "    Generate predictions for a single permutation across all edge types.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for edge_type, models in ensemble_models.items():\n",
    "        if edge_type not in edge_type_mappings:\n",
    "            print(f\"Warning: No mapping found for edge type {edge_type}\")\n",
    "            continue\n",
    "        \n",
    "        source_type, target_type = edge_type_mappings[edge_type]\n",
    "        \n",
    "        try:\n",
    "            # Load permutation data\n",
    "            perm_data = load_permutation_data(\n",
    "                permutations_dir,\n",
    "                permutation_name,\n",
    "                edge_type=edge_type,\n",
    "                source_node_type=source_type,\n",
    "                target_node_type=target_type\n",
    "            )\n",
    "            \n",
    "            if not perm_data:\n",
    "                print(f\"Warning: Could not load data for {permutation_name}, {edge_type}\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare features\n",
    "            features, labels = prepare_edge_prediction_data(perm_data, sample_negative_ratio=1.0)\n",
    "            \n",
    "            # Generate predictions with each model type\n",
    "            edge_results = {\n",
    "                'features': features,\n",
    "                'true_labels': labels,\n",
    "                'predictions': {},\n",
    "                'metadata': {\n",
    "                    'num_samples': len(features),\n",
    "                    'num_positive': labels.sum(),\n",
    "                    'num_negative': len(labels) - labels.sum(),\n",
    "                    'source_type': source_type,\n",
    "                    'target_type': target_type\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            for model_type, model in models.items():\n",
    "                try:\n",
    "                    # Generate predictions\n",
    "                    pred_proba = model.predict_proba(features)\n",
    "                    pred_binary = (pred_proba[:, 1] > prediction_threshold).astype(int)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    auc_score = roc_auc_score(labels, pred_proba[:, 1])\n",
    "                    ap_score = average_precision_score(labels, pred_proba[:, 1])\n",
    "                    \n",
    "                    edge_results['predictions'][model_type] = {\n",
    "                        'probabilities': pred_proba[:, 1],\n",
    "                        'binary_predictions': pred_binary,\n",
    "                        'auc': auc_score,\n",
    "                        'average_precision': ap_score,\n",
    "                        'accuracy': (pred_binary == labels).mean()\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error predicting with {model_type} for {edge_type}: {e}\")\n",
    "            \n",
    "            results[edge_type] = edge_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {edge_type} for {permutation_name}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Prediction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cfe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all permutations\n",
    "all_predictions = {}\n",
    "\n",
    "permutations_to_process = target_permutations if target_permutations else available_permutations\n",
    "\n",
    "print(f\"Generating predictions for {len(permutations_to_process)} permutations...\")\n",
    "\n",
    "for i, permutation in enumerate(tqdm(permutations_to_process, desc=\"Processing permutations\")):\n",
    "    print(f\"\\nProcessing permutation {i+1}/{len(permutations_to_process)}: {permutation}\")\n",
    "    \n",
    "    perm_results = generate_predictions_for_permutation(\n",
    "        permutation, ensemble_models, edge_type_mappings\n",
    "    )\n",
    "    \n",
    "    all_predictions[permutation] = perm_results\n",
    "    \n",
    "    # Print summary for this permutation\n",
    "    if perm_results:\n",
    "        print(f\"  Generated predictions for {len(perm_results)} edge types\")\n",
    "        for edge_type, results in perm_results.items():\n",
    "            models_count = len(results['predictions'])\n",
    "            print(f\"    {edge_type}: {models_count} models, {results['metadata']['num_samples']} samples\")\n",
    "\n",
    "print(f\"\\nCompleted predictions for {len(all_predictions)} permutations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304938c2",
   "metadata": {},
   "source": [
    "## Prediction Analysis and Reporting\n",
    "\n",
    "Analyze predictions and create comprehensive reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(all_predictions):\n",
    "    \"\"\"\n",
    "    Analyze predictions across all permutations and edge types.\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        'overall_stats': {},\n",
    "        'by_edge_type': {},\n",
    "        'by_model_type': {},\n",
    "        'by_permutation': {}\n",
    "    }\n",
    "    \n",
    "    # Collect all metrics\n",
    "    all_aucs = []\n",
    "    all_aps = []\n",
    "    all_accuracies = []\n",
    "    \n",
    "    edge_type_stats = defaultdict(lambda: defaultdict(list))\n",
    "    model_type_stats = defaultdict(lambda: defaultdict(list))\n",
    "    permutation_stats = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Process all predictions\n",
    "    for permutation, perm_results in all_predictions.items():\n",
    "        for edge_type, edge_results in perm_results.items():\n",
    "            for model_type, pred_results in edge_results['predictions'].items():\n",
    "                auc = pred_results['auc']\n",
    "                ap = pred_results['average_precision']\n",
    "                acc = pred_results['accuracy']\n",
    "                \n",
    "                # Overall stats\n",
    "                all_aucs.append(auc)\n",
    "                all_aps.append(ap)\n",
    "                all_accuracies.append(acc)\n",
    "                \n",
    "                # By edge type\n",
    "                edge_type_stats[edge_type]['auc'].append(auc)\n",
    "                edge_type_stats[edge_type]['ap'].append(ap)\n",
    "                edge_type_stats[edge_type]['accuracy'].append(acc)\n",
    "                \n",
    "                # By model type\n",
    "                model_type_stats[model_type]['auc'].append(auc)\n",
    "                model_type_stats[model_type]['ap'].append(ap)\n",
    "                model_type_stats[model_type]['accuracy'].append(acc)\n",
    "                \n",
    "                # By permutation\n",
    "                permutation_stats[permutation]['auc'].append(auc)\n",
    "                permutation_stats[permutation]['ap'].append(ap)\n",
    "                permutation_stats[permutation]['accuracy'].append(acc)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    analysis['overall_stats'] = {\n",
    "        'total_predictions': len(all_aucs),\n",
    "        'auc': {\n",
    "            'mean': np.mean(all_aucs),\n",
    "            'std': np.std(all_aucs),\n",
    "            'min': np.min(all_aucs),\n",
    "            'max': np.max(all_aucs),\n",
    "            'median': np.median(all_aucs)\n",
    "        },\n",
    "        'average_precision': {\n",
    "            'mean': np.mean(all_aps),\n",
    "            'std': np.std(all_aps),\n",
    "            'min': np.min(all_aps),\n",
    "            'max': np.max(all_aps),\n",
    "            'median': np.median(all_aps)\n",
    "        },\n",
    "        'accuracy': {\n",
    "            'mean': np.mean(all_accuracies),\n",
    "            'std': np.std(all_accuracies),\n",
    "            'min': np.min(all_accuracies),\n",
    "            'max': np.max(all_accuracies),\n",
    "            'median': np.median(all_accuracies)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate statistics by category\n",
    "    def calc_stats(values):\n",
    "        return {\n",
    "            'mean': np.mean(values),\n",
    "            'std': np.std(values),\n",
    "            'min': np.min(values),\n",
    "            'max': np.max(values),\n",
    "            'median': np.median(values),\n",
    "            'count': len(values)\n",
    "        }\n",
    "    \n",
    "    # By edge type\n",
    "    for edge_type, metrics in edge_type_stats.items():\n",
    "        analysis['by_edge_type'][edge_type] = {\n",
    "            'auc': calc_stats(metrics['auc']),\n",
    "            'ap': calc_stats(metrics['ap']),\n",
    "            'accuracy': calc_stats(metrics['accuracy'])\n",
    "        }\n",
    "    \n",
    "    # By model type\n",
    "    for model_type, metrics in model_type_stats.items():\n",
    "        analysis['by_model_type'][model_type] = {\n",
    "            'auc': calc_stats(metrics['auc']),\n",
    "            'ap': calc_stats(metrics['ap']),\n",
    "            'accuracy': calc_stats(metrics['accuracy'])\n",
    "        }\n",
    "    \n",
    "    # By permutation\n",
    "    for permutation, metrics in permutation_stats.items():\n",
    "        analysis['by_permutation'][permutation] = {\n",
    "            'auc': calc_stats(metrics['auc']),\n",
    "            'ap': calc_stats(metrics['ap']),\n",
    "            'accuracy': calc_stats(metrics['accuracy'])\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Perform analysis\n",
    "prediction_analysis = analyze_predictions(all_predictions)\n",
    "\n",
    "print(\"Prediction analysis completed\")\n",
    "print(f\"Total predictions analyzed: {prediction_analysis['overall_stats']['total_predictions']}\")\n",
    "print(f\"Overall mean AUC: {prediction_analysis['overall_stats']['auc']['mean']:.4f} ± {prediction_analysis['overall_stats']['auc']['std']:.4f}\")\n",
    "print(f\"Overall mean AP: {prediction_analysis['overall_stats']['average_precision']['mean']:.4f} ± {prediction_analysis['overall_stats']['average_precision']['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9ccf4",
   "metadata": {},
   "source": [
    "## Performance Summary Tables\n",
    "\n",
    "Create detailed performance tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2af824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary tables\n",
    "def create_summary_tables(analysis):\n",
    "    \"\"\"\n",
    "    Create summary tables for different groupings.\n",
    "    \"\"\"\n",
    "    tables = {}\n",
    "    \n",
    "    # Performance by edge type\n",
    "    edge_data = []\n",
    "    for edge_type, stats in analysis['by_edge_type'].items():\n",
    "        edge_data.append({\n",
    "            'Edge Type': edge_type,\n",
    "            'Mean AUC': f\"{stats['auc']['mean']:.4f}\",\n",
    "            'Std AUC': f\"{stats['auc']['std']:.4f}\",\n",
    "            'Mean AP': f\"{stats['ap']['mean']:.4f}\",\n",
    "            'Std AP': f\"{stats['ap']['std']:.4f}\",\n",
    "            'Mean Accuracy': f\"{stats['accuracy']['mean']:.4f}\",\n",
    "            'Count': stats['auc']['count']\n",
    "        })\n",
    "    \n",
    "    tables['by_edge_type'] = pd.DataFrame(edge_data).sort_values('Mean AUC', ascending=False)\n",
    "    \n",
    "    # Performance by model type\n",
    "    model_data = []\n",
    "    for model_type, stats in analysis['by_model_type'].items():\n",
    "        model_data.append({\n",
    "            'Model Type': model_type.replace('_', ' ').title(),\n",
    "            'Mean AUC': f\"{stats['auc']['mean']:.4f}\",\n",
    "            'Std AUC': f\"{stats['auc']['std']:.4f}\",\n",
    "            'Mean AP': f\"{stats['ap']['mean']:.4f}\",\n",
    "            'Std AP': f\"{stats['ap']['std']:.4f}\",\n",
    "            'Mean Accuracy': f\"{stats['accuracy']['mean']:.4f}\",\n",
    "            'Count': stats['auc']['count']\n",
    "        })\n",
    "    \n",
    "    tables['by_model_type'] = pd.DataFrame(model_data).sort_values('Mean AUC', ascending=False)\n",
    "    \n",
    "    # Performance by permutation (top 10)\n",
    "    perm_data = []\n",
    "    for permutation, stats in analysis['by_permutation'].items():\n",
    "        perm_data.append({\n",
    "            'Permutation': permutation,\n",
    "            'Mean AUC': f\"{stats['auc']['mean']:.4f}\",\n",
    "            'Std AUC': f\"{stats['auc']['std']:.4f}\",\n",
    "            'Mean AP': f\"{stats['ap']['mean']:.4f}\",\n",
    "            'Std AP': f\"{stats['ap']['std']:.4f}\",\n",
    "            'Mean Accuracy': f\"{stats['accuracy']['mean']:.4f}\",\n",
    "            'Count': stats['auc']['count']\n",
    "        })\n",
    "    \n",
    "    perm_df = pd.DataFrame(perm_data).sort_values('Mean AUC', ascending=False)\n",
    "    tables['by_permutation'] = perm_df.head(10)  # Top 10 permutations\n",
    "    \n",
    "    return tables\n",
    "\n",
    "summary_tables = create_summary_tables(prediction_analysis)\n",
    "\n",
    "print(\"Performance by Edge Type:\")\n",
    "print(summary_tables['by_edge_type'].to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nPerformance by Model Type:\")\n",
    "print(summary_tables['by_model_type'].to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nTop 10 Permutations by AUC:\")\n",
    "print(summary_tables['by_permutation'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedef029",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Create comprehensive visualizations of prediction performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63798fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "def create_prediction_visualizations(analysis, summary_tables, output_dir):\n",
    "    \"\"\"\n",
    "    Create various visualizations of prediction performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Figure 1: Performance by Edge Type\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # AUC by edge type\n",
    "    edge_types = summary_tables['by_edge_type']['Edge Type']\n",
    "    aucs = [float(x) for x in summary_tables['by_edge_type']['Mean AUC']]\n",
    "    auc_stds = [float(x) for x in summary_tables['by_edge_type']['Std AUC']]\n",
    "    \n",
    "    ax1 = axes[0, 0]\n",
    "    bars1 = ax1.bar(edge_types, aucs, yerr=auc_stds, capsize=5, alpha=0.7)\n",
    "    ax1.set_title('AUC Performance by Edge Type')\n",
    "    ax1.set_ylabel('AUC Score')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, auc in zip(bars1, aucs):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{auc:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # AP by edge type\n",
    "    aps = [float(x) for x in summary_tables['by_edge_type']['Mean AP']]\n",
    "    ap_stds = [float(x) for x in summary_tables['by_edge_type']['Std AP']]\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    bars2 = ax2.bar(edge_types, aps, yerr=ap_stds, capsize=5, alpha=0.7, color='orange')\n",
    "    ax2.set_title('Average Precision by Edge Type')\n",
    "    ax2.set_ylabel('Average Precision')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Model type comparison\n",
    "    model_types = summary_tables['by_model_type']['Model Type']\n",
    "    model_aucs = [float(x) for x in summary_tables['by_model_type']['Mean AUC']]\n",
    "    model_auc_stds = [float(x) for x in summary_tables['by_model_type']['Std AUC']]\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    bars3 = ax3.bar(model_types, model_aucs, yerr=model_auc_stds, capsize=5, alpha=0.7, color='green')\n",
    "    ax3.set_title('AUC Performance by Model Type')\n",
    "    ax3.set_ylabel('AUC Score')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overall distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.hist([float(x) for x in summary_tables['by_edge_type']['Mean AUC']], \n",
    "             bins=10, alpha=0.7, label='AUC', color='skyblue')\n",
    "    ax4.hist([float(x) for x in summary_tables['by_edge_type']['Mean AP']], \n",
    "             bins=10, alpha=0.7, label='AP', color='lightcoral')\n",
    "    ax4.set_title('Distribution of Performance Metrics')\n",
    "    ax4.set_xlabel('Score')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'prediction_performance_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Figure 2: Heatmap of performance by edge type and model type\n",
    "    heatmap_data = []\n",
    "    \n",
    "    for edge_type in analysis['by_edge_type'].keys():\n",
    "        row = []\n",
    "        for model_type in analysis['by_model_type'].keys():\n",
    "            # Find performance for this combination\n",
    "            performances = []\n",
    "            for perm_results in all_predictions.values():\n",
    "                if edge_type in perm_results and model_type in perm_results[edge_type]['predictions']:\n",
    "                    performances.append(perm_results[edge_type]['predictions'][model_type]['auc'])\n",
    "            \n",
    "            if performances:\n",
    "                row.append(np.mean(performances))\n",
    "            else:\n",
    "                row.append(np.nan)\n",
    "        \n",
    "        heatmap_data.append(row)\n",
    "    \n",
    "    if heatmap_data:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        heatmap_df = pd.DataFrame(\n",
    "            heatmap_data,\n",
    "            index=list(analysis['by_edge_type'].keys()),\n",
    "            columns=[mt.replace('_', ' ').title() for mt in analysis['by_model_type'].keys()]\n",
    "        )\n",
    "        \n",
    "        sns.heatmap(heatmap_df, annot=True, fmt='.3f', cmap='viridis', \n",
    "                   cbar_kws={'label': 'AUC Score'})\n",
    "        plt.title('AUC Performance Heatmap: Edge Type vs Model Type')\n",
    "        plt.ylabel('Edge Type')\n",
    "        plt.xlabel('Model Type')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "create_prediction_visualizations(prediction_analysis, summary_tables, output_dir)\n",
    "print(f\"Visualizations saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948628ae",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save all prediction results and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cefd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "def save_prediction_results(all_predictions, prediction_analysis, summary_tables, output_dir):\n",
    "    \"\"\"\n",
    "    Save all prediction results and analysis to files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save raw predictions (sampled due to size)\n",
    "    predictions_summary = {}\n",
    "    for permutation, perm_results in all_predictions.items():\n",
    "        predictions_summary[permutation] = {}\n",
    "        for edge_type, edge_results in perm_results.items():\n",
    "            predictions_summary[permutation][edge_type] = {\n",
    "                'metadata': edge_results['metadata'],\n",
    "                'model_performance': {}\n",
    "            }\n",
    "            \n",
    "            for model_type, pred_results in edge_results['predictions'].items():\n",
    "                predictions_summary[permutation][edge_type]['model_performance'][model_type] = {\n",
    "                    'auc': pred_results['auc'],\n",
    "                    'average_precision': pred_results['average_precision'],\n",
    "                    'accuracy': pred_results['accuracy'],\n",
    "                    'num_predictions': len(pred_results['probabilities'])\n",
    "                }\n",
    "    \n",
    "    # Save predictions summary\n",
    "    with open(output_dir / 'predictions_summary.json', 'w') as f:\n",
    "        json.dump(predictions_summary, f, indent=2)\n",
    "    \n",
    "    # Save analysis\n",
    "    with open(output_dir / 'prediction_analysis.json', 'w') as f:\n",
    "        json.dump(prediction_analysis, f, indent=2)\n",
    "    \n",
    "    # Save summary tables\n",
    "    for table_name, table_df in summary_tables.items():\n",
    "        table_df.to_csv(output_dir / f'{table_name}_summary.csv', index=False)\n",
    "    \n",
    "    # Create final report\n",
    "    report = {\n",
    "        'experiment_summary': {\n",
    "            'total_permutations': len(all_predictions),\n",
    "            'total_edge_types': len(prediction_analysis['by_edge_type']),\n",
    "            'total_model_types': len(prediction_analysis['by_model_type']),\n",
    "            'total_predictions': prediction_analysis['overall_stats']['total_predictions'],\n",
    "            'prediction_threshold': prediction_threshold\n",
    "        },\n",
    "        'overall_performance': prediction_analysis['overall_stats'],\n",
    "        'best_performing': {\n",
    "            'edge_type': summary_tables['by_edge_type'].iloc[0]['Edge Type'],\n",
    "            'model_type': summary_tables['by_model_type'].iloc[0]['Model Type'],\n",
    "            'permutation': summary_tables['by_permutation'].iloc[0]['Permutation']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'final_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"All results saved to {output_dir}\")\n",
    "    print(\"Files created:\")\n",
    "    for file_path in output_dir.glob('*'):\n",
    "        print(f\"  {file_path.name}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Save results\n",
    "final_report = save_prediction_results(all_predictions, prediction_analysis, summary_tables, output_dir)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {final_report['experiment_summary']['total_predictions']}\")\n",
    "print(f\"Mean AUC: {final_report['overall_performance']['auc']['mean']:.4f}\")\n",
    "print(f\"Mean AP: {final_report['overall_performance']['average_precision']['mean']:.4f}\")\n",
    "print(f\"Best edge type: {final_report['best_performing']['edge_type']}\")\n",
    "print(f\"Best model type: {final_report['best_performing']['model_type']}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
