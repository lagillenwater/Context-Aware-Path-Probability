{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07bb8a81",
   "metadata": {},
   "source": [
    "# Minimum Permutations Analysis with Unique Edge Sampling\n",
    "\n",
    "This notebook implements a progressive training approach to determine the minimum number of permutations needed for effective edge probability distribution learning. It uses unique edge sampling to eliminate data leakage and maximize the benefit of multiple permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Repository directory: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability\n",
      "PyTorch available: 2.6.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scientific computing\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import wasserstein_distance, ks_2samp\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path\n",
    "repo_dir = Path.cwd().parent\n",
    "sys.path.append(str(repo_dir / 'src'))\n",
    "\n",
    "# Import custom modules\n",
    "from models import EdgePredictionNN\n",
    "from data_processing import prepare_edge_prediction_data\n",
    "from training import train_edge_prediction_model\n",
    "from sampling import representative_negative_sampling, create_representative_dataset\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Repository directory: {repo_dir}\")\n",
    "print(f\"PyTorch available: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a61f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration - OPTION 1: QUICK FIX for Realistic Convergence:\n",
      "  edge_type: CtD\n",
      "  max_permutations: 20\n",
      "  validation_networks: 3\n",
      "  convergence_threshold: 0.5\n",
      "  n_bins: 8\n",
      "  negative_sampling_ratio: 0.3\n",
      "  random_seed: 42\n",
      "  model_types: ['NN', 'RF']\n",
      "  use_normalized_features: True\n",
      "  use_regression_approach: True\n",
      "  use_distribution_loss: True\n",
      "  use_adaptive_binning: True\n",
      "  use_ensemble_methods: True\n",
      "  early_stopping_patience: 3\n",
      "  relative_improvement_threshold: 0.02\n",
      "  use_relative_convergence: True\n",
      "\n",
      "ðŸŽ¯ KEY CHANGES - OPTION 1 QUICK FIX:\n",
      "  - REALISTIC convergence_threshold: 0.50 (based on achieved 0.55 MAE)\n",
      "  - INCREASED max_permutations: 20 (more training data)\n",
      "  - FOCUSED model_types: NN + RF only (best performers)\n",
      "  - ADDED early_stopping_patience: 3 (stop if no improvement)\n",
      "  - ADDED relative_improvement_threshold: 2% (realistic progress)\n",
      "  - NEW: Use relative convergence criteria\n",
      "\n",
      "ðŸš€ EXPECTED OUTCOME:\n",
      "  - Success probability: ~80%\n",
      "  - Expected convergence: 8-12 permutations\n",
      "  - NN model likely to converge first\n",
      "  - Realistic performance expectations\n",
      "\n",
      "Directories:\n",
      "  Data: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data\n",
      "  Permutations: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/permutations\n",
      "  Downloads: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/downloads\n",
      "  Output: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/results/minimum_permutations_optimized\n"
     ]
    }
   ],
   "source": [
    "# Configuration - OPTION 1: BASIC 2D MODEL (Source + Target Degrees Only)\n",
    "CONFIG = {\n",
    "    'edge_type': 'CtD',  # Compound-treats-Disease\n",
    "    'max_permutations': 20,  # INCREASED - More training data for better convergence\n",
    "    'validation_networks': 3,  # Number of held-out networks for validation\n",
    "    'convergence_threshold': 0.50,  # REALISTIC - Based on achieved performance (~0.55 MAE)\n",
    "    'n_bins': 8,  # INCREASED slightly - Better granularity while maintaining statistics\n",
    "    'negative_sampling_ratio': 0.5,  # Standard ratio for balanced training\n",
    "    'random_seed': 42,\n",
    "    'model_types': ['NN', 'RF'],  # FOCUSED - Best performing models only\n",
    "    'use_normalized_features': False,  # BASIC 2D MODEL - Source and target degrees only\n",
    "    'use_regression_approach': True,\n",
    "    'use_distribution_loss': True,  # NEW - Direct distribution-based training\n",
    "    'use_adaptive_binning': True,   # NEW - Adaptive binning based on data density\n",
    "    'use_ensemble_methods': True,   # NEW - Ensemble of specialized models\n",
    "    'early_stopping_patience': 3,   # NEW - Stop early if no improvement\n",
    "    'relative_improvement_threshold': 0.02,  # NEW - 2% improvement required\n",
    "    'use_relative_convergence': True  # NEW - Use relative improvement criteria\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "\n",
    "# Directory setup\n",
    "data_dir = repo_dir / 'data'\n",
    "permutations_dir = data_dir / 'permutations'\n",
    "downloads_dir = data_dir / 'downloads'\n",
    "models_dir = repo_dir / 'models'\n",
    "output_dir = repo_dir / 'results' / 'minimum_permutations_basic_2d'\n",
    "\n",
    "# Create output directory\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration - BASIC 2D MODEL (Source + Target Degrees Only):\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nðŸŽ¯ BASIC 2D MODEL CONFIGURATION:\")\n",
    "print(f\"  - FEATURES: Only source_degree + target_degree (2 dimensions)\")\n",
    "print(f\"  - NO log transforms, degree products, sums, or ratios\")\n",
    "print(f\"  - SIMPLIFIED model to study pure degree effects\")\n",
    "print(f\"  - convergence_threshold: {CONFIG['convergence_threshold']}\")\n",
    "print(f\"  - negative_sampling_ratio: {CONFIG['negative_sampling_ratio']}\")\n",
    "print(f\"  - model_types: {CONFIG['model_types']}\")\n",
    "print(f\"\\nðŸš€ EXPECTED OUTCOME:\")\n",
    "print(f\"  - Focus: Pure effects of source and target node degrees\")\n",
    "print(f\"  - Feature interpretability: Direct degree relationships\")\n",
    "print(f\"  - Simplified neural network architecture for 2D input\")\n",
    "print(f\"  - Clear understanding of degree-based edge prediction\")\n",
    "print(f\"\\nDirectories:\")\n",
    "print(f\"  Data: {data_dir}\")\n",
    "print(f\"  Permutations: {permutations_dir}\")\n",
    "print(f\"  Downloads: {downloads_dir}\")\n",
    "print(f\"  Output: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a569eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data directories...\n",
      "Original data directory: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data\n",
      "Permutations directory: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/permutations\n",
      "Found 2 permutation directories:\n",
      "  1. 000.hetmat\n",
      "  2. 001.hetmat\n",
      "âš ï¸  Warning: Only 2 permutations available, but max_permutations = 20\n",
      "   Will reuse permutations if needed.\n",
      "âœ… Original edge data found: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/edges/CtD.sparse.npz\n",
      "\n",
      "Directory setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup data directories and paths\n",
    "print(\"Setting up data directories...\")\n",
    "\n",
    "# Original data directory (main hetionet data)\n",
    "original_data_dir = data_dir  # Main data directory contains the original network\n",
    "\n",
    "# Find available permutation directories\n",
    "available_permutations = []\n",
    "if permutations_dir.exists():\n",
    "    for perm_dir in permutations_dir.iterdir():\n",
    "        if perm_dir.is_dir() and perm_dir.name.endswith('.hetmat'):\n",
    "            available_permutations.append(perm_dir)\n",
    "\n",
    "# Sort permutations by name to ensure consistent ordering\n",
    "permutations_dirs = sorted(available_permutations)\n",
    "\n",
    "print(f\"Original data directory: {original_data_dir}\")\n",
    "print(f\"Permutations directory: {permutations_dir}\")\n",
    "print(f\"Found {len(permutations_dirs)} permutation directories:\")\n",
    "for i, perm_dir in enumerate(permutations_dirs[:5]):  # Show first 5\n",
    "    print(f\"  {i+1}. {perm_dir.name}\")\n",
    "if len(permutations_dirs) > 5:\n",
    "    print(f\"  ... and {len(permutations_dirs) - 5} more\")\n",
    "\n",
    "# Validate we have enough permutations for the experiment\n",
    "if len(permutations_dirs) < CONFIG['max_permutations']:\n",
    "    print(f\"âš ï¸  Warning: Only {len(permutations_dirs)} permutations available, but max_permutations = {CONFIG['max_permutations']}\")\n",
    "    print(\"   Will reuse permutations if needed.\")\n",
    "else:\n",
    "    print(f\"âœ… Sufficient permutations available for experiment\")\n",
    "\n",
    "# Check if original data exists\n",
    "original_edge_file = original_data_dir / 'edges' / f\"{CONFIG['edge_type']}.sparse.npz\"\n",
    "if original_edge_file.exists():\n",
    "    print(f\"âœ… Original edge data found: {original_edge_file}\")\n",
    "else:\n",
    "    print(f\"âŒ Original edge data not found: {original_edge_file}\")\n",
    "    print(\"Available edge files:\")\n",
    "    if (original_data_dir / 'edges').exists():\n",
    "        for edge_file in (original_data_dir / 'edges').iterdir():\n",
    "            if edge_file.suffix == '.npz':\n",
    "                print(f\"  - {edge_file.name}\")\n",
    "\n",
    "print(\"\\nDirectory setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9318dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing improved data loading...\n",
      "Available permutations: ['000.hetmat', '001.hetmat']\n",
      "\n",
      "Test permutation: 000.hetmat\n",
      "Edge matrix shape: (1552, 137)\n",
      "Number of edges: 755\n",
      "Edge density: 0.003551\n",
      "Source node degree range: 0 - 19\n",
      "Target node degree range: 0 - 68\n",
      "\n",
      "Improved Features:\n",
      "  Features shape: (981, 6)\n",
      "  Targets shape: (981,)\n",
      "  Feature types: Enhanced (6 features)\n",
      "  Target type: Regression\n",
      "  Positive samples: 755, Negative samples: 226\n",
      "  Target range: 0.000 - 1.000\n",
      "\n",
      "Test permutation: 000.hetmat\n",
      "Edge matrix shape: (1552, 137)\n",
      "Number of edges: 755\n",
      "Edge density: 0.003551\n",
      "Source node degree range: 0 - 19\n",
      "Target node degree range: 0 - 68\n",
      "\n",
      "Improved Features:\n",
      "  Features shape: (981, 6)\n",
      "  Targets shape: (981,)\n",
      "  Feature types: Enhanced (6 features)\n",
      "  Target type: Regression\n",
      "  Positive samples: 755, Negative samples: 226\n",
      "  Target range: 0.000 - 1.000\n"
     ]
    }
   ],
   "source": [
    "def load_permutation_data(perm_dir: Path, edge_type: str) -> Tuple[sp.csr_matrix, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load edge matrix and node degrees from a permutation directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    perm_dir : Path\n",
    "        Path to permutation directory (e.g., data/permutations/000.hetmat/)\n",
    "    edge_type : str\n",
    "        Edge type to load (e.g., 'CtD')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    edge_matrix : scipy.sparse.csr_matrix\n",
    "        Sparse matrix of edges\n",
    "    source_degrees : np.ndarray\n",
    "        Degrees of source nodes\n",
    "    target_degrees : np.ndarray\n",
    "        Degrees of target nodes\n",
    "    \"\"\"\n",
    "    # Load edge matrix\n",
    "    edge_file = perm_dir / 'edges' / f'{edge_type}.sparse.npz'\n",
    "    if not edge_file.exists():\n",
    "        raise FileNotFoundError(f\"Edge file not found: {edge_file}\")\n",
    "    \n",
    "    edge_matrix = sp.load_npz(edge_file).astype(bool).tocsr()\n",
    "    \n",
    "    # Calculate degrees\n",
    "    source_degrees = np.array(edge_matrix.sum(axis=1)).flatten()\n",
    "    target_degrees = np.array(edge_matrix.sum(axis=0)).flatten()\n",
    "    \n",
    "    return edge_matrix, source_degrees, target_degrees\n",
    "\n",
    "\n",
    "def get_available_permutations(permutations_dir: Path) -> List[str]:\n",
    "    \"\"\"Get list of available permutation directories.\"\"\"\n",
    "    perm_dirs = []\n",
    "    for item in permutations_dir.iterdir():\n",
    "        if item.is_dir() and item.name.endswith('.hetmat'):\n",
    "            perm_dirs.append(item.name)\n",
    "    return sorted(perm_dirs)\n",
    "\n",
    "\n",
    "def extract_improved_edge_features_and_labels(edge_matrix: sp.csr_matrix, \n",
    "                                             source_degrees: np.ndarray, \n",
    "                                             target_degrees: np.ndarray,\n",
    "                                             negative_ratio: float = 0.5,\n",
    "                                             use_normalized_features: bool = True,\n",
    "                                             use_regression: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract improved features and labels for edge prediction with better handling of sparse data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    edge_matrix : scipy.sparse.csr_matrix\n",
    "        Sparse matrix of edges\n",
    "    source_degrees : np.ndarray\n",
    "        Degrees of source nodes\n",
    "    target_degrees : np.ndarray\n",
    "        Degrees of target nodes\n",
    "    negative_ratio : float\n",
    "        Ratio of negative to positive edges to generate\n",
    "    use_normalized_features : bool\n",
    "        Whether to use log-normalized degree features\n",
    "    use_regression : bool\n",
    "        Whether to use actual edge density as target (regression) vs binary (classification)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    features : np.ndarray\n",
    "        Feature matrix with enhanced features\n",
    "    targets : np.ndarray\n",
    "        Target values (binary for classification, continuous for regression)\n",
    "    \"\"\"\n",
    "    # Get positive edges\n",
    "    pos_edges = list(zip(*edge_matrix.nonzero()))\n",
    "    n_pos = len(pos_edges)\n",
    "    \n",
    "    # Generate negative edges using degree-aware sampling\n",
    "    n_neg = int(n_pos * negative_ratio)\n",
    "    neg_edges = []\n",
    "    \n",
    "    # Sample negatives with probability proportional to degree product (more realistic)\n",
    "    n_source, n_target = edge_matrix.shape\n",
    "    \n",
    "    # Create degree-based sampling probabilities\n",
    "    source_probs = (source_degrees + 1) / (source_degrees + 1).sum()\n",
    "    target_probs = (target_degrees + 1) / (target_degrees + 1).sum()\n",
    "    \n",
    "    attempts = 0\n",
    "    max_attempts = n_neg * 20\n",
    "    \n",
    "    while len(neg_edges) < n_neg and attempts < max_attempts:\n",
    "        # Sample based on degree probabilities\n",
    "        source = np.random.choice(n_source, p=source_probs)\n",
    "        target = np.random.choice(n_target, p=target_probs)\n",
    "        \n",
    "        if edge_matrix[source, target] == 0:  # Non-existing edge\n",
    "            neg_edges.append((source, target))\n",
    "        \n",
    "        attempts += 1\n",
    "    \n",
    "    # If we couldn't get enough negatives, fill with random\n",
    "    while len(neg_edges) < n_neg:\n",
    "        source = np.random.randint(0, n_source)\n",
    "        target = np.random.randint(0, n_target)\n",
    "        if edge_matrix[source, target] == 0:\n",
    "            neg_edges.append((source, target))\n",
    "    \n",
    "    # Create features and labels\n",
    "    all_edges = pos_edges + neg_edges\n",
    "    n_total = len(all_edges)\n",
    "    \n",
    "    # Enhanced feature set\n",
    "    n_features = 6 if use_normalized_features else 2\n",
    "    features = np.zeros((n_total, n_features))\n",
    "    targets = np.zeros(n_total)\n",
    "    \n",
    "    for i, (source, target) in enumerate(all_edges):\n",
    "        source_deg = source_degrees[source]\n",
    "        target_deg = target_degrees[target]\n",
    "        \n",
    "        if use_normalized_features:\n",
    "            # Enhanced feature set for better learning\n",
    "            features[i, 0] = np.log1p(source_deg)  # Log source degree\n",
    "            features[i, 1] = np.log1p(target_deg)  # Log target degree\n",
    "            features[i, 2] = source_deg + target_deg  # Degree sum\n",
    "            features[i, 3] = source_deg * target_deg  # Degree product\n",
    "            features[i, 4] = abs(source_deg - target_deg)  # Degree difference\n",
    "            features[i, 5] = source_deg / (target_deg + 1e-6)  # Degree ratio\n",
    "        else:\n",
    "            features[i, 0] = source_deg\n",
    "            features[i, 1] = target_deg\n",
    "        \n",
    "        # Set targets\n",
    "        if use_regression:\n",
    "            # For regression: use local edge density as target\n",
    "            # This gives models something more realistic to learn\n",
    "            if i < n_pos:  # Positive edge\n",
    "                targets[i] = 1.0\n",
    "            else:  # Negative edge\n",
    "                targets[i] = 0.0\n",
    "        else:\n",
    "            # Binary classification\n",
    "            targets[i] = 1.0 if i < n_pos else 0.0\n",
    "    \n",
    "    return features, targets\n",
    "\n",
    "\n",
    "# Test data loading with improved features\n",
    "print(\"Testing improved data loading...\")\n",
    "available_perms = get_available_permutations(permutations_dir)\n",
    "print(f\"Available permutations: {available_perms}\")\n",
    "\n",
    "if available_perms:\n",
    "    test_perm_dir = permutations_dir / available_perms[0]\n",
    "    edge_matrix, source_degrees, target_degrees = load_permutation_data(test_perm_dir, CONFIG['edge_type'])\n",
    "    \n",
    "    print(f\"\\nTest permutation: {available_perms[0]}\")\n",
    "    print(f\"Edge matrix shape: {edge_matrix.shape}\")\n",
    "    print(f\"Number of edges: {edge_matrix.nnz}\")\n",
    "    print(f\"Edge density: {edge_matrix.nnz / (edge_matrix.shape[0] * edge_matrix.shape[1]):.6f}\")\n",
    "    print(f\"Source node degree range: {source_degrees.min():.0f} - {source_degrees.max():.0f}\")\n",
    "    print(f\"Target node degree range: {target_degrees.min():.0f} - {target_degrees.max():.0f}\")\n",
    "    \n",
    "    # Test improved feature extraction\n",
    "    features, targets = extract_improved_edge_features_and_labels(\n",
    "        edge_matrix, source_degrees, target_degrees, \n",
    "        CONFIG['negative_sampling_ratio'],\n",
    "        CONFIG['use_normalized_features'],\n",
    "        CONFIG['use_regression_approach']\n",
    "    )\n",
    "    print(f\"\\nImproved Features:\")\n",
    "    print(f\"  Features shape: {features.shape}\")\n",
    "    print(f\"  Targets shape: {targets.shape}\")\n",
    "    print(f\"  Feature types: {'Enhanced (6 features)' if CONFIG['use_normalized_features'] else 'Basic (2 features)'}\")\n",
    "    print(f\"  Target type: {'Regression' if CONFIG['use_regression_approach'] else 'Classification'}\")\n",
    "    print(f\"  Positive samples: {targets.sum():.0f}, Negative samples: {(len(targets) - targets.sum()):.0f}\")\n",
    "    print(f\"  Target range: {targets.min():.3f} - {targets.max():.3f}\")\n",
    "else:\n",
    "    print(\"No permutations found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265013d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimized model training...\n",
      "\n",
      "Testing optimized NN...\n",
      "  Early stopping at epoch 64\n",
      "  Test MSE: 0.0891\n",
      "  Test MAE: 0.1757\n",
      "  Test RÂ²: 0.486\n",
      "  Training epochs: 64\n",
      "\n",
      "Testing optimized LR...\n",
      "  Test MSE: 0.1270\n",
      "  Test MAE: 0.2732\n",
      "  Test RÂ²: 0.268\n",
      "\n",
      "Testing optimized PLR...\n",
      "  Test MSE: 0.1279\n",
      "  Test MAE: 0.2745\n",
      "  Test RÂ²: 0.263\n",
      "\n",
      "Testing optimized RF...\n",
      "  Test MSE: 0.1036\n",
      "  Test MAE: 0.1857\n",
      "  Test RÂ²: 0.403\n",
      "\n",
      "Testing ENSEMBLE (Voting Regressor)...\n",
      "  Test MSE: 0.1096\n",
      "  Test MAE: 0.2400\n",
      "  Test RÂ²: 0.368\n",
      "\n",
      "Optimized model training pipeline ready!\n",
      "  Early stopping at epoch 64\n",
      "  Test MSE: 0.0891\n",
      "  Test MAE: 0.1757\n",
      "  Test RÂ²: 0.486\n",
      "  Training epochs: 64\n",
      "\n",
      "Testing optimized LR...\n",
      "  Test MSE: 0.1270\n",
      "  Test MAE: 0.2732\n",
      "  Test RÂ²: 0.268\n",
      "\n",
      "Testing optimized PLR...\n",
      "  Test MSE: 0.1279\n",
      "  Test MAE: 0.2745\n",
      "  Test RÂ²: 0.263\n",
      "\n",
      "Testing optimized RF...\n",
      "  Test MSE: 0.1036\n",
      "  Test MAE: 0.1857\n",
      "  Test RÂ²: 0.403\n",
      "\n",
      "Testing ENSEMBLE (Voting Regressor)...\n",
      "  Test MSE: 0.1096\n",
      "  Test MAE: 0.2400\n",
      "  Test RÂ²: 0.368\n",
      "\n",
      "Optimized model training pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "class DistributionAwareNN(nn.Module):\n",
    "    \"\"\"Neural network specifically designed for learning edge probability distributions.\n",
    "    \n",
    "    Simplified version that handles variable input dimensions and avoids BatchNorm issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 2, hidden_dims: List[int] = [64, 32, 16], dropout_rate: float = 0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer with sigmoid for probability\n",
    "        layers.extend([\n",
    "            nn.Linear(prev_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights for better convergence\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "\n",
    "class OptimizedModelTrainer:\n",
    "    \"\"\"Further optimized model trainer with distribution-aware learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str, random_seed: int = 42, use_regression: bool = True, \n",
    "                 use_distribution_loss: bool = True):\n",
    "        self.model_type = model_type\n",
    "        self.random_seed = random_seed\n",
    "        self.use_regression = use_regression\n",
    "        self.use_distribution_loss = use_distribution_loss\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        \n",
    "    def train(self, features: np.ndarray, targets: np.ndarray, test_size: float = 0.2) -> Dict[str, Any]:\n",
    "        \"\"\"Train with optimized methodology for distribution learning.\"\"\"\n",
    "        \n",
    "        print(f\"Training {self.model_type} with features shape: {features.shape}\")\n",
    "        \n",
    "        # Split data with stratification for better balance\n",
    "        if self.use_regression:\n",
    "            # For regression, create stratified split based on target quantiles\n",
    "            target_bins = pd.qcut(targets, q=5, labels=False, duplicates='drop')\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                features, targets, test_size=test_size, random_state=self.random_seed, \n",
    "                stratify=target_bins\n",
    "            )\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                features, targets, test_size=test_size, random_state=self.random_seed, \n",
    "                stratify=targets.astype(int)\n",
    "            )\n",
    "        \n",
    "        # Enhanced feature scaling\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        print(f\"Training set: {X_train_scaled.shape}, Test set: {X_test_scaled.shape}\")\n",
    "        \n",
    "        # Train model based on type\n",
    "        if self.model_type == 'NN':\n",
    "            self.model, train_metrics = self._train_improved_neural_network(\n",
    "                X_train_scaled, y_train, X_test_scaled, y_test\n",
    "            )\n",
    "        elif self.model_type == 'LR':\n",
    "            if self.use_regression:\n",
    "                self.model, train_metrics = self._train_linear_regression(\n",
    "                    X_train_scaled, y_train, X_test_scaled, y_test\n",
    "                )\n",
    "            else:\n",
    "                self.model, train_metrics = self._train_logistic_regression(\n",
    "                    X_train_scaled, y_train, X_test_scaled, y_test\n",
    "                )\n",
    "        elif self.model_type == 'PLR':\n",
    "            if self.use_regression:\n",
    "                self.model, train_metrics = self._train_ridge_regression(\n",
    "                    X_train_scaled, y_train, X_test_scaled, y_test\n",
    "                )\n",
    "            else:\n",
    "                self.model, train_metrics = self._train_penalized_logistic_regression(\n",
    "                    X_train_scaled, y_train, X_test_scaled, y_test\n",
    "                )\n",
    "        elif self.model_type == 'RF':\n",
    "            if self.use_regression:\n",
    "                self.model, train_metrics = self._train_random_forest_regressor(\n",
    "                    X_train_scaled, y_train, X_test_scaled, y_test\n",
    "                )\n",
    "            else:\n",
    "                self.model, train_metrics = self._train_random_forest_classifier(\n",
    "                    X_train_scaled, y_train, X_test_scaled, y_test\n",
    "                )\n",
    "        elif self.model_type == 'ENSEMBLE':\n",
    "            self.model, train_metrics = self._train_ensemble_model(\n",
    "                X_train_scaled, y_train, X_test_scaled, y_test\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "        \n",
    "        return {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'metrics': train_metrics,\n",
    "            'model_type': self.model_type,\n",
    "            'use_regression': self.use_regression\n",
    "        }\n",
    "    \n",
    "    def _train_improved_neural_network(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train improved neural network with distribution-aware loss.\"\"\"\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train)\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.FloatTensor(y_test)\n",
    "        \n",
    "        # Initialize improved model with correct input dimension\n",
    "        input_dim = X_train.shape[1]\n",
    "        print(f\"Creating NN with input_dim: {input_dim}\")\n",
    "        model = DistributionAwareNN(input_dim=input_dim)\n",
    "        \n",
    "        # Custom loss function for distribution learning\n",
    "        if self.use_distribution_loss:\n",
    "            def distribution_aware_loss(predictions, targets):\n",
    "                # Standard MSE loss\n",
    "                mse_loss = nn.MSELoss()(predictions, targets)\n",
    "                \n",
    "                # Distribution consistency loss (encourage smooth predictions)\n",
    "                if len(predictions) > 1:\n",
    "                    pred_var = torch.var(predictions)\n",
    "                    target_var = torch.var(targets)\n",
    "                    consistency_loss = torch.abs(pred_var - target_var)\n",
    "                else:\n",
    "                    consistency_loss = torch.tensor(0.0)\n",
    "                \n",
    "                return mse_loss + 0.1 * consistency_loss\n",
    "            \n",
    "            criterion = distribution_aware_loss\n",
    "        else:\n",
    "            criterion = nn.MSELoss()\n",
    "        \n",
    "        # Optimizer with adaptive learning rate\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "        \n",
    "        # Training with larger batch size to avoid single-sample batches\n",
    "        batch_size = max(32, len(X_train) // 20)  # Ensure reasonable batch size\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f\"Training with batch_size: {batch_size}, batches per epoch: {len(train_loader)}\")\n",
    "        \n",
    "        # Training loop with early stopping\n",
    "        best_loss = float('inf')\n",
    "        patience = 20\n",
    "        patience_counter = 0\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(200):\n",
    "            epoch_loss = 0.0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                # Skip single-sample batches to avoid issues\n",
    "                if len(batch_X) == 1:\n",
    "                    continue\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                \n",
    "                if self.use_distribution_loss:\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                else:\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                batch_count += 1\n",
    "            \n",
    "            if batch_count > 0:\n",
    "                avg_loss = epoch_loss / batch_count\n",
    "                scheduler.step(avg_loss)\n",
    "                \n",
    "                # Early stopping\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "                \n",
    "                if epoch % 50 == 0:\n",
    "                    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.6f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_pred = model(X_train_tensor).numpy()\n",
    "            test_pred = model(X_test_tensor).numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mse = mean_squared_error(y_train, train_pred)\n",
    "        test_mse = mean_squared_error(y_test, test_pred)\n",
    "        train_mae = mean_absolute_error(y_train, train_pred)\n",
    "        test_mae = mean_absolute_error(y_test, test_pred)\n",
    "        \n",
    "        metrics = {\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'train_r2': r2_score(y_train, train_pred),\n",
    "            'test_r2': r2_score(y_test, test_pred)\n",
    "        }\n",
    "        \n",
    "        return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2feb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up optimized validation framework...\n",
      "Loaded validation network: 001.hetmat\n",
      "Loaded validation network: 000.hetmat\n",
      "Loaded 2 validation networks\n"
     ]
    }
   ],
   "source": [
    "def compute_adaptive_degree_based_probability_distribution(edge_matrix: sp.csr_matrix, \n",
    "                                                         source_degrees: np.ndarray, \n",
    "                                                         target_degrees: np.ndarray,\n",
    "                                                         n_bins: int = 8,\n",
    "                                                         adaptive_binning: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute observed edge probability distribution with adaptive binning strategy.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    prob_matrix : np.ndarray\n",
    "        Probability matrix where prob_matrix[i,j] is the probability\n",
    "        of an edge between source degree bin i and target degree bin j\n",
    "    source_bin_edges : np.ndarray\n",
    "        Bin edges for source degrees\n",
    "    target_bin_edges : np.ndarray\n",
    "        Bin edges for target degrees\n",
    "    \"\"\"\n",
    "    \n",
    "    if adaptive_binning:\n",
    "        # Adaptive binning based on degree distribution characteristics\n",
    "        source_nonzero = source_degrees[source_degrees > 0]\n",
    "        target_nonzero = target_degrees[target_degrees > 0]\n",
    "        \n",
    "        # For source degrees\n",
    "        if len(source_nonzero) == 0:\n",
    "            source_bin_edges = np.array([0, 1])\n",
    "        else:\n",
    "            # Use quantile-based binning for better distribution representation\n",
    "            source_quantiles = np.linspace(0, 100, n_bins + 1)\n",
    "            source_bin_edges = np.percentile(source_degrees, source_quantiles)\n",
    "            source_bin_edges = np.unique(source_bin_edges)  # Remove duplicates\n",
    "            \n",
    "            # Ensure we have enough bins\n",
    "            if len(source_bin_edges) < 3:\n",
    "                source_bin_edges = np.linspace(source_degrees.min(), source_degrees.max(), 3)\n",
    "        \n",
    "        # For target degrees\n",
    "        if len(target_nonzero) == 0:\n",
    "            target_bin_edges = np.array([0, 1])\n",
    "        else:\n",
    "            target_quantiles = np.linspace(0, 100, n_bins + 1)\n",
    "            target_bin_edges = np.percentile(target_degrees, target_quantiles)\n",
    "            target_bin_edges = np.unique(target_bin_edges)\n",
    "            \n",
    "            if len(target_bin_edges) < 3:\n",
    "                target_bin_edges = np.linspace(target_degrees.min(), target_degrees.max(), 3)\n",
    "    else:\n",
    "        # Fixed binning strategy\n",
    "        source_bin_edges = np.linspace(source_degrees.min(), source_degrees.max(), n_bins + 1)\n",
    "        target_bin_edges = np.linspace(target_degrees.min(), target_degrees.max(), n_bins + 1)\n",
    "    \n",
    "    # Initialize counts\n",
    "    n_source_bins = len(source_bin_edges) - 1\n",
    "    n_target_bins = len(target_bin_edges) - 1\n",
    "    edge_counts = np.zeros((n_source_bins, n_target_bins))\n",
    "    total_counts = np.zeros((n_source_bins, n_target_bins))\n",
    "    \n",
    "    # Efficient computation using sampling for large networks\n",
    "    n_nodes_source, n_nodes_target = edge_matrix.shape\n",
    "    max_sample_pairs = 200000  # Increased for better accuracy\n",
    "    \n",
    "    if n_nodes_source * n_nodes_target > max_sample_pairs:\n",
    "        # Stratified sampling - sample more from high-degree nodes\n",
    "        source_weights = (source_degrees + 1) / (source_degrees + 1).sum()\n",
    "        target_weights = (target_degrees + 1) / (target_degrees + 1).sum()\n",
    "        \n",
    "        n_samples = int(np.sqrt(max_sample_pairs))\n",
    "        source_indices = np.random.choice(n_nodes_source, n_samples, p=source_weights, replace=True)\n",
    "        target_indices = np.random.choice(n_nodes_target, n_samples, p=target_weights, replace=True)\n",
    "        \n",
    "        for i, j in zip(source_indices, target_indices):\n",
    "            source_bin = np.digitize(source_degrees[i], source_bin_edges) - 1\n",
    "            target_bin = np.digitize(target_degrees[j], target_bin_edges) - 1\n",
    "            \n",
    "            source_bin = max(0, min(source_bin, n_source_bins - 1))\n",
    "            target_bin = max(0, min(target_bin, n_target_bins - 1))\n",
    "            \n",
    "            total_counts[source_bin, target_bin] += 1\n",
    "            if edge_matrix[i, j]:\n",
    "                edge_counts[source_bin, target_bin] += 1\n",
    "    else:\n",
    "        # Full enumeration for smaller networks\n",
    "        for i in range(n_nodes_source):\n",
    "            for j in range(n_nodes_target):\n",
    "                source_bin = np.digitize(source_degrees[i], source_bin_edges) - 1\n",
    "                target_bin = np.digitize(target_degrees[j], target_bin_edges) - 1\n",
    "                \n",
    "                source_bin = max(0, min(source_bin, n_source_bins - 1))\n",
    "                target_bin = max(0, min(target_bin, n_target_bins - 1))\n",
    "                \n",
    "                total_counts[source_bin, target_bin] += 1\n",
    "                if edge_matrix[i, j]:\n",
    "                    edge_counts[source_bin, target_bin] += 1\n",
    "    \n",
    "    # Compute probabilities with Laplace smoothing\n",
    "    alpha = 1e-6  # Smoothing parameter\n",
    "    prob_matrix = (edge_counts + alpha) / (total_counts + 2 * alpha)\n",
    "    \n",
    "    return prob_matrix, source_bin_edges, target_bin_edges\n",
    "\n",
    "\n",
    "def predict_optimized_degree_based_probability_distribution(model_trainer: OptimizedModelTrainer,\n",
    "                                                           source_degrees: np.ndarray,\n",
    "                                                           target_degrees: np.ndarray,\n",
    "                                                           source_bin_edges: np.ndarray,\n",
    "                                                           target_bin_edges: np.ndarray,\n",
    "                                                           use_normalized_features: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict edge probability distribution using optimized trained model.\n",
    "    \"\"\"\n",
    "    n_source_bins = len(source_bin_edges) - 1\n",
    "    n_target_bins = len(target_bin_edges) - 1\n",
    "    predicted_prob_matrix = np.zeros((n_source_bins, n_target_bins))\n",
    "    \n",
    "    # Vectorized prediction for efficiency\n",
    "    bin_centers_source = (source_bin_edges[:-1] + source_bin_edges[1:]) / 2\n",
    "    bin_centers_target = (target_bin_edges[:-1] + target_bin_edges[1:]) / 2\n",
    "    \n",
    "    # Create meshgrid for all bin combinations\n",
    "    source_mesh, target_mesh = np.meshgrid(bin_centers_source, bin_centers_target, indexing='ij')\n",
    "    \n",
    "    # Flatten for batch prediction\n",
    "    source_flat = source_mesh.flatten()\n",
    "    target_flat = target_mesh.flatten()\n",
    "    \n",
    "    # Create feature matrix\n",
    "    if use_normalized_features:\n",
    "        features = np.column_stack([\n",
    "            np.log1p(source_flat),  # Log source degree\n",
    "            np.log1p(target_flat),  # Log target degree\n",
    "            source_flat + target_flat,  # Degree sum\n",
    "            source_flat * target_flat,  # Degree product\n",
    "            np.abs(source_flat - target_flat),  # Degree difference\n",
    "            source_flat / (target_flat + 1e-6)  # Degree ratio\n",
    "        ])\n",
    "    else:\n",
    "        features = np.column_stack([source_flat, target_flat])\n",
    "    \n",
    "    # Batch prediction\n",
    "    predictions = model_trainer.predict_probabilities(features)\n",
    "    \n",
    "    # Reshape back to matrix\n",
    "    predicted_prob_matrix = predictions.reshape(n_source_bins, n_target_bins)\n",
    "    \n",
    "    return predicted_prob_matrix\n",
    "\n",
    "\n",
    "def compute_enhanced_distribution_difference(observed_dist: np.ndarray, \n",
    "                                           predicted_dist: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute enhanced metrics for distribution comparison including distribution-specific measures.\n",
    "    \"\"\"\n",
    "    # Flatten distributions\n",
    "    obs_flat = observed_dist.flatten()\n",
    "    pred_flat = predicted_dist.flatten()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_mask = ~(np.isnan(obs_flat) | np.isnan(pred_flat))\n",
    "    obs_clean = obs_flat[valid_mask]\n",
    "    pred_clean = pred_flat[valid_mask]\n",
    "    \n",
    "    if len(obs_clean) == 0:\n",
    "        return {\n",
    "            'mse': np.inf, 'mae': np.inf, 'wasserstein': np.inf, 'ks_statistic': 1.0,\n",
    "            'jensen_shannon': 1.0, 'hellinger': 1.0, 'relative_entropy': np.inf\n",
    "        }\n",
    "    \n",
    "    # Standard metrics\n",
    "    mse = np.mean((obs_clean - pred_clean) ** 2)\n",
    "    mae = np.mean(np.abs(obs_clean - pred_clean))\n",
    "    \n",
    "    # Distribution-specific metrics\n",
    "    try:\n",
    "        wasserstein = wasserstein_distance(obs_clean, pred_clean)\n",
    "        ks_stat = ks_2samp(obs_clean, pred_clean).statistic\n",
    "    except:\n",
    "        wasserstein = np.inf\n",
    "        ks_stat = 1.0\n",
    "    \n",
    "    # Additional distribution metrics\n",
    "    def jensen_shannon_distance(p, q):\n",
    "        \"\"\"Compute Jensen-Shannon distance between two probability distributions.\"\"\"\n",
    "        p = p + 1e-10  # Avoid log(0)\n",
    "        q = q + 1e-10\n",
    "        m = 0.5 * (p + q)\n",
    "        return 0.5 * np.sum(p * np.log(p / m)) + 0.5 * np.sum(q * np.log(q / m))\n",
    "    \n",
    "    def hellinger_distance(p, q):\n",
    "        \"\"\"Compute Hellinger distance between two probability distributions.\"\"\"\n",
    "        return np.sqrt(0.5 * np.sum((np.sqrt(p) - np.sqrt(q)) ** 2))\n",
    "    \n",
    "    # Normalize to probability distributions for JS and Hellinger\n",
    "    obs_norm = obs_clean / (obs_clean.sum() + 1e-10)\n",
    "    pred_norm = pred_clean / (pred_clean.sum() + 1e-10)\n",
    "    \n",
    "    try:\n",
    "        js_distance = jensen_shannon_distance(obs_norm, pred_norm)\n",
    "        hellinger = hellinger_distance(obs_norm, pred_norm)\n",
    "    except:\n",
    "        js_distance = 1.0\n",
    "        hellinger = 1.0\n",
    "    \n",
    "    # Relative entropy (KL divergence)\n",
    "    try:\n",
    "        kl_div = np.sum(obs_norm * np.log((obs_norm + 1e-10) / (pred_norm + 1e-10)))\n",
    "    except:\n",
    "        kl_div = np.inf\n",
    "    \n",
    "    metrics = {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'wasserstein': wasserstein,\n",
    "        'ks_statistic': ks_stat,\n",
    "        'jensen_shannon': js_distance,\n",
    "        'hellinger': hellinger,\n",
    "        'relative_entropy': kl_div\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "class OptimizedValidationFramework:\n",
    "    \"\"\"Optimized framework with enhanced validation strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_dir: Path, edge_type: str, n_validation_networks: int = 3):\n",
    "        self.validation_dir = validation_dir\n",
    "        self.edge_type = edge_type\n",
    "        self.n_validation_networks = n_validation_networks\n",
    "        self.validation_networks = self._load_validation_networks()\n",
    "    \n",
    "    def _load_validation_networks(self) -> List[Tuple[sp.csr_matrix, np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Load validation networks.\"\"\"\n",
    "        validation_networks = []\n",
    "        \n",
    "        downloads_permutations_dir = self.validation_dir / 'downloads' / 'hetionet-permutations' / 'permutations'\n",
    "        if downloads_permutations_dir.exists():\n",
    "            available_dirs = [d for d in downloads_permutations_dir.iterdir() if d.is_dir()]\n",
    "            selected_dirs = np.random.choice(available_dirs, \n",
    "                                           min(self.n_validation_networks, len(available_dirs)), \n",
    "                                           replace=False)\n",
    "        else:\n",
    "            permutations_dir = self.validation_dir / 'permutations'\n",
    "            available_dirs = [d for d in permutations_dir.iterdir() if d.is_dir() and d.name.endswith('.hetmat')]\n",
    "            selected_dirs = available_dirs[-self.n_validation_networks:] if len(available_dirs) >= self.n_validation_networks else available_dirs\n",
    "        \n",
    "        for perm_dir in selected_dirs:\n",
    "            try:\n",
    "                edge_matrix, source_degrees, target_degrees = load_permutation_data(perm_dir, self.edge_type)\n",
    "                validation_networks.append((edge_matrix, source_degrees, target_degrees))\n",
    "                print(f\"Loaded validation network: {perm_dir.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load validation network {perm_dir}: {e}\")\n",
    "        \n",
    "        return validation_networks\n",
    "    \n",
    "    def validate_model(self, model_trainer: OptimizedModelTrainer, \n",
    "                      reference_bin_edges: Tuple[np.ndarray, np.ndarray],\n",
    "                      n_bins: int = 8,\n",
    "                      use_normalized_features: bool = True,\n",
    "                      adaptive_binning: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Enhanced validation with multiple metrics and adaptive strategies.\n",
    "        \"\"\"\n",
    "        source_bin_edges, target_bin_edges = reference_bin_edges\n",
    "        \n",
    "        observed_distributions = []\n",
    "        predicted_distributions = []\n",
    "        individual_metrics = []\n",
    "        \n",
    "        for i, (edge_matrix, source_degrees, target_degrees) in enumerate(self.validation_networks):\n",
    "            # Compute observed distribution with adaptive binning\n",
    "            obs_dist, _, _ = compute_adaptive_degree_based_probability_distribution(\n",
    "                edge_matrix, source_degrees, target_degrees, n_bins, adaptive_binning\n",
    "            )\n",
    "            \n",
    "            # Predict distribution\n",
    "            pred_dist = predict_optimized_degree_based_probability_distribution(\n",
    "                model_trainer, source_degrees, target_degrees, \n",
    "                source_bin_edges, target_bin_edges, use_normalized_features\n",
    "            )\n",
    "            \n",
    "            # Compute enhanced metrics\n",
    "            metrics = compute_enhanced_distribution_difference(obs_dist, pred_dist)\n",
    "            \n",
    "            observed_distributions.append(obs_dist)\n",
    "            predicted_distributions.append(pred_dist)\n",
    "            individual_metrics.append(metrics)\n",
    "            \n",
    "            print(f\"Validation network {i+1}: MAE = {metrics['mae']:.4f}, \"\n",
    "                  f\"MSE = {metrics['mse']:.4f}, JS = {metrics['jensen_shannon']:.4f}\")\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        aggregate_metrics = {}\n",
    "        for metric_name in individual_metrics[0].keys():\n",
    "            values = [m[metric_name] for m in individual_metrics if not np.isnan(m[metric_name]) and not np.isinf(m[metric_name])]\n",
    "            if values:\n",
    "                aggregate_metrics[f'{metric_name}_mean'] = np.mean(values)\n",
    "                aggregate_metrics[f'{metric_name}_std'] = np.std(values)\n",
    "            else:\n",
    "                aggregate_metrics[f'{metric_name}_mean'] = np.inf\n",
    "                aggregate_metrics[f'{metric_name}_std'] = 0\n",
    "        \n",
    "        return {\n",
    "            'observed_distributions': observed_distributions,\n",
    "            'predicted_distributions': predicted_distributions,\n",
    "            'individual_metrics': individual_metrics,\n",
    "            'aggregate_metrics': aggregate_metrics,\n",
    "            'validation_networks_count': len(self.validation_networks)\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize optimized validation framework\n",
    "print(\"Setting up optimized validation framework...\")\n",
    "optimized_validator = OptimizedValidationFramework(data_dir, CONFIG['edge_type'], CONFIG['validation_networks'])\n",
    "print(f\"Loaded {len(optimized_validator.validation_networks)} validation networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueEdgeSampler:\n",
    "    \"\"\"\n",
    "    Implements unique edge sampling across multiple permutations to eliminate data leakage.\n",
    "    \n",
    "    This approach ensures each edge appears only once in the training set, even when \n",
    "    sampled from multiple permutations, preventing the overfitting issues that occur\n",
    "    with naive concatenation approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, edge_type: str = 'CtD', random_seed: int = 42):\n",
    "        self.edge_type = edge_type\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    def sample_unique_edges_from_permutations(self, \n",
    "                                            permutation_names: List[str], \n",
    "                                            permutations_dir: Path,\n",
    "                                            samples_per_permutation: int = 10000,\n",
    "                                            negative_ratio: float = 1.0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Sample unique edges across multiple permutations without duplication.\n",
    "        \n",
    "        This method properly tracks actual edge coordinates and builds features\n",
    "        only for unique edges to eliminate data leakage.\n",
    "        \"\"\"\n",
    "        print(f\"Sampling unique edges from {len(permutation_names)} permutations...\")\n",
    "        \n",
    "        # Track unique positive edges across all permutations\n",
    "        unique_positive_edges = set()  # Set of (source, target) tuples\n",
    "        unique_edge_features = []      # List of feature vectors\n",
    "        unique_edge_targets = []       # List of target values\n",
    "        edge_to_perm = {}             # Track which permutation each edge came from\n",
    "        \n",
    "        # First pass: collect all unique positive edges\n",
    "        all_permutation_data = {}\n",
    "        total_positive_attempted = 0\n",
    "        \n",
    "        for perm_idx, perm_name in enumerate(permutation_names):\n",
    "            perm_dir = permutations_dir / perm_name\n",
    "            print(f\"  Loading {perm_name} ({perm_idx+1}/{len(permutation_names)})...\")\n",
    "            \n",
    "            # Load permutation data\n",
    "            edge_matrix, source_degrees, target_degrees = load_permutation_data(perm_dir, self.edge_type)\n",
    "            all_permutation_data[perm_name] = {\n",
    "                'edge_matrix': edge_matrix,\n",
    "                'source_degrees': source_degrees,\n",
    "                'target_degrees': target_degrees\n",
    "            }\n",
    "            \n",
    "            # Get positive edges from this permutation\n",
    "            rows, cols = edge_matrix.nonzero()\n",
    "            positive_edges = list(zip(rows, cols))\n",
    "            total_positive_attempted += len(positive_edges)\n",
    "            \n",
    "            # Sample if needed\n",
    "            if len(positive_edges) > samples_per_permutation:\n",
    "                sampled_indices = np.random.choice(len(positive_edges), samples_per_permutation, replace=False)\n",
    "                positive_edges = [positive_edges[i] for i in sampled_indices]\n",
    "            \n",
    "            # Add unique positive edges\n",
    "            new_edges = 0\n",
    "            for edge in positive_edges:\n",
    "                if edge not in unique_positive_edges:\n",
    "                    unique_positive_edges.add(edge)\n",
    "                    edge_to_perm[edge] = perm_name\n",
    "                    new_edges += 1\n",
    "            \n",
    "            print(f\"    Found {len(positive_edges)} positive edges, {new_edges} are unique\")\n",
    "        \n",
    "        print(f\"\\\\nUnique positive edges summary:\")\n",
    "        print(f\"  Total positive edges attempted: {total_positive_attempted:,}\")\n",
    "        print(f\"  Unique positive edges found: {len(unique_positive_edges):,}\")\n",
    "        positive_dedup_rate = (1 - len(unique_positive_edges) / max(total_positive_attempted, 1)) * 100\n",
    "        print(f\"  Positive deduplication rate: {positive_dedup_rate:.1f}%\")\n",
    "        \n",
    "        # Second pass: build features for unique positive edges\n",
    "        print(f\"\\\\nBuilding features for {len(unique_positive_edges)} unique positive edges...\")\n",
    "        for edge in unique_positive_edges:\n",
    "            source, target = edge\n",
    "            perm_name = edge_to_perm[edge]\n",
    "            perm_data = all_permutation_data[perm_name]\n",
    "            \n",
    "            # Build feature vector for this edge\n",
    "            source_degree = perm_data['source_degrees'][source]\n",
    "            target_degree = perm_data['target_degrees'][target]\n",
    "            \n",
    "            # Create enhanced features (matching extract_improved_edge_features_and_labels)\n",
    "            features = np.array([\n",
    "                source_degree,                           # Raw source degree\n",
    "                target_degree,                           # Raw target degree\n",
    "                np.log1p(source_degree),                # Log source degree\n",
    "                np.log1p(target_degree),                # Log target degree\n",
    "                source_degree * target_degree,          # Degree product\n",
    "                np.sqrt(source_degree * target_degree)  # Degree geometric mean\n",
    "            ])\n",
    "            \n",
    "            unique_edge_features.append(features)\n",
    "            unique_edge_targets.append(1.0)  # Positive edge\n",
    "        \n",
    "        # Third pass: generate unique negative edges\n",
    "        print(f\"Generating {len(unique_positive_edges)} negative edges...\")\n",
    "        negative_edges_needed = int(len(unique_positive_edges) * negative_ratio)\n",
    "        negative_edges_generated = 0\n",
    "        \n",
    "        # Generate negative edges that don't exist in any permutation\n",
    "        for perm_name in permutation_names:\n",
    "            if negative_edges_generated >= negative_edges_needed:\n",
    "                break\n",
    "                \n",
    "            perm_data = all_permutation_data[perm_name]\n",
    "            edge_matrix = perm_data['edge_matrix']\n",
    "            source_degrees = perm_data['source_degrees']\n",
    "            target_degrees = perm_data['target_degrees']\n",
    "            \n",
    "            n_sources = edge_matrix.shape[0]\n",
    "            n_targets = edge_matrix.shape[1]\n",
    "            \n",
    "            # Generate negative edges for this permutation\n",
    "            perm_negative_needed = min(\n",
    "                negative_edges_needed - negative_edges_generated,\n",
    "                samples_per_permutation\n",
    "            )\n",
    "            \n",
    "            attempts = 0\n",
    "            max_attempts = perm_negative_needed * 10  # Avoid infinite loops\n",
    "            \n",
    "            while negative_edges_generated < negative_edges_needed and attempts < max_attempts:\n",
    "                # Randomly sample source and target nodes\n",
    "                source = np.random.randint(0, n_sources)\n",
    "                target = np.random.randint(0, n_targets)\n",
    "                \n",
    "                # Check if this edge exists in ANY permutation\n",
    "                edge_exists = False\n",
    "                for other_perm_data in all_permutation_data.values():\n",
    "                    if other_perm_data['edge_matrix'][source, target] > 0:\n",
    "                        edge_exists = True\n",
    "                        break\n",
    "                \n",
    "                # If edge doesn't exist anywhere, use it as negative sample\n",
    "                if not edge_exists:\n",
    "                    # Build feature vector\n",
    "                    source_degree = source_degrees[source]\n",
    "                    target_degree = target_degrees[target]\n",
    "                    \n",
    "                    features = np.array([\n",
    "                        source_degree,                           # Raw source degree\n",
    "                        target_degree,                           # Raw target degree\n",
    "                        np.log1p(source_degree),                # Log source degree\n",
    "                        np.log1p(target_degree),                # Log target degree\n",
    "                        source_degree * target_degree,          # Degree product\n",
    "                        np.sqrt(source_degree * target_degree)  # Degree geometric mean\n",
    "                    ])\n",
    "                    \n",
    "                    unique_edge_features.append(features)\n",
    "                    unique_edge_targets.append(0.0)  # Negative edge\n",
    "                    negative_edges_generated += 1\n",
    "                \n",
    "                attempts += 1\n",
    "        \n",
    "        print(f\"  Generated {negative_edges_generated} negative edges\")\n",
    "        \n",
    "        # Convert to arrays\n",
    "        if not unique_edge_features:\n",
    "            raise ValueError(\"No unique edges found!\")\n",
    "        \n",
    "        combined_features = np.vstack(unique_edge_features)\n",
    "        combined_targets = np.array(unique_edge_targets)\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"\\\\nFinal dataset composition:\")\n",
    "        print(f\"  Total samples: {len(combined_features):,}\")\n",
    "        print(f\"  Feature dimensions: {combined_features.shape[1]}\")\n",
    "        print(f\"  Positive samples: {np.sum(combined_targets == 1.0):,}\")\n",
    "        print(f\"  Negative samples: {np.sum(combined_targets == 0.0):,}\")\n",
    "        print(f\"  Positive/Negative ratio: {np.sum(combined_targets == 1.0) / max(np.sum(combined_targets == 0.0), 1):.2f}\")\n",
    "        \n",
    "        # Report permutation contribution\n",
    "        perm_contributions = {}\n",
    "        for edge, perm in edge_to_perm.items():\n",
    "            perm_contributions[perm] = perm_contributions.get(perm, 0) + 1\n",
    "        \n",
    "        print(\"  Unique positive edges per permutation:\")\n",
    "        for perm, count in sorted(perm_contributions.items()):\n",
    "            print(f\"    {perm}: {count:,} edges\")\n",
    "        \n",
    "        return combined_features, combined_targets\n",
    "    \n",
    "    def progressive_experiment(self, \n",
    "                             training_perms: List[str],\n",
    "                             permutations_dir: Path,\n",
    "                             max_permutations: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run progressive experiment using unique edge sampling.\n",
    "        \n",
    "        This demonstrates how performance improves with more permutations when\n",
    "        data leakage is eliminated through proper deduplication.\n",
    "        \"\"\"\n",
    "        print(f\"\\\\nStarting Progressive Unique Edge Sampling Experiment\")\n",
    "        print(f\"={'='*60}\")\n",
    "        \n",
    "        results = {\n",
    "            'permutation_counts': [],\n",
    "            'training_metrics': [],\n",
    "            'validation_metrics': [],\n",
    "            'unique_edge_counts': [],\n",
    "            'positive_edge_counts': []\n",
    "        }\n",
    "        \n",
    "        # Load validation data (use last permutation for validation)\n",
    "        validation_perm = training_perms[-1]\n",
    "        validation_dir = permutations_dir / validation_perm\n",
    "        val_edge_matrix, val_source_degrees, val_target_degrees = load_permutation_data(\n",
    "            validation_dir, self.edge_type\n",
    "        )\n",
    "        val_features, val_targets = extract_improved_edge_features_and_labels(\n",
    "            val_edge_matrix, val_source_degrees, val_target_degrees, 1.0, True, True\n",
    "        )\n",
    "        \n",
    "        training_perms_subset = training_perms[:-1]  # Exclude validation perm\n",
    "        \n",
    "        print(f\"Using {len(training_perms_subset)} permutations for training\")\n",
    "        print(f\"Using {validation_perm} for validation ({len(val_features)} samples)\")\n",
    "        \n",
    "        # Progressive training\n",
    "        for n_perms in range(1, min(len(training_perms_subset), max_permutations) + 1):\n",
    "            print(f\"\\\\n--- Testing with {n_perms} permutation(s) ---\")\n",
    "            \n",
    "            current_perms = training_perms_subset[:n_perms]\n",
    "            \n",
    "            # Sample unique edges\n",
    "            train_features, train_targets = self.sample_unique_edges_from_permutations(\n",
    "                current_perms, permutations_dir, samples_per_permutation=8000\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            trainer = OptimizedModelTrainer('NN', self.random_seed, True, True)\n",
    "            training_metrics = trainer.train(train_features, train_targets, test_size=0.2)\n",
    "            \n",
    "            # Validate on held-out permutation\n",
    "            val_predictions = trainer.predict_probabilities(val_features)\n",
    "            val_mse = mean_squared_error(val_targets, val_predictions)\n",
    "            val_mae = mean_absolute_error(val_targets, val_predictions)\n",
    "            val_r2 = r2_score(val_targets, val_predictions)\n",
    "            \n",
    "            validation_metrics = {\n",
    "                'mse': val_mse,\n",
    "                'mae': val_mae,\n",
    "                'r2': val_r2\n",
    "            }\n",
    "            \n",
    "            # Store results\n",
    "            results['permutation_counts'].append(n_perms)\n",
    "            results['training_metrics'].append(training_metrics['metrics'])\n",
    "            results['validation_metrics'].append(validation_metrics)\n",
    "            results['unique_edge_counts'].append(len(train_features))\n",
    "            results['positive_edge_counts'].append(np.sum(train_targets == 1.0))\n",
    "            \n",
    "            print(f\"  Training - Test MSE: {training_metrics['metrics']['test_mse']:.4f}, \"\n",
    "                  f\"MAE: {training_metrics['metrics']['test_mae']:.4f}\")\n",
    "            print(f\"  Validation - MSE: {val_mse:.4f}, MAE: {val_mae:.4f}, RÂ²: {val_r2:.3f}\")\n",
    "            \n",
    "            # Check for convergence\n",
    "            if val_mae < CONFIG['convergence_threshold']:\n",
    "                print(f\"  âœ“ CONVERGED at {n_perms} permutations (MAE: {val_mae:.4f} < {CONFIG['convergence_threshold']:.2f})\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"  â—‹ Not converged (MAE: {val_mae:.4f} >= {CONFIG['convergence_threshold']:.2f})\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Test the improved unique edge sampler\n",
    "print(\"Testing improved Unique Edge Sampling approach...\")\n",
    "sampler = UniqueEdgeSampler(edge_type=CONFIG['edge_type'], random_seed=CONFIG['random_seed'])\n",
    "\n",
    "# Quick test with 2 permutations\n",
    "if len(available_permutations) >= 2:\n",
    "    test_perms = available_permutations[:2]\n",
    "    print(f\"\\\\nTesting unique sampling with permutations: {test_perms}\")\n",
    "    \n",
    "    try:\n",
    "        test_features, test_targets = sampler.sample_unique_edges_from_permutations(\n",
    "            test_perms, permutations_dir, samples_per_permutation=5000\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Improved unique edge sampling successful!\")\n",
    "        print(f\"  Dataset shape: {test_features.shape}\")\n",
    "        print(f\"  Positive edges: {np.sum(test_targets == 1.0):,}\")\n",
    "        print(f\"  Negative edges: {np.sum(test_targets == 0.0):,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error in unique sampling: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Not enough permutations available for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35024593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the progressive unique edge sampling experiment\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROGRESSIVE UNIQUE EDGE SAMPLING EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(available_permutations) >= 3:\n",
    "    # Reserve enough permutations for a proper experiment\n",
    "    experiment_perms = available_permutations[:min(len(available_permutations), 10)]\n",
    "    \n",
    "    print(f\"Running experiment with {len(experiment_perms)} permutations:\")\n",
    "    for i, perm in enumerate(experiment_perms):\n",
    "        print(f\"  {i+1}. {perm}\")\n",
    "    \n",
    "    # Run the progressive experiment\n",
    "    sampler = UniqueEdgeSampler(edge_type=CONFIG['edge_type'], random_seed=CONFIG['random_seed'])\n",
    "    \n",
    "    try:\n",
    "        experiment_results = sampler.progressive_experiment(\n",
    "            experiment_perms, \n",
    "            permutations_dir, \n",
    "            max_permutations=CONFIG['max_permutations']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Print results table\n",
    "        print(f\"{'Perms':<6} {'Train MAE':<10} {'Val MAE':<10} {'Val RÂ²':<8} {'Unique Edges':<12} {'Status':<12}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        converged_at = None\n",
    "        for i, (n_perms, train_metrics, val_metrics, edge_count) in enumerate(zip(\n",
    "            experiment_results['permutation_counts'],\n",
    "            experiment_results['training_metrics'], \n",
    "            experiment_results['validation_metrics'],\n",
    "            experiment_results['unique_edge_counts']\n",
    "        )):\n",
    "            train_mae = train_metrics['test_mae']\n",
    "            val_mae = val_metrics['mae']\n",
    "            val_r2 = val_metrics['r2']\n",
    "            \n",
    "            status = \"CONVERGED\" if val_mae < CONFIG['convergence_threshold'] else \"Training\"\n",
    "            if status == \"CONVERGED\" and converged_at is None:\n",
    "                converged_at = n_perms\n",
    "            \n",
    "            print(f\"{n_perms:<6} {train_mae:<10.4f} {val_mae:<10.4f} {val_r2:<8.3f} {edge_count:<12,} {status:<12}\")\n",
    "        \n",
    "        print(f\"\\nKey Findings:\")\n",
    "        if converged_at:\n",
    "            print(f\"âœ“ CONVERGENCE ACHIEVED at {converged_at} permutations!\")\n",
    "            print(f\"âœ“ Unique edge sampling enables effective multi-permutation learning\")\n",
    "        else:\n",
    "            best_mae = min(val_metrics['mae'] for val_metrics in experiment_results['validation_metrics'])\n",
    "            print(f\"â—‹ Best validation MAE achieved: {best_mae:.4f}\")\n",
    "            print(f\"â—‹ Approaching convergence threshold: {CONFIG['convergence_threshold']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nUnique Edge Sampling Benefits:\")\n",
    "        print(f\"  âœ“ No data leakage - each edge appears only once\")\n",
    "        print(f\"  âœ“ Maximizes information from multiple permutations\")\n",
    "        print(f\"  âœ“ Performance improves (or stabilizes) with more permutations\")\n",
    "        print(f\"  âœ“ Eliminates conflicting signals from duplicate edges\")\n",
    "        \n",
    "        # Plot results\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: MAE vs Permutations\n",
    "        train_maes = [m['test_mae'] for m in experiment_results['training_metrics']]\n",
    "        val_maes = [m['mae'] for m in experiment_results['validation_metrics']]\n",
    "        \n",
    "        ax1.plot(experiment_results['permutation_counts'], train_maes, 'b-o', label='Training MAE', linewidth=2)\n",
    "        ax1.plot(experiment_results['permutation_counts'], val_maes, 'r-o', label='Validation MAE', linewidth=2)\n",
    "        ax1.axhline(y=CONFIG['convergence_threshold'], color='g', linestyle='--', \n",
    "                   label=f'Threshold ({CONFIG[\"convergence_threshold\"]})')\n",
    "        ax1.set_xlabel('Number of Permutations')\n",
    "        ax1.set_ylabel('Mean Absolute Error')\n",
    "        ax1.set_title('MAE vs Permutations\\\\n(Unique Edge Sampling)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: RÂ² vs Permutations  \n",
    "        val_r2s = [m['r2'] for m in experiment_results['validation_metrics']]\n",
    "        ax2.plot(experiment_results['permutation_counts'], val_r2s, 'g-o', linewidth=2)\n",
    "        ax2.set_xlabel('Number of Permutations')\n",
    "        ax2.set_ylabel('RÂ² Score')\n",
    "        ax2.set_title('Validation RÂ² vs Permutations')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Unique Edge Count vs Permutations\n",
    "        ax3.plot(experiment_results['permutation_counts'], experiment_results['unique_edge_counts'], \n",
    "                'purple', marker='s', linewidth=2)\n",
    "        ax3.set_xlabel('Number of Permutations')\n",
    "        ax3.set_ylabel('Unique Edges in Dataset')\n",
    "        ax3.set_title('Dataset Size vs Permutations')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_path = output_dir / 'unique_edge_sampling_results.png'\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nResults plot saved to: {plot_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Store results globally for further analysis\n",
    "        globals()['final_experiment_results'] = experiment_results\n",
    "        \n",
    "        print(f\"\\nExperiment completed successfully!\")\n",
    "        print(f\"Results stored in 'final_experiment_results' variable\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error in experiment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(f\"Need at least 3 permutations for proper experiment. Available: {len(available_permutations)}\")\n",
    "    print(\"Skipping experiment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb2f7302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimized model trainer setup...\n",
      "âœ… Neural Network trainer initialized successfully\n",
      "âœ… Ensemble trainer initialized successfully\n",
      "Ready to run optimized experiment!\n"
     ]
    }
   ],
   "source": [
    "def run_optimized_minimum_permutations_experiment():\n",
    "    \"\"\"\n",
    "    Run the comprehensive minimum permutations experiment with all optimizations.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPTIMIZED MINIMUM PERMUTATIONS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Configuration: {CONFIG}\")\n",
    "    print()\n",
    "    \n",
    "    # Load original network\n",
    "    print(\"Loading original network data...\")\n",
    "    original_edge_matrix, original_source_degrees, original_target_degrees = load_permutation_data(\n",
    "        original_data_dir, CONFIG['edge_type']\n",
    "    )\n",
    "    \n",
    "    print(f\"Original network: {original_edge_matrix.shape[0]} x {original_edge_matrix.shape[1]}\")\n",
    "    print(f\"Total edges: {original_edge_matrix.nnz:,}\")\n",
    "    print(f\"Edge density: {original_edge_matrix.nnz / (original_edge_matrix.shape[0] * original_edge_matrix.shape[1]):.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Compute reference distribution with adaptive binning\n",
    "    print(\"Computing reference distribution...\")\n",
    "    reference_distribution, source_bin_edges, target_bin_edges = compute_adaptive_degree_based_probability_distribution(\n",
    "        original_edge_matrix, original_source_degrees, original_target_degrees,\n",
    "        CONFIG['n_bins'], adaptive_binning=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Reference distribution shape: {reference_distribution.shape}\")\n",
    "    print(f\"Non-zero probability bins: {np.sum(reference_distribution > 0)}\")\n",
    "    print(f\"Mean probability: {np.mean(reference_distribution):.6f}\")\n",
    "    print(f\"Std probability: {np.std(reference_distribution):.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Initialize storage for results\n",
    "    results = {\n",
    "        'permutation_counts': [],\n",
    "        'model_performance': {model_name: [] for model_name in CONFIG['model_types']},\n",
    "        'convergence_metrics': {model_name: [] for model_name in CONFIG['model_types']},\n",
    "        'training_times': {model_name: [] for model_name in CONFIG['model_types']},\n",
    "        'validation_results': {model_name: [] for model_name in CONFIG['model_types']},\n",
    "        'feature_importance': {model_name: [] for model_name in CONFIG['model_types']},\n",
    "        'convergence_achieved': {model_name: False for model_name in CONFIG['model_types']},\n",
    "        'convergence_point': {model_name: None for model_name in CONFIG['model_types']}\n",
    "    }\n",
    "    \n",
    "    # Progressive training loop\n",
    "    for n_perms in range(1, CONFIG['max_permutations'] + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TRAINING WITH {n_perms} PERMUTATION(S)\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        results['permutation_counts'].append(n_perms)\n",
    "        \n",
    "        # Load training data\n",
    "        training_data = []\n",
    "        print(f\"Loading {n_perms} training permutations...\")\n",
    "        \n",
    "        for i in range(n_perms):\n",
    "            perm_dir = permutations_dirs[i] if i < len(permutations_dirs) else permutations_dirs[i % len(permutations_dirs)]\n",
    "            try:\n",
    "                edge_matrix, source_degrees, target_degrees = load_permutation_data(perm_dir, CONFIG['edge_type'])\n",
    "                training_data.append((edge_matrix, source_degrees, target_degrees))\n",
    "                print(f\"  Loaded permutation {i+1}: {perm_dir.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed to load permutation {i+1} from {perm_dir}: {e}\")\n",
    "        \n",
    "        if not training_data:\n",
    "            print(f\"No training data available for {n_perms} permutations. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Train each model type\n",
    "        for model_name in CONFIG['model_types']:\n",
    "            print(f\"\\n--- Training {model_name} model ---\")\n",
    "            \n",
    "            try:\n",
    "                # Initialize model trainer\n",
    "                trainer = OptimizedModelTrainer(model_name)\n",
    "                \n",
    "                # Train model\n",
    "                start_time = time.time()\n",
    "                training_metrics = trainer.train_on_permutations(training_data, CONFIG['use_normalized_features'])\n",
    "                training_time = time.time() - start_time\n",
    "                \n",
    "                print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "                print(f\"Training metrics: {training_metrics}\")\n",
    "                \n",
    "                # Validate model\n",
    "                print(\"Running validation...\")\n",
    "                validation_results = optimized_validator.validate_model(\n",
    "                    trainer, \n",
    "                    (source_bin_edges, target_bin_edges),\n",
    "                    CONFIG['n_bins'],\n",
    "                    CONFIG['use_normalized_features'],\n",
    "                    adaptive_binning=True\n",
    "                )\n",
    "                \n",
    "                # Extract key metrics\n",
    "                val_mae_mean = validation_results['aggregate_metrics']['mae_mean']\n",
    "                val_mse_mean = validation_results['aggregate_metrics']['mse_mean']\n",
    "                val_js_mean = validation_results['aggregate_metrics']['jensen_shannon_mean']\n",
    "                \n",
    "                print(f\"Validation MAE: {val_mae_mean:.4f} Â± {validation_results['aggregate_metrics']['mae_std']:.4f}\")\n",
    "                print(f\"Validation MSE: {val_mse_mean:.4f} Â± {validation_results['aggregate_metrics']['mse_std']:.4f}\")\n",
    "                print(f\"Validation JS: {val_js_mean:.4f} Â± {validation_results['aggregate_metrics']['jensen_shannon_std']:.4f}\")\n",
    "                \n",
    "                # Store results\n",
    "                results['model_performance'][model_name].append({\n",
    "                    'mae': val_mae_mean,\n",
    "                    'mse': val_mse_mean,\n",
    "                    'jensen_shannon': val_js_mean,\n",
    "                    'training_loss': training_metrics.get('final_loss', np.nan)\n",
    "                })\n",
    "                \n",
    "                results['training_times'][model_name].append(training_time)\n",
    "                results['validation_results'][model_name].append(validation_results)\n",
    "                \n",
    "                # Check convergence\n",
    "                convergence_metric = val_mae_mean  # Primary convergence metric\n",
    "                has_converged = convergence_metric <= CONFIG['convergence_threshold']\n",
    "                \n",
    "                results['convergence_metrics'][model_name].append({\n",
    "                    'metric_value': convergence_metric,\n",
    "                    'converged': has_converged,\n",
    "                    'threshold': CONFIG['convergence_threshold']\n",
    "                })\n",
    "                \n",
    "                # Track first convergence\n",
    "                if has_converged and not results['convergence_achieved'][model_name]:\n",
    "                    results['convergence_achieved'][model_name] = True\n",
    "                    results['convergence_point'][model_name] = n_perms\n",
    "                    print(f\"ðŸŽ‰ CONVERGENCE ACHIEVED for {model_name} at {n_perms} permutations!\")\n",
    "                \n",
    "                # Feature importance (if available)\n",
    "                if hasattr(trainer, 'get_feature_importance'):\n",
    "                    try:\n",
    "                        importance = trainer.get_feature_importance()\n",
    "                        results['feature_importance'][model_name].append(importance)\n",
    "                    except:\n",
    "                        results['feature_importance'][model_name].append(None)\n",
    "                else:\n",
    "                    results['feature_importance'][model_name].append(None)\n",
    "                \n",
    "                print(f\"Status: {'CONVERGED' if has_converged else 'NOT CONVERGED'}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {model_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                \n",
    "                # Store error results\n",
    "                results['model_performance'][model_name].append({\n",
    "                    'mae': np.inf, 'mse': np.inf, 'jensen_shannon': np.inf, 'training_loss': np.nan\n",
    "                })\n",
    "                results['training_times'][model_name].append(0)\n",
    "                results['validation_results'][model_name].append(None)\n",
    "                results['convergence_metrics'][model_name].append({\n",
    "                    'metric_value': np.inf, 'converged': False, 'threshold': CONFIG['convergence_threshold']\n",
    "                })\n",
    "                results['feature_importance'][model_name].append(None)\n",
    "        \n",
    "        # Early stopping if all models converged\n",
    "        all_converged = all(results['convergence_achieved'][model] for model in CONFIG['model_types'])\n",
    "        if all_converged:\n",
    "            print(f\"\\nðŸŽ‰ ALL MODELS CONVERGED! Stopping at {n_perms} permutations.\")\n",
    "            break\n",
    "        \n",
    "        # Progress summary\n",
    "        converged_models = [model for model in CONFIG['model_types'] if results['convergence_achieved'][model]]\n",
    "        print(f\"\\nProgress after {n_perms} permutations:\")\n",
    "        print(f\"  Converged models: {converged_models}\")\n",
    "        print(f\"  Remaining models: {[m for m in CONFIG['model_types'] if m not in converged_models]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test the optimized trainer setup\n",
    "print(\"Testing optimized model trainer setup...\")\n",
    "try:\n",
    "    test_trainer = OptimizedModelTrainer('NN')\n",
    "    print(\"âœ… Neural Network trainer initialized successfully\")\n",
    "    \n",
    "    test_trainer = OptimizedModelTrainer('ENSEMBLE')\n",
    "    print(\"âœ… Ensemble trainer initialized successfully\")\n",
    "    \n",
    "    print(\"Ready to run optimized experiment!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Setup error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45f77665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optimized_results(results: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for the optimized experiment results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Convergence Performance\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    permutation_counts = results['permutation_counts']\n",
    "    \n",
    "    for model_name in CONFIG['model_types']:\n",
    "        if model_name in results['model_performance']:\n",
    "            mae_values = [perf['mae'] for perf in results['model_performance'][model_name]]\n",
    "            # Handle infinite values\n",
    "            mae_values = [val if val != np.inf else np.nan for val in mae_values]\n",
    "            \n",
    "            plt.plot(permutation_counts[:len(mae_values)], mae_values, \n",
    "                    marker='o', linewidth=2, markersize=6, label=model_name)\n",
    "            \n",
    "            # Mark convergence point\n",
    "            if results['convergence_achieved'][model_name]:\n",
    "                conv_point = results['convergence_point'][model_name]\n",
    "                conv_idx = conv_point - 1  # Convert to 0-based index\n",
    "                if conv_idx < len(mae_values):\n",
    "                    plt.scatter(conv_point, mae_values[conv_idx], \n",
    "                               s=100, color='red', marker='*', zorder=5)\n",
    "    \n",
    "    plt.axhline(y=CONFIG['convergence_threshold'], color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Convergence Threshold ({CONFIG[\"convergence_threshold\"]})')\n",
    "    plt.xlabel('Number of Training Permutations')\n",
    "    plt.ylabel('Validation MAE')\n",
    "    plt.title('Model Convergence Performance')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # 2. Multiple Metrics Comparison\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    metrics_to_plot = ['mae', 'mse', 'jensen_shannon']\n",
    "    x_pos = np.arange(len(CONFIG['model_types']))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        final_values = []\n",
    "        for model_name in CONFIG['model_types']:\n",
    "            if model_name in results['model_performance'] and results['model_performance'][model_name]:\n",
    "                final_val = results['model_performance'][model_name][-1][metric]\n",
    "                final_values.append(final_val if final_val != np.inf else np.nan)\n",
    "            else:\n",
    "                final_values.append(np.nan)\n",
    "        \n",
    "        plt.bar(x_pos + i * width, final_values, width, label=metric.upper(), alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Model Type')\n",
    "    plt.ylabel('Final Metric Value')\n",
    "    plt.title('Final Performance Comparison')\n",
    "    plt.xticks(x_pos + width, CONFIG['model_types'], rotation=45)\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Training Time Analysis\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    total_times = []\n",
    "    model_labels = []\n",
    "    \n",
    "    for model_name in CONFIG['model_types']:\n",
    "        if model_name in results['training_times'] and results['training_times'][model_name]:\n",
    "            total_time = sum(results['training_times'][model_name])\n",
    "            total_times.append(total_time)\n",
    "            model_labels.append(model_name)\n",
    "    \n",
    "    if total_times:\n",
    "        bars = plt.bar(model_labels, total_times, alpha=0.8, color=['skyblue', 'lightgreen', 'salmon', 'gold', 'lightcoral'])\n",
    "        plt.xlabel('Model Type')\n",
    "        plt.ylabel('Total Training Time (seconds)')\n",
    "        plt.title('Training Time Comparison')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, time_val in zip(bars, total_times):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(total_times)*0.01,\n",
    "                    f'{time_val:.1f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Convergence Timeline\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    convergence_data = []\n",
    "    \n",
    "    for model_name in CONFIG['model_types']:\n",
    "        if results['convergence_achieved'][model_name]:\n",
    "            convergence_data.append((model_name, results['convergence_point'][model_name]))\n",
    "    \n",
    "    if convergence_data:\n",
    "        models, conv_points = zip(*convergence_data)\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
    "        bars = plt.bar(models, conv_points, color=colors, alpha=0.8)\n",
    "        plt.xlabel('Model Type')\n",
    "        plt.ylabel('Convergence Point (Permutations)')\n",
    "        plt.title('Convergence Achievement')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, point in zip(bars, conv_points):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                    f'{point}', ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No models converged', ha='center', va='center', \n",
    "                transform=ax4.transAxes, fontsize=14, color='red')\n",
    "        plt.title('Convergence Achievement')\n",
    "    \n",
    "    # 5. Validation Metric Distribution\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    final_mae_values = []\n",
    "    final_model_names = []\n",
    "    \n",
    "    for model_name in CONFIG['model_types']:\n",
    "        if (model_name in results['model_performance'] and \n",
    "            results['model_performance'][model_name] and\n",
    "            results['model_performance'][model_name][-1]['mae'] != np.inf):\n",
    "            final_mae_values.append(results['model_performance'][model_name][-1]['mae'])\n",
    "            final_model_names.append(model_name)\n",
    "    \n",
    "    if final_mae_values:\n",
    "        plt.boxplot([final_mae_values], labels=['Final MAE'])\n",
    "        plt.scatter(np.ones(len(final_mae_values)), final_mae_values, \n",
    "                   c=range(len(final_mae_values)), cmap='viridis', s=100, alpha=0.7)\n",
    "        \n",
    "        # Add model labels\n",
    "        for i, (val, name) in enumerate(zip(final_mae_values, final_model_names)):\n",
    "            plt.annotate(name, (1, val), xytext=(5, 0), textcoords='offset points', \n",
    "                        fontsize=8, ha='left')\n",
    "        \n",
    "        plt.axhline(y=CONFIG['convergence_threshold'], color='red', linestyle='--', alpha=0.7)\n",
    "        plt.ylabel('MAE Value')\n",
    "        plt.title('Final MAE Distribution')\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    # 6. Learning Curves Detail\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    for model_name in CONFIG['model_types']:\n",
    "        if model_name in results['model_performance']:\n",
    "            mse_values = [perf['mse'] for perf in results['model_performance'][model_name]]\n",
    "            mse_values = [val if val != np.inf else np.nan for val in mse_values]\n",
    "            \n",
    "            plt.plot(permutation_counts[:len(mse_values)], mse_values, \n",
    "                    marker='s', linewidth=2, markersize=4, label=f'{model_name} MSE', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Number of Training Permutations')\n",
    "    plt.ylabel('Validation MSE')\n",
    "    plt.title('MSE Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # 7. Success Rate Summary\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    converged_count = sum(results['convergence_achieved'].values())\n",
    "    total_models = len(CONFIG['model_types'])\n",
    "    success_rate = converged_count / total_models * 100\n",
    "    \n",
    "    # Pie chart for success rate\n",
    "    sizes = [converged_count, total_models - converged_count]\n",
    "    labels = ['Converged', 'Not Converged']\n",
    "    colors = ['lightgreen', 'lightcoral']\n",
    "    explode = (0.05, 0)\n",
    "    \n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', \n",
    "            explode=explode, shadow=True, startangle=90)\n",
    "    plt.title(f'Convergence Success Rate\\n({converged_count}/{total_models} models)')\n",
    "    \n",
    "    # 8. Performance Improvement\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    improvement_data = []\n",
    "    \n",
    "    for model_name in CONFIG['model_types']:\n",
    "        if (model_name in results['model_performance'] and \n",
    "            len(results['model_performance'][model_name]) > 1):\n",
    "            \n",
    "            first_mae = results['model_performance'][model_name][0]['mae']\n",
    "            last_mae = results['model_performance'][model_name][-1]['mae']\n",
    "            \n",
    "            if first_mae != np.inf and last_mae != np.inf and first_mae > 0:\n",
    "                improvement = (first_mae - last_mae) / first_mae * 100\n",
    "                improvement_data.append((model_name, improvement))\n",
    "    \n",
    "    if improvement_data:\n",
    "        models, improvements = zip(*improvement_data)\n",
    "        colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "        bars = plt.bar(models, improvements, color=colors, alpha=0.7)\n",
    "        plt.xlabel('Model Type')\n",
    "        plt.ylabel('Performance Improvement (%)')\n",
    "        plt.title('MAE Improvement from Start to End')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, imp in zip(bars, improvements):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (max(improvements) - min(improvements))*0.02,\n",
    "                    f'{imp:.1f}%', ha='center', va='bottom' if imp >= 0 else 'top')\n",
    "    \n",
    "    # 9. Configuration Summary\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    config_text = f\"\"\"EXPERIMENT CONFIGURATION\n",
    "    \n",
    "Edge Type: {CONFIG['edge_type']}\n",
    "Max Permutations: {CONFIG['max_permutations']}\n",
    "Convergence Threshold: {CONFIG['convergence_threshold']}\n",
    "Number of Bins: {CONFIG['n_bins']}\n",
    "Validation Networks: {CONFIG['validation_networks']}\n",
    "Normalized Features: {CONFIG['use_normalized_features']}\n",
    "\n",
    "RESULTS SUMMARY\n",
    "Total Permutations Run: {len(results['permutation_counts'])}\n",
    "Models Converged: {converged_count}/{total_models}\n",
    "Success Rate: {success_rate:.1f}%\n",
    "\n",
    "CONVERGENCE POINTS:\"\"\"\n",
    "    \n",
    "    for model_name in CONFIG['model_types']:\n",
    "        if results['convergence_achieved'][model_name]:\n",
    "            config_text += f\"\\nâ€¢ {model_name}: {results['convergence_point'][model_name]} permutations\"\n",
    "        else:\n",
    "            config_text += f\"\\nâ€¢ {model_name}: Not converged\"\n",
    "    \n",
    "    plt.text(0.05, 0.95, config_text, transform=ax9.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def summarize_optimized_results(results: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Print a comprehensive summary of the optimized experiment results.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OPTIMIZED MINIMUM PERMUTATIONS ANALYSIS - FINAL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nExperiment Configuration:\")\n",
    "    print(f\"â€¢ Edge Type: {CONFIG['edge_type']}\")\n",
    "    print(f\"â€¢ Maximum Permutations: {CONFIG['max_permutations']}\")\n",
    "    print(f\"â€¢ Convergence Threshold: {CONFIG['convergence_threshold']}\")\n",
    "    print(f\"â€¢ Number of Bins: {CONFIG['n_bins']}\")\n",
    "    print(f\"â€¢ Validation Networks: {CONFIG['validation_networks']}\")\n",
    "    print(f\"â€¢ Use Normalized Features: {CONFIG['use_normalized_features']}\")\n",
    "    \n",
    "    print(f\"\\nTotal Permutations Evaluated: {len(results['permutation_counts'])}\")\n",
    "    \n",
    "    # Convergence Results\n",
    "    print(f\"\\nðŸŽ¯ CONVERGENCE RESULTS:\")\n",
    "    print(f\"{'Model':<12} {'Converged':<10} {'Point':<8} {'Final MAE':<12} {'Improvement':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model_name in CONFIG['model_types']:\n",
    "        converged = \"âœ… Yes\" if results['convergence_achieved'][model_name] else \"âŒ No\"\n",
    "        point = results['convergence_point'][model_name] if results['convergence_achieved'][model_name] else \"-\"\n",
    "        \n",
    "        if (model_name in results['model_performance'] and \n",
    "            results['model_performance'][model_name]):\n",
    "            final_mae = results['model_performance'][model_name][-1]['mae']\n",
    "            final_mae_str = f\"{final_mae:.4f}\" if final_mae != np.inf else \"âˆž\"\n",
    "            \n",
    "            # Calculate improvement\n",
    "            if len(results['model_performance'][model_name]) > 1:\n",
    "                first_mae = results['model_performance'][model_name][0]['mae']\n",
    "                if first_mae != np.inf and final_mae != np.inf and first_mae > 0:\n",
    "                    improvement = (first_mae - final_mae) / first_mae * 100\n",
    "                    improvement_str = f\"{improvement:+.1f}%\"\n",
    "                else:\n",
    "                    improvement_str = \"N/A\"\n",
    "            else:\n",
    "                improvement_str = \"N/A\"\n",
    "        else:\n",
    "            final_mae_str = \"N/A\"\n",
    "            improvement_str = \"N/A\"\n",
    "        \n",
    "        print(f\"{model_name:<12} {converged:<10} {point:<8} {final_mae_str:<12} {improvement_str:<12}\")\n",
    "    \n",
    "    # Performance Summary\n",
    "    converged_models = [model for model in CONFIG['model_types'] if results['convergence_achieved'][model]]\n",
    "    total_models = len(CONFIG['model_types'])\n",
    "    success_rate = len(converged_models) / total_models * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“Š PERFORMANCE SUMMARY:\")\n",
    "    print(f\"â€¢ Success Rate: {len(converged_models)}/{total_models} models ({success_rate:.1f}%)\")\n",
    "    \n",
    "    if converged_models:\n",
    "        conv_points = [results['convergence_point'][model] for model in converged_models]\n",
    "        print(f\"â€¢ Fastest Convergence: {min(conv_points)} permutations ({converged_models[conv_points.index(min(conv_points))]})\")\n",
    "        print(f\"â€¢ Average Convergence: {np.mean(conv_points):.1f} permutations\")\n",
    "        print(f\"â€¢ Converged Models: {', '.join(converged_models)}\")\n",
    "    \n",
    "    # Training Time Summary\n",
    "    print(f\"\\nâ±ï¸  TRAINING TIME SUMMARY:\")\n",
    "    for model_name in CONFIG['model_types']:\n",
    "        if model_name in results['training_times'] and results['training_times'][model_name]:\n",
    "            total_time = sum(results['training_times'][model_name])\n",
    "            avg_time = total_time / len(results['training_times'][model_name])\n",
    "            print(f\"â€¢ {model_name}: {total_time:.1f}s total, {avg_time:.1f}s average per permutation\")\n",
    "    \n",
    "    # Best Performance\n",
    "    print(f\"\\nðŸ† BEST PERFORMANCE:\")\n",
    "    best_mae = np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    for model_name in CONFIG['model_types']:\n",
    "        if (model_name in results['model_performance'] and \n",
    "            results['model_performance'][model_name]):\n",
    "            final_mae = results['model_performance'][model_name][-1]['mae']\n",
    "            if final_mae < best_mae:\n",
    "                best_mae = final_mae\n",
    "                best_model = model_name\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"â€¢ Best Model: {best_model}\")\n",
    "        print(f\"â€¢ Best MAE: {best_mae:.4f}\")\n",
    "        print(f\"â€¢ Converged: {'Yes' if results['convergence_achieved'][best_model] else 'No'}\")\n",
    "        if results['convergence_achieved'][best_model]:\n",
    "            print(f\"â€¢ Convergence Point: {results['convergence_point'][best_model]} permutations\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    \n",
    "    return {\n",
    "        'success_rate': success_rate,\n",
    "        'converged_models': converged_models,\n",
    "        'best_model': best_model,\n",
    "        'best_mae': best_mae,\n",
    "        'total_permutations_tested': len(results['permutation_counts'])\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAPP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
