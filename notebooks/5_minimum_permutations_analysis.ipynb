{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07bb8a81",
   "metadata": {},
   "source": [
    "# Minimum Permutations Analysis with Unique Edge Sampling\n",
    "\n",
    "This notebook implements a progressive training approach to determine the minimum number of permutations needed for effective edge probability distribution learning. It uses unique edge sampling to eliminate data leakage and maximize the benefit of multiple permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e55e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Repository directory: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "# Add src to path\n",
    "repo_dir = Path.cwd().parent\n",
    "sys.path.append(str(repo_dir / 'src'))\n",
    "\n",
    "# # Import helper modules moved from notebook\n",
    "# from data_processing_helpers import load_permutation_data, get_available_permutations, extract_improved_edge_features_and_labels\n",
    "# from optimized_model import DistributionAwareNN, OptimizedModelTrainer\n",
    "# from unique_sampling import UniqueEdgeSampler\n",
    "# from validation_utils import compute_adaptive_degree_based_probability_distribution, compute_enhanced_distribution_difference\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Repository directory: {repo_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51a61f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papermill Parameters:\n",
      "  edge_type: AeG\n",
      "  max_permutations: 2\n",
      "  validation_networks: 3\n",
      "  convergence_threshold: 0.2\n",
      "  n_bins: 8\n",
      "  random_seed: 42\n",
      "  model_types: ['NN', 'RF']\n",
      "  use_distribution_loss: True\n",
      "  use_adaptive_binning: True\n",
      "  early_stopping_patience: 3\n",
      "  relative_improvement_threshold: 0.02\n",
      "  use_relative_convergence: True\n",
      "\n",
      "Directories:\n",
      "  Data: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data\n",
      "  Permutations: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/permutations\n",
      "  Downloads: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/downloads\n",
      "  Output: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/results/minimum_permutations_basic_2d\n"
     ]
    }
   ],
   "source": [
    "# Papermill parameters cell for reproducible runs\n",
    "# Parameters set by papermill or default values for interactive use\n",
    "\n",
    "# Parameters (these will be overridden by papermill if run as a pipeline)\n",
    "edge_type = os.environ.get('EDGE_TYPE', 'AeG')  # Compound-treats-Disease\n",
    "max_permutations = int(os.environ.get('MAX_PERMUTATIONS', 2))\n",
    "validation_networks = int(os.environ.get('VALIDATION_NETWORKS', 3))\n",
    "convergence_threshold = float(os.environ.get('CONVERGENCE_THRESHOLD', 0.2))\n",
    "n_bins = int(os.environ.get('N_BINS', 8))\n",
    "negative_sampling_ratio = float(os.environ.get('NEGATIVE_SAMPLING_RATIO', 0.5))\n",
    "random_seed = int(os.environ.get('RANDOM_SEED', 42))\n",
    "model_types = os.environ.get('MODEL_TYPES', 'NN,RF').split(',')\n",
    "use_normalized_features = os.environ.get('USE_NORMALIZED_FEATURES', 'False') == 'True'\n",
    "use_regression_approach = os.environ.get('USE_REGRESSION_APPROACH', 'True') == 'True'\n",
    "use_distribution_loss = os.environ.get('USE_DISTRIBUTION_LOSS', 'True') == 'True'\n",
    "use_adaptive_binning = os.environ.get('USE_ADAPTIVE_BINNING', 'True') == 'True'\n",
    "use_ensemble_methods = os.environ.get('USE_ENSEMBLE_METHODS', 'True') == 'True'\n",
    "early_stopping_patience = int(os.environ.get('EARLY_STOPPING_PATIENCE', 3))\n",
    "relative_improvement_threshold = float(os.environ.get('RELATIVE_IMPROVEMENT_THRESHOLD', 0.02))\n",
    "use_relative_convergence = os.environ.get('USE_RELATIVE_CONVERGENCE', 'True') == 'True'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Directory setup\n",
    "repo_dir = Path.cwd().parent\n",
    "\n",
    "# These are the same as before, but now use the parameters above\n",
    "data_dir = repo_dir / 'data'\n",
    "permutations_dir = data_dir / 'permutations'\n",
    "downloads_dir = data_dir / 'downloads'\n",
    "models_dir = repo_dir / 'models'\n",
    "output_dir = repo_dir / 'results' / 'minimum_permutations_basic_2d'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Papermill Parameters:\")\n",
    "print(f\"  edge_type: {edge_type}\")\n",
    "print(f\"  max_permutations: {max_permutations}\")\n",
    "print(f\"  validation_networks: {validation_networks}\")\n",
    "print(f\"  convergence_threshold: {convergence_threshold}\")\n",
    "print(f\"  n_bins: {n_bins}\")\n",
    "print(f\"  random_seed: {random_seed}\")\n",
    "print(f\"  model_types: {model_types}\")\n",
    "print(f\"  use_distribution_loss: {use_distribution_loss}\")\n",
    "print(f\"  use_adaptive_binning: {use_adaptive_binning}\")\n",
    "print(f\"  early_stopping_patience: {early_stopping_patience}\")\n",
    "print(f\"  relative_improvement_threshold: {relative_improvement_threshold}\")\n",
    "print(f\"  use_relative_convergence: {use_relative_convergence}\")\n",
    "print(f\"\\nDirectories:\")\n",
    "print(f\"  Data: {data_dir}\")\n",
    "print(f\"  Permutations: {permutations_dir}\")\n",
    "print(f\"  Downloads: {downloads_dir}\")\n",
    "print(f\"  Output: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6921d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source_degree  target_degree  empirical_frequency\n",
      "0           7939             20             0.272037\n",
      "1           7939             75             0.979326\n",
      "2           7939             38             0.627020\n",
      "3           7939             56             0.874961\n",
      "4           7939             67             0.954649\n"
     ]
    }
   ],
   "source": [
    "empirical_freq_df = pd.read_csv('../results/edge_frequency_by_degree.csv')\n",
    "empirical_freq_df = empirical_freq_df.rename(columns={'frequency': 'empirical_frequency'})\n",
    "print(empirical_freq_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a569eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data directories...\n",
      "Original data directory: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data\n",
      "Permutations directory: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/permutations\n",
      "Found 2 permutation directories:\n",
      "  1. 000.hetmat\n",
      "  2. 001.hetmat\n",
      "✅ Sufficient permutations available for experiment\n",
      "✅ Original edge data found: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/edges/AeG.sparse.npz\n",
      "\n",
      "Directory setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup data directories and paths\n",
    "print(\"Setting up data directories...\")\n",
    "\n",
    "# Original data directory (main hetionet data)\n",
    "original_data_dir = data_dir  # Main data directory contains the original network\n",
    "\n",
    "# Find available permutation directories\n",
    "available_permutations = []\n",
    "if permutations_dir.exists():\n",
    "    for perm_dir in permutations_dir.iterdir():\n",
    "        if perm_dir.is_dir() and perm_dir.name.endswith('.hetmat'):\n",
    "            available_permutations.append(perm_dir)\n",
    "\n",
    "# Sort permutations by name to ensure consistent ordering\n",
    "permutations_dirs = sorted(available_permutations)\n",
    "\n",
    "print(f\"Original data directory: {original_data_dir}\")\n",
    "print(f\"Permutations directory: {permutations_dir}\")\n",
    "print(f\"Found {len(permutations_dirs)} permutation directories:\")\n",
    "for i, perm_dir in enumerate(permutations_dirs[:5]):  # Show first 5\n",
    "    print(f\"  {i+1}. {perm_dir.name}\")\n",
    "if len(permutations_dirs) > 5:\n",
    "    print(f\"  ... and {len(permutations_dirs) - 5} more\")\n",
    "\n",
    "# Validate we have enough permutations for the experiment\n",
    "if len(permutations_dirs) < max_permutations:\n",
    "    print(f\"⚠️  Warning: Only {len(permutations_dirs)} permutations available, but max_permutations = {max_permutations}\")\n",
    "    print(\"   Will reuse permutations if needed.\")\n",
    "else:\n",
    "    print(f\"✅ Sufficient permutations available for experiment\")\n",
    "\n",
    "# Check if original data exists\n",
    "original_edge_file = original_data_dir / 'edges' / f\"{edge_type}.sparse.npz\"\n",
    "if original_edge_file.exists():\n",
    "    print(f\"✅ Original edge data found: {original_edge_file}\")\n",
    "else:\n",
    "    print(f\"❌ Original edge data not found: {original_edge_file}\")\n",
    "    print(\"Available edge files:\")\n",
    "    if (original_data_dir / 'edges').exists():\n",
    "        for edge_file in (original_data_dir / 'edges').iterdir():\n",
    "            if edge_file.suffix == '.npz':\n",
    "                print(f\"  - {edge_file.name}\")\n",
    "\n",
    "print(\"\\nDirectory setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bccc635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Combining permutation 1 | Using permutation: 000.hetmat ===\n",
      "Loading data from permutation: 000.hetmat\n",
      "Permutation path: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/permutations/000.hetmat\n",
      "Edge type: AeG (Anatomy -> Gene)\n",
      "Loaded AeG edges: (402, 20945) matrix with 526407 non-zero entries\n",
      "Loaded Anatomy nodes: 402 nodes\n",
      "Anatomy columns: ['position', 'identifier', 'name']\n",
      "Loaded Gene nodes: 20945 nodes\n",
      "Gene columns: ['position', 'identifier', 'name']\n",
      "Preparing AeG edge prediction data (Anatomy -> Gene)\n",
      "Anatomy degree range: 0 - 15036\n",
      "Gene degree range: 0 - 98\n",
      "Number of positive examples (existing edges): 526407\n",
      "Number of positive examples (existing edges): 526407\n",
      "Number of negative examples (non-existing edges): 526407\n",
      "Shape of X: (1052814, 2)\n",
      "Combined dataset size: 1052814 samples\n",
      "Predicted probabilities for 1052814 edges.\n",
      "(13167, 4)\n",
      "T-statistic: -78.3737010429412\n",
      "P-value: 0.0\n",
      "The difference between predicted probabilities and empirical frequencies is statistically significant.\n",
      "\n",
      "=== Combining permutation 2 | Using permutation: 001.hetmat ===\n",
      "Loading data from permutation: 001.hetmat\n",
      "Permutation path: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/permutations/001.hetmat\n",
      "Edge type: AeG (Anatomy -> Gene)\n",
      "Loaded AeG edges: (402, 20945) matrix with 526407 non-zero entries\n",
      "Loaded Anatomy nodes: 402 nodes\n",
      "Anatomy columns: ['position', 'identifier', 'name']\n",
      "Loaded Gene nodes: 20945 nodes\n",
      "Gene columns: ['position', 'identifier', 'name']\n",
      "Preparing AeG edge prediction data (Anatomy -> Gene)\n",
      "Anatomy degree range: 0 - 15036\n",
      "Gene degree range: 0 - 98\n",
      "Number of negative examples (non-existing edges): 526407\n",
      "Shape of X: (1052814, 2)\n",
      "Combined dataset size: 1052814 samples\n",
      "Predicted probabilities for 1052814 edges.\n",
      "(13167, 4)\n",
      "T-statistic: -78.3737010429412\n",
      "P-value: 0.0\n",
      "The difference between predicted probabilities and empirical frequencies is statistically significant.\n",
      "\n",
      "=== Combining permutation 2 | Using permutation: 001.hetmat ===\n",
      "Loading data from permutation: 001.hetmat\n",
      "Permutation path: /Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability/data/permutations/001.hetmat\n",
      "Edge type: AeG (Anatomy -> Gene)\n",
      "Loaded AeG edges: (402, 20945) matrix with 526407 non-zero entries\n",
      "Loaded Anatomy nodes: 402 nodes\n",
      "Anatomy columns: ['position', 'identifier', 'name']\n",
      "Loaded Gene nodes: 20945 nodes\n",
      "Gene columns: ['position', 'identifier', 'name']\n",
      "Preparing AeG edge prediction data (Anatomy -> Gene)\n",
      "Anatomy degree range: 0 - 15036\n",
      "Gene degree range: 0 - 98\n",
      "Number of positive examples (existing edges): 526407\n",
      "Number of positive examples (existing edges): 526407\n",
      "Number of negative examples (non-existing edges): 526407\n",
      "Shape of X: (1052814, 2)\n",
      "Combined dataset size: 2105628 samples\n",
      "Number of negative examples (non-existing edges): 526407\n",
      "Shape of X: (1052814, 2)\n",
      "Combined dataset size: 2105628 samples\n",
      "Predicted probabilities for 2105628 edges.\n",
      "(13167, 4)\n",
      "T-statistic: 194.37121364676221\n",
      "P-value: 0.0\n",
      "The difference between predicted probabilities and empirical frequencies is statistically significant.\n",
      "Predicted probabilities for 2105628 edges.\n",
      "(13167, 4)\n",
      "T-statistic: 194.37121364676221\n",
      "P-value: 0.0\n",
      "The difference between predicted probabilities and empirical frequencies is statistically significant.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Combine permutations iteratively to create larger datasets\n",
    "combined_data = None\n",
    "combined_labels = None\n",
    "\n",
    "for idx, perm_dir in enumerate(permutations_dirs[:max_permutations]):\n",
    "    print(f\"\\n=== Combining permutation {idx+1} | Using permutation: {perm_dir.name} ===\")\n",
    "    # Load permutation data\n",
    "    perm_data = load_permutation_data(\n",
    "        permutation_name=perm_dir.name,\n",
    "        permutations_dir=permutations_dir,\n",
    "        edge_type=edge_type,\n",
    "        source_node_type='Anatomy',  # adjust as needed\n",
    "        target_node_type='Gene'      # adjust as needed\n",
    "    )\n",
    "    \n",
    "    # Compute source and target degrees\n",
    "    source_degrees = np.array(perm_data['edges'].sum(axis=1)).flatten()\n",
    "    target_degrees = np.array(perm_data['edges'].sum(axis=0)).flatten()\n",
    "    perm_data['source_degrees'] = source_degrees\n",
    "    perm_data['target_degrees'] = target_degrees\n",
    "\n",
    "\n",
    "    # Prepare features and labels\n",
    "    X, y = prepare_edge_prediction_data(perm_data, sample_negative_ratio=1)\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "\n",
    "    # Combine with existing data\n",
    "    if combined_data is None:\n",
    "        combined_data = X\n",
    "        combined_labels = y\n",
    "        source_degrees_combined = source_degrees\n",
    "        target_degrees_combined = target_degrees\n",
    "    else:\n",
    "        combined_data = np.vstack([combined_data, X])\n",
    "        combined_labels = np.hstack([combined_labels, y])\n",
    "        combined_source_degrees = np.hstack([source_degrees_combined, source_degrees])\n",
    "        combined_target_degrees = np.hstack([target_degrees_combined, target_degrees])\n",
    "\n",
    "    print(f\"Combined dataset size: {combined_data.shape[0]} samples\")\n",
    "\n",
    "    # Predict edge probabilities using EdgePredictionNN\n",
    "    model = EdgePredictionNN(\n",
    "        input_dim=combined_data.shape[1],\n",
    "        hidden_dims=[64, 32],  # adjust as needed\n",
    "        dropout_rate=0.2      # adjust as needed\n",
    "    )\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(combined_data, dtype=torch.float32)\n",
    "        predictions = model(X_tensor).numpy()\n",
    "    print(f\"Predicted probabilities for {len(predictions)} edges.\")\n",
    "\n",
    "    # Create a DataFrame for predictions with source and target degrees\n",
    "    prediction_df = pd.DataFrame({\n",
    "        'source_degree': combined_data[:, 0],\n",
    "        'target_degree': combined_data[:, 1],\n",
    "        'predicted_probability': predictions.flatten()\n",
    "    })\n",
    "\n",
    "    # Group by source and target degree and compute mean predicted probability\n",
    "    predicted_means = prediction_df.groupby(['source_degree', 'target_degree'])['predicted_probability'].mean().reset_index()\n",
    "\n",
    "    # Align predicted probabilities with empirical frequencies\n",
    "    comparison_df = empirical_freq_df.merge(predicted_means, on=['source_degree', 'target_degree'], how='left')\n",
    "\n",
    "    # Print comparison\n",
    "    print(comparison_df.shape)\n",
    "\n",
    "    # Perform paired t-test between predicted probabilities and empirical frequencies\n",
    "    valid_comparison_df = comparison_df.dropna(subset=['predicted_probability'])\n",
    "    t_stat, p_value = ttest_rel(valid_comparison_df['predicted_probability'], valid_comparison_df['empirical_frequency'])\n",
    "\n",
    "    print(f\"T-statistic: {t_stat}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "\n",
    "    # Interpret the result\n",
    "    if p_value < 0.05:\n",
    "        print(\"The difference between predicted probabilities and empirical frequencies is statistically significant.\")\n",
    "    else:\n",
    "        print(\"The difference between predicted probabilities and empirical frequencies is not statistically significant.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAPP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
