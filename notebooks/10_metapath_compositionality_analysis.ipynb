{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metapath Compositionality Analysis\n",
    "\n",
    "## Research Question\n",
    "\n",
    "**Are metapath edge probabilities compositional (independent) or conditional (dependent)?**\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "For a metapath like **CbG → GpPW** (Compound binds Gene, Gene participates in Pathway):\n",
    "\n",
    "- **Compositional (H0)**: P(CbG ∩ GpPW) = P(CbG) × P(GpPW)\n",
    "  - Edge probabilities are independent\n",
    "  - Simple multiplication of marginal probabilities\n",
    "  - Analytical prior should work well\n",
    "\n",
    "- **Conditional (H1)**: P(CbG ∩ GpPW) ≠ P(CbG) × P(GpPW)\n",
    "  - Edge probabilities are conditionally dependent\n",
    "  - P(GpPW | CbG) ≠ P(GpPW)\n",
    "  - Need learned/empirical priors\n",
    "\n",
    "### Measurement\n",
    "\n",
    "We use **Pointwise Mutual Information (PMI)** to quantify dependency:\n",
    "\n",
    "```\n",
    "PMI(edge1, edge2) = log₂(P(edge1, edge2) / (P(edge1) × P(edge2)))\n",
    "```\n",
    "\n",
    "- **PMI = 0**: Independent (compositional)\n",
    "- **PMI > 0**: Positive association (conditional)\n",
    "- **PMI < 0**: Negative association (anti-correlation)\n",
    "\n",
    "### Test Metapath\n",
    "\n",
    "**CbGpPW**: Compound → binds Gene → participates in Pathway\n",
    "\n",
    "We compare:\n",
    "1. **Observed**: Actual metapath frequencies in Hetionet\n",
    "2. **Compositional**: Analytical prior (independent edge probabilities)\n",
    "3. **Learned** (optional): Learned formula or empirical priors from N permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd().parent\n",
    "src_dir = repo_dir / 'src'\n",
    "data_dir = repo_dir / 'data'\n",
    "results_dir = repo_dir / 'results'\n",
    "\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "print(f\"Repository: {repo_dir}\")\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metapath: CbGpPW\n",
    "metapath = ['CbG', 'GpPW']\n",
    "metapath_name = 'CbGpPW'\n",
    "\n",
    "# Source type: Compound\n",
    "# Intermediate type: Gene\n",
    "# Target type: Pathway\n",
    "\n",
    "print(f\"Testing metapath: {metapath_name}\")\n",
    "print(f\"  Edge 1: {metapath[0]} (Compound → Gene)\")\n",
    "print(f\"  Edge 2: {metapath[1]} (Gene → Pathway)\")\n",
    "\n",
    "# Prior options (can be swapped)\n",
    "PRIOR_TYPE = 'analytical'  # Options: 'analytical', 'learned', 'empirical'\n",
    "\n",
    "print(f\"\\nUsing prior: {PRIOR_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_edge_matrix(edge_type: str, perm_id: int = 0) -> sp.csr_matrix:\n    \"\"\"Load edge matrix for given edge type and permutation.\"\"\"\n    edge_file = data_dir / 'permutations' / f'{perm_id:03d}.hetmat' / 'edges' / f'{edge_type}.sparse.npz'\n    return sp.load_npz(edge_file)\n\ndef filter_zero_degrees(matrix: sp.csr_matrix):\n    \"\"\"Remove nodes with zero degree.\"\"\"\n    # Get degrees\n    source_degrees = np.array(matrix.sum(axis=1)).flatten()\n    target_degrees = np.array(matrix.sum(axis=0)).flatten()\n    \n    # Find non-zero indices\n    source_nonzero = np.where(source_degrees > 0)[0]\n    target_nonzero = np.where(target_degrees > 0)[0]\n    \n    # Filter matrix\n    filtered = matrix[source_nonzero, :][:, target_nonzero]\n    \n    return filtered, source_nonzero, target_nonzero\n\ndef analytical_prior(u: float, v: float, m: float) -> float:\n    \"\"\"Current analytical formula for edge probability.\"\"\"\n    uv = u * v\n    denominator = np.sqrt(uv**2 + (m - u - v + 1)**2)\n    return uv / denominator if denominator > 0 else 0.0\n\ndef compute_edge_priors_sparse(edge_matrix: sp.csr_matrix, prior_type: str = 'analytical'):\n    \"\"\"\n    Compute prior probabilities ONLY for existing edges (sparse).\n    \n    Parameters\n    ----------\n    edge_matrix : sparse matrix\n        Edge matrix (sources × targets)\n    prior_type : str\n        'analytical', 'learned', or 'empirical'\n    \n    Returns\n    -------\n    priors : dict\n        {(source_idx, target_idx): probability}\n    \"\"\"\n    n_sources, n_targets = edge_matrix.shape\n    m = edge_matrix.nnz\n    \n    # Get degrees\n    source_degrees = np.array(edge_matrix.sum(axis=1)).flatten()\n    target_degrees = np.array(edge_matrix.sum(axis=0)).flatten()\n    \n    priors = {}\n    \n    if prior_type == 'analytical':\n        # Analytical prior - only for existing edges\n        rows, cols = edge_matrix.nonzero()\n        for i, j in zip(rows, cols):\n            u, v = source_degrees[i], target_degrees[j]\n            if u > 0 and v > 0:\n                priors[(i, j)] = analytical_prior(u, v, m)\n    \n    elif prior_type == 'learned':\n        # TODO: Load learned formula and compute priors\n        # For now, fallback to analytical\n        print(\"WARNING: Learned prior not yet implemented, using analytical\")\n        return compute_edge_priors_sparse(edge_matrix, 'analytical')\n    \n    elif prior_type == 'empirical':\n        # Empirical prior from permutations\n        # TODO: Load from empirical frequency files\n        print(\"WARNING: Empirical prior not yet implemented, using analytical\")\n        return compute_edge_priors_sparse(edge_matrix, 'analytical')\n    \n    return priors"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hetionet Data (Observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load Hetionet (permutation 000)\nprint(\"Loading Hetionet edge matrices...\\n\")\n\nedge1_matrix = load_edge_matrix(metapath[0], perm_id=0)\nedge2_matrix = load_edge_matrix(metapath[1], perm_id=0)\n\n# Filter zero degrees\nedge1_filtered, edge1_source_map, edge1_target_map = filter_zero_degrees(edge1_matrix)\nedge2_filtered, edge2_source_map, edge2_target_map = filter_zero_degrees(edge2_matrix)\n\nprint(f\"{metapath[0]} (Compound → Gene):\")\nprint(f\"  Original shape: {edge1_matrix.shape}\")\nprint(f\"  Filtered shape: {edge1_filtered.shape}\")\nprint(f\"  Edges: {edge1_filtered.nnz}\")\nprint(f\"  Density: {edge1_filtered.nnz / (edge1_filtered.shape[0] * edge1_filtered.shape[1]):.6f}\")\n\nprint(f\"\\n{metapath[1]} (Gene → Pathway):\")\nprint(f\"  Original shape: {edge2_matrix.shape}\")\nprint(f\"  Filtered shape: {edge2_filtered.shape}\")\nprint(f\"  Edges: {edge2_filtered.nnz}\")\nprint(f\"  Density: {edge2_filtered.nnz / (edge2_filtered.shape[0] * edge2_filtered.shape[1]):.6f}\")\n\n# Align gene dimensions - use ORIGINAL unfiltered matrices\n# Both matrices should have the same gene dimension in the original data\nprint(f\"\\nAligning to common gene space (using original dimensions)...\")\nprint(f\"  CbG genes (columns): {edge1_matrix.shape[1]}\")\nprint(f\"  GpPW genes (rows): {edge2_matrix.shape[0]}\")\n\n# Use original matrices for metapath computation\nassert edge1_matrix.shape[1] == edge2_matrix.shape[0], \"Gene dimension mismatch in original matrices!\"\n\n# Filter only source (Compound) and target (Pathway) nodes with zero degree\n# Keep all genes to maintain alignment\ncompound_degrees = np.array(edge1_matrix.sum(axis=1)).flatten()\npathway_degrees = np.array(edge2_matrix.sum(axis=0)).flatten()\n\ncompound_nonzero = np.where(compound_degrees > 0)[0]\npathway_nonzero = np.where(pathway_degrees > 0)[0]\n\n# Filter only compounds and pathways, keep all genes\nedge1_aligned = edge1_matrix[compound_nonzero, :]\nedge2_aligned = edge2_matrix[:, pathway_nonzero]\n\nn_compounds = edge1_aligned.shape[0]\nn_genes = edge1_aligned.shape[1]\nn_pathways = edge2_aligned.shape[1]\n\nprint(f\"\\nAligned matrices:\")\nprint(f\"  CbG: {edge1_aligned.shape} (Compounds × Genes)\")\nprint(f\"  GpPW: {edge2_aligned.shape} (Genes × Pathways)\")\nprint(f\"\\nNode counts:\")\nprint(f\"  Compounds: {n_compounds}\")\nprint(f\"  Genes: {n_genes}\")\nprint(f\"  Pathways: {n_pathways}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metapath Frequencies (Observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute metapath matrix: Compound → Gene → Pathway\nprint(\"Computing metapath matrix...\")\n\n# Metapath matrix = edge1 × edge2\nmetapath_matrix = edge1_aligned @ edge2_aligned\n\nprint(f\"\\nMetapath matrix {metapath_name}:\")\nprint(f\"  Shape: {metapath_matrix.shape} (Compounds × Pathways)\")\nprint(f\"  Metapaths: {metapath_matrix.nnz}\")\nprint(f\"  Max count: {metapath_matrix.max()}\")\nprint(f\"  Mean count (nonzero): {metapath_matrix.data.mean():.2f}\")\n\n# Compute observed frequencies\n# For each (compound, pathway) pair, normalize by number of possible paths\nprint(\"\\nComputing observed metapath frequencies...\")\n\nobserved_freq = {}\n\nfor i, j in zip(*metapath_matrix.nonzero()):\n    # Get compound i's gene neighbors\n    compound_genes = edge1_aligned.getrow(i).nonzero()[1]\n    \n    # Get pathway j's gene neighbors  \n    pathway_genes = edge2_aligned.getcol(j).nonzero()[0]\n    \n    # Count shared genes (actual metapaths)\n    shared_genes = set(compound_genes) & set(pathway_genes)\n    n_paths = len(shared_genes)\n    \n    # Total possible paths = all genes that connect compound to ANY pathway\n    n_possible = len(compound_genes)\n    \n    if n_possible > 0:\n        observed_freq[(i, j)] = n_paths / n_possible\n\nprint(f\"Observed frequencies computed for {len(observed_freq)} (compound, pathway) pairs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Compositional Probabilities (Prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Computing compositional probabilities using {PRIOR_TYPE} prior...\\n\")\n\n# Compute priors for each edge type (sparse - only existing edges)\nedge1_priors = compute_edge_priors_sparse(edge1_aligned, PRIOR_TYPE)\nedge2_priors = compute_edge_priors_sparse(edge2_aligned, PRIOR_TYPE)\n\nprint(f\"Computed {len(edge1_priors)} priors for {metapath[0]}\")\nprint(f\"Computed {len(edge2_priors)} priors for {metapath[1]}\")\n\n# Compute compositional metapath probabilities\n# P(compound → pathway) = Σ_gene P(compound → gene) × P(gene → pathway)\nprint(\"\\nComputing compositional metapath probabilities...\")\n\ncompositional_prob = {}\n\nfor i in range(n_compounds):\n    # Get genes connected to this compound\n    compound_genes = edge1_aligned.getrow(i).nonzero()[1]\n    \n    for j in range(n_pathways):\n        # Get genes connected to this pathway\n        pathway_genes = edge2_aligned.getcol(j).nonzero()[0]\n        \n        # Sum over all possible intermediate genes\n        total_prob = 0.0\n        \n        for gene in set(compound_genes) & set(pathway_genes):\n            # Compositional assumption: independent\n            p_edge1 = edge1_priors.get((i, gene), 0.0)\n            p_edge2 = edge2_priors.get((gene, j), 0.0)\n            total_prob += p_edge1 * p_edge2\n        \n        if total_prob > 0:\n            compositional_prob[(i, j)] = total_prob\n\nprint(f\"Compositional probabilities computed for {len(compositional_prob)} (compound, pathway) pairs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Pointwise Mutual Information (PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing Pointwise Mutual Information (PMI)...\\n\")\n",
    "\n",
    "# Align observed and compositional probabilities\n",
    "common_pairs = set(observed_freq.keys()) & set(compositional_prob.keys())\n",
    "\n",
    "print(f\"Common (compound, pathway) pairs: {len(common_pairs)}\")\n",
    "\n",
    "pmi_values = {}\n",
    "observed_vals = []\n",
    "compositional_vals = []\n",
    "\n",
    "for pair in common_pairs:\n",
    "    p_observed = observed_freq[pair]\n",
    "    p_compositional = compositional_prob[pair]\n",
    "    \n",
    "    # PMI = log₂(P(observed) / P(compositional))\n",
    "    if p_observed > 0 and p_compositional > 0:\n",
    "        pmi = np.log2(p_observed / p_compositional)\n",
    "        pmi_values[pair] = pmi\n",
    "        observed_vals.append(p_observed)\n",
    "        compositional_vals.append(p_compositional)\n",
    "\n",
    "pmi_array = np.array(list(pmi_values.values()))\n",
    "\n",
    "print(f\"\\nPMI Statistics:\")\n",
    "print(f\"  Mean PMI: {pmi_array.mean():.4f}\")\n",
    "print(f\"  Median PMI: {np.median(pmi_array):.4f}\")\n",
    "print(f\"  Std PMI: {pmi_array.std():.4f}\")\n",
    "print(f\"  Min PMI: {pmi_array.min():.4f}\")\n",
    "print(f\"  Max PMI: {pmi_array.max():.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if pmi_array.mean() > 0.5:\n",
    "    print(f\"  ✓ STRONG CONDITIONAL DEPENDENCY (mean PMI = {pmi_array.mean():.4f} > 0.5)\")\n",
    "    print(f\"    → Metapath probabilities are NOT compositional\")\n",
    "    print(f\"    → P(GpPW | CbG) ≠ P(GpPW)\")\n",
    "    print(f\"    → Need learned/empirical priors\")\n",
    "elif pmi_array.mean() > 0.1:\n",
    "    print(f\"  → MODERATE CONDITIONAL DEPENDENCY (mean PMI = {pmi_array.mean():.4f})\")\n",
    "    print(f\"    → Some conditional structure present\")\n",
    "elif abs(pmi_array.mean()) < 0.1:\n",
    "    print(f\"  ✓ APPROXIMATELY INDEPENDENT (mean PMI ≈ {pmi_array.mean():.4f} ≈ 0)\")\n",
    "    print(f\"    → Metapath probabilities are compositional\")\n",
    "    print(f\"    → P(GpPW | CbG) ≈ P(GpPW)\")\n",
    "    print(f\"    → Analytical prior works well\")\n",
    "else:\n",
    "    print(f\"  → NEGATIVE ASSOCIATION (mean PMI = {pmi_array.mean():.4f} < 0)\")\n",
    "    print(f\"    → Anti-correlation between edge types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations\n",
    "observed_vals = np.array(observed_vals)\n",
    "compositional_vals = np.array(compositional_vals)\n",
    "\n",
    "pearson_r, pearson_p = pearsonr(observed_vals, compositional_vals)\n",
    "spearman_r, spearman_p = spearmanr(observed_vals, compositional_vals)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORRELATION BETWEEN OBSERVED AND COMPOSITIONAL PROBABILITIES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPearson correlation: r = {pearson_r:.4f} (p = {pearson_p:.2e})\")\n",
    "print(f\"Spearman correlation: ρ = {spearman_r:.4f} (p = {spearman_p:.2e})\")\n",
    "\n",
    "# Compute MAE and RMSE\n",
    "mae = np.mean(np.abs(observed_vals - compositional_vals))\n",
    "rmse = np.sqrt(np.mean((observed_vals - compositional_vals)**2))\n",
    "\n",
    "print(f\"\\nMean Absolute Error: {mae:.6f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.6f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "if pearson_r > 0.9:\n",
    "    print(f\"  ✓ EXCELLENT FIT (r = {pearson_r:.4f} > 0.9)\")\n",
    "    print(f\"    → Compositional model works very well\")\n",
    "    print(f\"    → Analytical prior is appropriate\")\n",
    "elif pearson_r > 0.7:\n",
    "    print(f\"  → GOOD FIT (r = {pearson_r:.4f} > 0.7)\")\n",
    "    print(f\"    → Compositional model captures most variance\")\n",
    "    print(f\"    → But some conditional structure exists\")\n",
    "elif pearson_r > 0.5:\n",
    "    print(f\"  → MODERATE FIT (r = {pearson_r:.4f})\")\n",
    "    print(f\"    → Significant conditional dependencies\")\n",
    "    print(f\"    → Consider learned/empirical priors\")\n",
    "else:\n",
    "    print(f\"  ✗ POOR FIT (r = {pearson_r:.4f} < 0.5)\")\n",
    "    print(f\"    → Strong conditional dependencies\")\n",
    "    print(f\"    → Compositional assumption fails\")\n",
    "    print(f\"    → Must use learned/empirical priors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# Plot 1: Observed vs Compositional scatter\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(compositional_vals, observed_vals, alpha=0.5, s=20, edgecolors='black', linewidths=0.5)\n",
    "ax.plot([0, max(compositional_vals)], [0, max(compositional_vals)], 'r--', linewidth=2, label='Perfect composition')\n",
    "ax.set_xlabel(f'Compositional Probability ({PRIOR_TYPE} prior)', fontsize=12)\n",
    "ax.set_ylabel('Observed Frequency (Hetionet)', fontsize=12)\n",
    "ax.set_title(f'{metapath_name}: Observed vs Compositional\\nr = {pearson_r:.4f}', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: PMI distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(pmi_array, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='PMI = 0 (independent)')\n",
    "ax.axvline(pmi_array.mean(), color='orange', linestyle='-', linewidth=2, label=f'Mean PMI = {pmi_array.mean():.2f}')\n",
    "ax.set_xlabel('Pointwise Mutual Information (PMI)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title(f'{metapath_name}: PMI Distribution', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals\n",
    "ax = axes[1, 0]\n",
    "residuals = observed_vals - compositional_vals\n",
    "ax.scatter(compositional_vals, residuals, alpha=0.5, s=20, edgecolors='black', linewidths=0.5)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel(f'Compositional Probability ({PRIOR_TYPE} prior)', fontsize=12)\n",
    "ax.set_ylabel('Residual (Observed - Compositional)', fontsize=12)\n",
    "ax.set_title(f'{metapath_name}: Residuals\\nMAE = {mae:.6f}, RMSE = {rmse:.6f}', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: PMI vs Compositional probability\n",
    "ax = axes[1, 1]\n",
    "pmi_list = [pmi_values[pair] for pair in common_pairs if pair in pmi_values]\n",
    "ax.scatter(compositional_vals, pmi_list, alpha=0.5, s=20, edgecolors='black', linewidths=0.5, c=observed_vals, cmap='viridis')\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=2, label='PMI = 0')\n",
    "ax.set_xlabel(f'Compositional Probability ({PRIOR_TYPE} prior)', fontsize=12)\n",
    "ax.set_ylabel('PMI', fontsize=12)\n",
    "ax.set_title(f'{metapath_name}: PMI vs Compositional Probability', fontsize=14, fontweight='bold')\n",
    "cbar = plt.colorbar(ax.collections[0], ax=ax)\n",
    "cbar.set_label('Observed Frequency', fontsize=10)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / f'metapath_{metapath_name}_compositionality_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nSaved plot to: {results_dir / f'metapath_{metapath_name}_compositionality_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_data = []\n",
    "\n",
    "for pair in common_pairs:\n",
    "    i, j = pair\n",
    "    results_data.append({\n",
    "        'compound_idx': i,\n",
    "        'pathway_idx': j,\n",
    "        'observed_freq': observed_freq[pair],\n",
    "        'compositional_prob': compositional_prob[pair],\n",
    "        'pmi': pmi_values.get(pair, np.nan),\n",
    "        'residual': observed_freq[pair] - compositional_prob[pair]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = results_dir / f'metapath_{metapath_name}_compositionality_results.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to: {output_file}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary = {\n",
    "    'metapath': metapath_name,\n",
    "    'prior_type': PRIOR_TYPE,\n",
    "    'n_pairs': len(common_pairs),\n",
    "    'pearson_r': pearson_r,\n",
    "    'pearson_p': pearson_p,\n",
    "    'spearman_r': spearman_r,\n",
    "    'spearman_p': spearman_p,\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "    'mean_pmi': pmi_array.mean(),\n",
    "    'median_pmi': np.median(pmi_array),\n",
    "    'std_pmi': pmi_array.std(),\n",
    "    'min_pmi': pmi_array.min(),\n",
    "    'max_pmi': pmi_array.max()\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_file = results_dir / f'metapath_{metapath_name}_compositionality_summary.csv'\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "print(f\"Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nMetapath: {metapath_name}\")\n",
    "print(f\"Prior type: {PRIOR_TYPE}\")\n",
    "\n",
    "print(f\"\\n1. COMPOSITIONALITY TEST:\")\n",
    "if abs(pmi_array.mean()) < 0.1:\n",
    "    print(f\"   ✓ COMPOSITIONAL (mean PMI ≈ 0)\")\n",
    "    print(f\"   → Edge probabilities are approximately independent\")\n",
    "else:\n",
    "    print(f\"   ✗ NON-COMPOSITIONAL (mean PMI = {pmi_array.mean():.4f})\")\n",
    "    print(f\"   → Edge probabilities exhibit conditional dependencies\")\n",
    "\n",
    "print(f\"\\n2. PRIOR PERFORMANCE:\")\n",
    "if pearson_r > 0.9:\n",
    "    print(f\"   ✓ EXCELLENT (r = {pearson_r:.4f})\")\n",
    "    print(f\"   → {PRIOR_TYPE.capitalize()} prior works very well\")\n",
    "elif pearson_r > 0.7:\n",
    "    print(f\"   ✓ GOOD (r = {pearson_r:.4f})\")\n",
    "    print(f\"   → {PRIOR_TYPE.capitalize()} prior captures most structure\")\n",
    "elif pearson_r > 0.5:\n",
    "    print(f\"   → MODERATE (r = {pearson_r:.4f})\")\n",
    "    print(f\"   → {PRIOR_TYPE.capitalize()} prior has limitations\")\n",
    "else:\n",
    "    print(f\"   ✗ POOR (r = {pearson_r:.4f})\")\n",
    "    print(f\"   → {PRIOR_TYPE.capitalize()} prior fails to capture structure\")\n",
    "\n",
    "print(f\"\\n3. RECOMMENDATIONS:\")\n",
    "if pearson_r > 0.9 and abs(pmi_array.mean()) < 0.1:\n",
    "    print(f\"   → Continue using {PRIOR_TYPE} prior\")\n",
    "    print(f\"   → Compositional assumption is valid\")\n",
    "    print(f\"   → No need for learned/empirical priors\")\n",
    "elif pearson_r > 0.7:\n",
    "    print(f\"   → {PRIOR_TYPE.capitalize()} prior is reasonable baseline\")\n",
    "    print(f\"   → Consider testing learned/empirical priors for improvement\")\n",
    "    print(f\"   → Some conditional structure could be captured\")\n",
    "else:\n",
    "    print(f\"   → Switch to learned/empirical priors\")\n",
    "    print(f\"   → Strong conditional dependencies detected\")\n",
    "    print(f\"   → Compositional assumption violated\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Swap Priors\n",
    "\n",
    "To test with different priors, change the `PRIOR_TYPE` variable at the top:\n",
    "\n",
    "```python\n",
    "PRIOR_TYPE = 'analytical'  # Current analytical formula\n",
    "PRIOR_TYPE = 'learned'     # Learned formula from notebook 8\n",
    "PRIOR_TYPE = 'empirical'   # Empirical frequencies from N permutations\n",
    "```\n",
    "\n",
    "The `compute_edge_priors()` function can be extended to support:\n",
    "- **Learned**: Load trained LearnedAnalyticalFormula and use its predictions\n",
    "- **Empirical**: Load frequency files from `results/empirical_edge_frequencies/`\n",
    "- **ML Model**: Load trained RF/LogReg model and use its predictions\n",
    "\n",
    "This makes it easy to compare different prior sources!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}