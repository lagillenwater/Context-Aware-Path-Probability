{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learned Analytical Formula for Edge Probability Estimation\n",
    "\n",
    "This notebook learns an improved analytical function that generalizes across sparse and dense graphs.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. Train parameterized analytical function on N permutations\n",
    "2. Validate against 200-permutation empirical frequencies\n",
    "3. Find minimum N where performance converges\n",
    "4. Save optimized predictions for all edges\n",
    "\n",
    "## Parameterized Formula\n",
    "\n",
    "```\n",
    "P(u, v | graph) = α × (u^β × v^γ) / (δ + ε×m + ζ×(u×v)^η + θ×density^κ)\n",
    "```\n",
    "\n",
    "Where α, β, γ, δ, ε, ζ, η, θ, κ are learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill parameters\n",
    "edge_type = \"CtD\"  # Edge type to analyze\n",
    "N_candidates = [2, 3, 5, 7, 10, 15, 20, 30, 40, 50]  # Permutation counts to test\n",
    "convergence_threshold = 0.02  # Stop if improvement < 2%\n",
    "target_metric = \"correlation\"  # Metric to optimize\n",
    "min_metric_value = 0.95  # Target performance level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd().parent\n",
    "src_dir = repo_dir / 'src'\n",
    "data_dir = repo_dir / 'data'\n",
    "results_dir = repo_dir / 'results'\n",
    "\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "# Import custom module\n",
    "from learned_analytical import LearnedAnalyticalFormula\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"Repository directory: {repo_dir}\")\n",
    "print(f\"Analyzing edge type: {edge_type}\")\n",
    "print(f\"Testing N values: {N_candidates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Minimum Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize learner\n",
    "learner = LearnedAnalyticalFormula()\n",
    "\n",
    "# Find minimum permutations\n",
    "results = learner.find_minimum_permutations(\n",
    "    graph_name=edge_type,\n",
    "    data_dir=data_dir,\n",
    "    results_dir=results_dir,\n",
    "    N_candidates=N_candidates,\n",
    "    convergence_threshold=convergence_threshold,\n",
    "    target_metric=target_metric,\n",
    "    min_metric_value=min_metric_value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL RESULTS FOR {edge_type}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nMinimum permutations: N = {results['N_min']}\")\n",
    "print(f\"\\nLearned parameters:\")\n",
    "param_names = ['α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'κ']\n",
    "for name, value in zip(param_names, results['best_params']):\n",
    "    print(f\"  {name} = {value:.6f}\")\n",
    "\n",
    "print(f\"\\nValidation metrics (vs 200-perm empirical):\")\n",
    "for metric, value in results['final_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.6f}\")\n",
    "\n",
    "print(f\"\\nBaseline (current analytical):\")\n",
    "for metric, value in results['baseline_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.6f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "mae_improv = (results['baseline_metrics']['mae'] - results['final_metrics']['mae']) / results['baseline_metrics']['mae'] * 100\n",
    "corr_improv = (results['final_metrics']['correlation'] - results['baseline_metrics']['correlation']) / results['baseline_metrics']['correlation'] * 100\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  MAE: {mae_improv:+.2f}%\")\n",
    "print(f\"  Correlation: {corr_improv:+.2f}%\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions for All Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating predictions for all source-target combinations...\")\n",
    "predictions_df = learner.predict_all_edges(edge_type, data_dir)\n",
    "\n",
    "print(f\"\\nPredictions generated:\")\n",
    "print(f\"  Total combinations: {len(predictions_df):,}\")\n",
    "print(f\"  Learned probability range: {predictions_df['learned_probability'].min():.6f} - {predictions_df['learned_probability'].max():.6f}\")\n",
    "print(f\"  Analytical probability range: {predictions_df['analytical_probability'].min():.6f} - {predictions_df['analytical_probability'].max():.6f}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(predictions_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = results_dir / 'learned_analytical' / f'{edge_type}_results'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save learned parameters and metrics\n",
    "learner.save_results(results, output_dir)\n",
    "\n",
    "# Save predictions\n",
    "predictions_file = output_dir / f'{edge_type}_learned_predictions.csv'\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "print(f\"\\nPredictions saved to: {predictions_file}\")\n",
    "\n",
    "# Save compressed version\n",
    "predictions_file_gz = output_dir / f'{edge_type}_learned_predictions.csv.gz'\n",
    "predictions_df.to_csv(predictions_file_gz, index=False, compression='gzip')\n",
    "print(f\"Compressed predictions saved to: {predictions_file_gz}\")\n",
    "\n",
    "# Save convergence data\n",
    "convergence_df = pd.DataFrame([\n",
    "    {\n",
    "        'N': r['N'],\n",
    "        'train_mae': r['train_metrics']['mae'],\n",
    "        'train_rmse': r['train_metrics']['rmse'],\n",
    "        'train_correlation': r['train_metrics']['correlation'],\n",
    "        'val_mae': r['val_metrics']['mae'],\n",
    "        'val_rmse': r['val_metrics']['rmse'],\n",
    "        'val_correlation': r['val_metrics']['correlation']\n",
    "    }\n",
    "    for r in results['convergence_curve']\n",
    "])\n",
    "\n",
    "convergence_file = output_dir / f'{edge_type}_convergence_data.csv'\n",
    "convergence_df.to_csv(convergence_file, index=False)\n",
    "print(f\"Convergence data saved to: {convergence_file}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ALL RESULTS SAVED TO: {output_dir}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Learned vs Analytical vs Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load empirical frequencies\n",
    "empirical_file = results_dir / 'empirical_edge_frequencies' / f'edge_frequency_by_degree_{edge_type}.csv'\n",
    "empirical_df = pd.read_csv(empirical_file)\n",
    "\n",
    "# Merge with predictions\n",
    "comparison_df = empirical_df.merge(\n",
    "    predictions_df,\n",
    "    on=['source_degree', 'target_degree'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Matched {len(comparison_df)} degree combinations\")\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Learned vs Empirical\n",
    "axes[0].scatter(comparison_df['frequency'], comparison_df['learned_probability'], \n",
    "                alpha=0.5, s=20, label='Learned')\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect')\n",
    "corr_learned = np.corrcoef(comparison_df['frequency'], comparison_df['learned_probability'])[0, 1]\n",
    "axes[0].set_xlabel('Empirical Frequency (200 perms)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Probability', fontsize=12)\n",
    "axes[0].set_title(f'Learned Formula vs Empirical\\nr = {corr_learned:.4f}', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Analytical vs Empirical\n",
    "axes[1].scatter(comparison_df['frequency'], comparison_df['analytical_probability'], \n",
    "                alpha=0.5, s=20, label='Current Analytical', color='orange')\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect')\n",
    "corr_analytical = np.corrcoef(comparison_df['frequency'], comparison_df['analytical_probability'])[0, 1]\n",
    "axes[1].set_xlabel('Empirical Frequency (200 perms)', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Probability', fontsize=12)\n",
    "axes[1].set_title(f'Current Analytical vs Empirical\\nr = {corr_analytical:.4f}', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_plot = output_dir / f'{edge_type}_learned_vs_analytical_comparison.png'\n",
    "plt.savefig(comparison_plot, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nComparison plot saved to: {comparison_plot}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY FOR {edge_type}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGraph characteristics:\")\n",
    "print(f\"  Total edges: {results['graph_stats']['m']:,}\")\n",
    "print(f\"  Density: {results['graph_stats']['density']:.4f}\")\n",
    "print(f\"  Nodes: {results['graph_stats']['n_sources']} × {results['graph_stats']['n_targets']}\")\n",
    "\n",
    "print(f\"\\nOptimization:\")\n",
    "print(f\"  Minimum permutations: {results['N_min']}\")\n",
    "print(f\"  Convergence threshold: {convergence_threshold} ({convergence_threshold*100}%)\")\n",
    "print(f\"  Target metric: {target_metric} > {min_metric_value}\")\n",
    "\n",
    "print(f\"\\nPerformance (vs 200-perm empirical):\")\n",
    "print(f\"  Learned:    MAE={results['final_metrics']['mae']:.6f}, r={results['final_metrics']['correlation']:.6f}\")\n",
    "print(f\"  Analytical: MAE={results['baseline_metrics']['mae']:.6f}, r={results['baseline_metrics']['correlation']:.6f}\")\n",
    "print(f\"  Improvement: MAE {mae_improv:+.1f}%, Correlation {corr_improv:+.1f}%\")\n",
    "\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  Parameters: {output_dir / f'{edge_type}_learned_parameters.json'}\")\n",
    "print(f\"  Metrics: {output_dir / f'{edge_type}_metrics.json'}\")\n",
    "print(f\"  Predictions: {output_dir / f'{edge_type}_learned_predictions.csv.gz'}\")\n",
    "print(f\"  Convergence: {output_dir / f'{edge_type}_convergence_curve.png'}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
