{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Learned Analytical Formula with Degree-Based Error Analysis\n",
    "\n",
    "This notebook extends the original learned analytical formula analysis (Notebook 8) with\n",
    "comprehensive degree-based error analysis for better understanding of formula performance.\n",
    "\n",
    "## Enhanced Features\n",
    "\n",
    "- **Degree-stratified residual analysis**: Understand where the formula fails\n",
    "- **Enhanced parameter sensitivity**: Analyze parameter importance by degree range\n",
    "- **Bias-variance decomposition**: Break down errors by degree combinations\n",
    "- **Improved convergence analysis**: Degree-aware minimum permutations\n",
    "\n",
    "## Formula Types Supported\n",
    "\n",
    "1. **Original**: `P = α × (u^β × v^γ) / (δ + ε×m + ζ×(u×v)^η + θ×density^κ)`\n",
    "2. **Extended**: Adds logarithmic terms for high-degree nodes\n",
    "3. **Polynomial**: Linear combination approach for sparse graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Papermill parameters\n",
    "edge_type = \"PCiC\"  # Start with smallest edge type for testing\n",
    "N_candidates = [2, 3, 5, 7, 10]  # Reduced for faster testing\n",
    "convergence_threshold = 0.01  # 1% for faster convergence\n",
    "target_metric = \"correlation\"\n",
    "min_metric_value = 0.95\n",
    "formula_type = \"original\"  # 'original', 'extended', 'polynomial'\n",
    "small_graph_mode = True  # Use small graph optimizations\n",
    "\n",
    "# Handle string-to-list conversion for N_candidates (from papermill)\n",
    "import json\n",
    "if isinstance(N_candidates, str):\n",
    "    N_candidates = json.loads(N_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd().parent\n",
    "src_dir = repo_dir / 'src'\n",
    "data_dir = repo_dir / 'data'\n",
    "results_dir = repo_dir / 'results'\n",
    "output_dir = results_dir / 'learned_analytical_enhanced'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "# Import modules\n",
    "from learned_analytical import LearnedAnalyticalFormula\n",
    "from degree_analysis import DegreeAnalyzer, identify_small_graphs\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"Repository directory: {repo_dir}\")\n",
    "print(f\"Analyzing edge type: {edge_type}\")\n",
    "print(f\"Formula type: {formula_type}\")\n",
    "print(f\"Small graph mode: {small_graph_mode}\")\n",
    "print(f\"Testing N values: {N_candidates}\")\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Edge Type Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If in small graph mode, validate edge type is actually small\n",
    "if small_graph_mode:\n",
    "    small_graphs = identify_small_graphs(data_dir, max_edges=10000)\n",
    "    small_edge_types = [g['edge_type'] for g in small_graphs]\n",
    "    \n",
    "    print(f\"Available small edge types: {small_edge_types[:10]}...\")  # Show first 10\n",
    "    \n",
    "    if edge_type not in small_edge_types:\n",
    "        print(f\"Warning: {edge_type} not in small graphs. Using {small_edge_types[0]} instead.\")\n",
    "        edge_type = small_edge_types[0]\n",
    "    \n",
    "    # Get graph info\n",
    "    edge_info = next((g for g in small_graphs if g['edge_type'] == edge_type), None)\n",
    "    if edge_info:\n",
    "        print(f\"\\nSelected edge type: {edge_type}\")\n",
    "        print(f\"  Edges: {edge_info['n_edges']:,}\")\n",
    "        print(f\"  Shape: {edge_info['shape']}\")\n",
    "        print(f\"  Density: {edge_info['density']:.6f}\")\n",
    "else:\n",
    "    print(f\"Full-scale mode: analyzing {edge_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Enhanced Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize learner with enhanced configuration\n",
    "learner = LearnedAnalyticalFormula(\n",
    "    n_random_starts=5 if small_graph_mode else 10,  # Reduced for faster testing\n",
    "    regularization_lambda=0.001,\n",
    "    formula_type=formula_type,\n",
    "    bootstrap_samples=1,  # No bootstrap for small graphs\n",
    "    ensemble_size=1\n",
    ")\n",
    "\n",
    "print(f\"Initialized {formula_type} learner with:\")\n",
    "print(f\"  Random starts: {learner.n_random_starts}\")\n",
    "print(f\"  Regularization: {learner.regularization_lambda}\")\n",
    "print(f\"  Bootstrap samples: {learner.bootstrap_samples}\")\n",
    "print(f\"  Ensemble size: {learner.ensemble_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Minimum Permutations Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run enhanced minimum permutations analysis\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"ENHANCED MINIMUM PERMUTATIONS ANALYSIS - {edge_type}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results = learner.find_minimum_permutations(\n",
    "    graph_name=edge_type,\n",
    "    data_dir=data_dir,\n",
    "    results_dir=results_dir,\n",
    "    N_candidates=N_candidates,\n",
    "    convergence_threshold=convergence_threshold,\n",
    "    target_metric=target_metric,\n",
    "    min_metric_value=min_metric_value\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Minimum permutations: N = {results['N_min']}\")\n",
    "print(f\"Final validation correlation: {results['final_metrics']['correlation']:.6f}\")\n",
    "print(f\"Improvement over baseline: {((results['final_metrics']['correlation'] - results['baseline_metrics']['correlation']) / results['baseline_metrics']['correlation'] * 100):+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Residual Analysis with Degree Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 200-permutation empirical frequencies for enhanced analysis\n",
    "empirical_file = results_dir / 'empirical_edge_frequencies' / f'edge_frequency_by_degree_{edge_type}.csv'\n",
    "\n",
    "if empirical_file.exists():\n",
    "    empirical_df = pd.read_csv(empirical_file)\n",
    "    empirical_200 = {}\n",
    "    for _, row in empirical_df.iterrows():\n",
    "        u = int(row['source_degree'])\n",
    "        v = int(row['target_degree'])\n",
    "        freq = float(row['frequency'])\n",
    "        empirical_200[(u, v)] = freq\n",
    "    \n",
    "    print(f\"Loaded empirical frequencies: {len(empirical_200)} degree combinations\")\n",
    "    \n",
    "    # Run enhanced residual analysis\n",
    "    print(f\"\\nRunning enhanced residual analysis with degree decomposition...\")\n",
    "    residuals_df, degree_error_metrics = learner.analyze_residuals(\n",
    "        empirical_200=empirical_200,\n",
    "        m=results['graph_stats']['m'],\n",
    "        density=results['graph_stats']['density'],\n",
    "        results_dir=output_dir,\n",
    "        graph_name=edge_type,\n",
    "        small_graph_mode=small_graph_mode\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerated enhanced residual analysis with {len(degree_error_metrics)} degree combinations\")\n",
    "    \n",
    "    # Display degree-based error summary\n",
    "    print(f\"\\nDegree-based error metrics:\")\n",
    "    display_cols = ['n_samples', 'bias_learned', 'rmse_learned', 'rel_error_mean_learned']\n",
    "    print(degree_error_metrics[display_cols].round(4))\n",
    "    \n",
    "else:\n",
    "    print(f\"Empirical frequency file not found: {empirical_file}\")\n",
    "    print(\"Skipping enhanced residual analysis\")\n",
    "    residuals_df = None\n",
    "    degree_error_metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Parameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run enhanced parameter sensitivity analysis\n",
    "if empirical_200 is not None:\n",
    "    print(f\"\\nRunning enhanced parameter sensitivity analysis...\")\n",
    "    \n",
    "    sensitivity_df = learner.analyze_parameter_importance(\n",
    "        empirical_200=empirical_200,\n",
    "        m=results['graph_stats']['m'],\n",
    "        density=results['graph_stats']['density'],\n",
    "        graph_name=edge_type,\n",
    "        results_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nParameter sensitivity analysis complete!\")\n",
    "    print(f\"Most sensitive parameters:\")\n",
    "    top_params = sensitivity_df.head(3)\n",
    "    for _, row in top_params.iterrows():\n",
    "        print(f\"  {row['parameter']}: {row['sensitivity']:.6f} (value: {row['value']:.4f})\")\n",
    "else:\n",
    "    print(\"Skipping parameter sensitivity analysis (no empirical data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree-Stratified Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze convergence patterns by degree combination\n",
    "if degree_error_metrics is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DEGREE-STRATIFIED CONVERGENCE ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create enhanced convergence visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Sample size vs error\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(degree_error_metrics['n_samples'], degree_error_metrics['rmse_learned'],\n",
    "               alpha=0.7, s=50, c=degree_error_metrics['bias_learned'], \n",
    "               cmap='RdBu_r', edgecolor='black')\n",
    "    ax.set_xlabel('Sample Size', fontsize=12)\n",
    "    ax.set_ylabel('RMSE', fontsize=12)\n",
    "    ax.set_title(f'{edge_type} - Error vs Sample Size\\n(Color = Bias)', fontsize=14, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(ax.collections[0], ax=ax)\n",
    "    cbar.set_label('Bias', fontsize=10)\n",
    "    \n",
    "    # Plot 2: Error by degree combination\n",
    "    ax = axes[0, 1]\n",
    "    degree_error_metrics['rmse_learned'].plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n",
    "    ax.set_xlabel('Degree Combination', fontsize=12)\n",
    "    ax.set_ylabel('RMSE', fontsize=12)\n",
    "    ax.set_title(f'{edge_type} - Error by Degree Combination', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 3: Bias analysis\n",
    "    ax = axes[1, 0]\n",
    "    bias_data = degree_error_metrics[['bias_learned', 'bias_analytical']]\n",
    "    bias_data.plot(kind='bar', ax=ax, color=['green', 'orange'], alpha=0.7)\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    ax.set_xlabel('Degree Combination', fontsize=12)\n",
    "    ax.set_ylabel('Bias', fontsize=12)\n",
    "    ax.set_title(f'{edge_type} - Bias Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.legend(['Learned', 'Analytical'])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 4: Relative error distribution\n",
    "    ax = axes[1, 1]\n",
    "    rel_error_data = degree_error_metrics[['rel_error_mean_learned', 'rel_error_mean_analytical']]\n",
    "    rel_error_data.plot(kind='bar', ax=ax, color=['green', 'orange'], alpha=0.7)\n",
    "    ax.set_xlabel('Degree Combination', fontsize=12)\n",
    "    ax.set_ylabel('Mean Relative Error', fontsize=12)\n",
    "    ax.set_title(f'{edge_type} - Relative Error Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.legend(['Learned', 'Analytical'])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    convergence_plot = output_dir / f'{edge_type}_degree_convergence_analysis.png'\n",
    "    plt.savefig(convergence_plot, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Degree-stratified convergence analysis saved to: {convergence_plot}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nConvergence insights:\")\n",
    "    print(f\"  Most stable combination: {degree_error_metrics['rmse_learned'].idxmin()} (RMSE: {degree_error_metrics['rmse_learned'].min():.6f})\")\n",
    "    print(f\"  Least stable combination: {degree_error_metrics['rmse_learned'].idxmax()} (RMSE: {degree_error_metrics['rmse_learned'].max():.6f})\")\n",
    "    \n",
    "    high_bias = degree_error_metrics[np.abs(degree_error_metrics['bias_learned']) > 0.01]\n",
    "    if len(high_bias) > 0:\n",
    "        print(f\"  High bias combinations ({len(high_bias)}): {list(high_bias.index)}\")\n",
    "    else:\n",
    "        print(f\"  No high bias combinations detected\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping degree-stratified convergence analysis (no degree metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Enhanced Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with degree-based analysis\n",
    "print(f\"\\nGenerating enhanced predictions for all source-target combinations...\")\n",
    "\n",
    "predictions_df = learner.predict_all_edges(edge_type, data_dir)\n",
    "\n",
    "# Add degree-based analysis to predictions\n",
    "if small_graph_mode:\n",
    "    # For small graphs, we can afford to add degree analysis to all predictions\n",
    "    analyzer = DegreeAnalyzer(small_graph_mode=True)\n",
    "    \n",
    "    # Load graph degrees\n",
    "    source_degrees, target_degrees = analyzer.load_graph_degrees(edge_type, data_dir)\n",
    "    \n",
    "    # Add degree categories to predictions\n",
    "    predictions_df['source_degree_category'] = analyzer.categorize_degrees(\n",
    "        source_degrees[predictions_df['source_index']]\n",
    "    ).astype(str)\n",
    "    predictions_df['target_degree_category'] = analyzer.categorize_degrees(\n",
    "        target_degrees[predictions_df['target_index']]\n",
    "    ).astype(str)\n",
    "    predictions_df['degree_combination'] = analyzer.create_degree_combination_labels(\n",
    "        predictions_df['source_degree_category'].values,\n",
    "        predictions_df['target_degree_category'].values\n",
    "    )\n",
    "    \n",
    "    print(f\"Enhanced predictions generated:\")\n",
    "    print(f\"  Total combinations: {len(predictions_df):,}\")\n",
    "    print(f\"  Degree combinations: {predictions_df['degree_combination'].nunique()}\")\n",
    "    print(f\"  Probability range: {predictions_df['learned_probability'].min():.6f} - {predictions_df['learned_probability'].max():.6f}\")\n",
    "    \n",
    "    # Sample by degree combination\n",
    "    print(f\"\\nSample predictions by degree combination:\")\n",
    "    for combo in predictions_df['degree_combination'].unique()[:5]:  # Show first 5\n",
    "        combo_data = predictions_df[predictions_df['degree_combination'] == combo]\n",
    "        print(f\"  {combo}: {len(combo_data)} pairs, avg prob: {combo_data['learned_probability'].mean():.6f}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Predictions generated:\")\n",
    "    print(f\"  Total combinations: {len(predictions_df):,}\")\n",
    "    print(f\"  Probability range: {predictions_df['learned_probability'].min():.6f} - {predictions_df['learned_probability'].max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Enhanced Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced results\n",
    "print(f\"\\nSaving enhanced results to: {output_dir}\")\n",
    "\n",
    "# Save learned parameters with enhanced metadata\n",
    "enhanced_results = results.copy()\n",
    "enhanced_results['formula_type'] = formula_type\n",
    "enhanced_results['small_graph_mode'] = small_graph_mode\n",
    "enhanced_results['convergence_threshold'] = convergence_threshold\n",
    "\n",
    "learner.save_results(enhanced_results, output_dir)\n",
    "\n",
    "# Save enhanced predictions\n",
    "predictions_file = output_dir / f'{edge_type}_enhanced_predictions.csv'\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "print(f\"Enhanced predictions saved to: {predictions_file}\")\n",
    "\n",
    "# Save degree-based error metrics if available\n",
    "if degree_error_metrics is not None:\n",
    "    degree_metrics_file = output_dir / f'{edge_type}_degree_error_metrics.csv'\n",
    "    degree_error_metrics.to_csv(degree_metrics_file)\n",
    "    print(f\"Degree-based error metrics saved to: {degree_metrics_file}\")\n",
    "\n",
    "# Save residual data if available\n",
    "if residuals_df is not None:\n",
    "    residuals_file = output_dir / f'{edge_type}_enhanced_residuals.csv'\n",
    "    residuals_df.to_csv(residuals_file, index=False)\n",
    "    print(f\"Enhanced residual data saved to: {residuals_file}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = {\n",
    "    'edge_type': edge_type,\n",
    "    'formula_type': formula_type,\n",
    "    'small_graph_mode': small_graph_mode,\n",
    "    'N_min': results['N_min'],\n",
    "    'final_correlation': results['final_metrics']['correlation'],\n",
    "    'final_mae': results['final_metrics']['mae'],\n",
    "    'baseline_correlation': results['baseline_metrics']['correlation'],\n",
    "    'improvement_pct': ((results['final_metrics']['correlation'] - results['baseline_metrics']['correlation']) / results['baseline_metrics']['correlation'] * 100),\n",
    "    'graph_stats': results['graph_stats'],\n",
    "    'degree_combinations_analyzed': len(degree_error_metrics) if degree_error_metrics is not None else 0,\n",
    "    'total_predictions': len(predictions_df)\n",
    "}\n",
    "\n",
    "summary_file = output_dir / f'{edge_type}_enhanced_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "print(f\"Summary report saved to: {summary_file}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ENHANCED ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "for file in sorted(output_dir.glob(f'{edge_type}_*')):\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "print(f\"\\n✓ Enhanced learned analytical formula analysis complete!\")\n",
    "print(f\"✓ Degree-based error decomposition: {len(degree_error_metrics) if degree_error_metrics is not None else 0} combinations\")\n",
    "print(f\"✓ Formula type: {formula_type}\")\n",
    "print(f\"✓ Minimum permutations: N = {results['N_min']}\")\n",
    "print(f\"✓ Performance improvement: {summary_report['improvement_pct']:+.1f}%\")\n",
    "\n",
    "if small_graph_mode:\n",
    "    print(f\"\\n✓ Small graph validation successful!\")\n",
    "    print(f\"✓ Framework ready for HPC deployment on all edge types\")\n",
    "else:\n",
    "    print(f\"\\n✓ Full-scale analysis complete!\")\n",
    "    print(f\"✓ Results ready for publication\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}