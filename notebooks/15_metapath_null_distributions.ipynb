{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metapath Null Distributions\n",
    "\n",
    "This notebook generates degree-aware null distributions for multiple metapaths using the compositional null models trained in notebook 13.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load trained null models for each edge type\n",
    "2. For each metapath, compute compositional null predictions\n",
    "3. Compare Hetionet metapath frequencies vs null distributions\n",
    "4. Statistical tests: KS test, correlation analysis\n",
    "5. Detect anomalous paths in Hetionet\n",
    "6. Save results and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from scipy.stats import ks_2samp, pearsonr\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path('/Users/lucas/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Repositories/Context-Aware-Path-Probability')\n",
    "data_dir = repo_dir / 'data'\n",
    "results_dir = repo_dir / 'results' / 'metapath_null_distributions'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "models_dir = repo_dir / 'results' / 'null_models'\n",
    "\n",
    "print(f\"Results will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Null Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_null_models(edge_type, model_type='rf'):\n",
    "    \"\"\"\n",
    "    Load trained null models for an edge type.\n",
    "    \n",
    "    Args:\n",
    "        edge_type: Edge type abbreviation (e.g., 'CbG', 'GpPW')\n",
    "        model_type: 'rf' for Random Forest or 'poly_logreg' for Polynomial Logistic Regression\n",
    "    \n",
    "    Returns:\n",
    "        dict: Loaded model and scaler\n",
    "    \"\"\"\n",
    "    model_file = models_dir / f'{edge_type}_{model_type}_model.pkl'\n",
    "    \n",
    "    if not model_file.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_file}\")\n",
    "    \n",
    "    with open(model_file, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    \n",
    "    return model_data\n",
    "\n",
    "# Test loading\n",
    "test_model = load_null_models('CbG', 'rf')\n",
    "print(f\"Successfully loaded model with keys: {test_model.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Node Degree Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_node_degrees(edge_type):\n",
    "    \"\"\"\n",
    "    Load source and target node degrees for an edge type from Hetionet.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (source_degrees, target_degrees) as numpy arrays\n",
    "    \"\"\"\n",
    "    edge_file = data_dir / 'hetionet-v1.0' / 'hetmat' / 'edges' / f'{edge_type}.sparse.npz'\n",
    "    matrix = sp.load_npz(str(edge_file))\n",
    "    \n",
    "    # Compute degrees\n",
    "    source_degrees = np.array(matrix.sum(axis=1)).flatten()\n",
    "    target_degrees = np.array(matrix.sum(axis=0)).flatten()\n",
    "    \n",
    "    return source_degrees, target_degrees\n",
    "\n",
    "def get_intermediate_node_degree_distribution(node_type):\n",
    "    \"\"\"\n",
    "    Get degree distribution for intermediate nodes of a given type.\n",
    "    Uses all edges involving this node type.\n",
    "    \n",
    "    Args:\n",
    "        node_type: Node type abbreviation (e.g., 'Gene', 'Pathway')\n",
    "    \n",
    "    Returns:\n",
    "        dict: {degree: frequency} normalized to sum to 1\n",
    "    \"\"\"\n",
    "    # Load metagraph to find all edges involving this node type\n",
    "    metagraph_file = data_dir / 'hetionet-v1.0' / 'hetmat' / 'metagraph.json'\n",
    "    import json\n",
    "    with open(metagraph_file, 'r') as f:\n",
    "        metagraph = json.load(f)\n",
    "    \n",
    "    # Find edges where node_type appears\n",
    "    node_degrees = []\n",
    "    for edge in metagraph['edges']:\n",
    "        edge_abbrev = edge['abbreviation']\n",
    "        source_type = edge['source']\n",
    "        target_type = edge['target']\n",
    "        \n",
    "        # Load edge matrix\n",
    "        edge_file = data_dir / 'hetionet-v1.0' / 'hetmat' / 'edges' / f'{edge_abbrev}.sparse.npz'\n",
    "        if not edge_file.exists():\n",
    "            continue\n",
    "            \n",
    "        matrix = sp.load_npz(str(edge_file))\n",
    "        \n",
    "        # If source matches, use source degrees\n",
    "        if source_type == node_type:\n",
    "            degrees = np.array(matrix.sum(axis=1)).flatten()\n",
    "            node_degrees.extend(degrees[degrees > 0].tolist())\n",
    "        \n",
    "        # If target matches, use target degrees\n",
    "        if target_type == node_type:\n",
    "            degrees = np.array(matrix.sum(axis=0)).flatten()\n",
    "            node_degrees.extend(degrees[degrees > 0].tolist())\n",
    "    \n",
    "    # Compute frequency distribution\n",
    "    unique_degrees, counts = np.unique(node_degrees, return_counts=True)\n",
    "    total_count = counts.sum()\n",
    "    \n",
    "    degree_freq = {int(deg): float(count / total_count) \n",
    "                   for deg, count in zip(unique_degrees, counts)}\n",
    "    \n",
    "    return degree_freq\n",
    "\n",
    "# Test\n",
    "gene_deg_dist = get_intermediate_node_degree_distribution('Gene')\n",
    "print(f\"Gene degree distribution has {len(gene_deg_dist)} unique degrees\")\n",
    "print(f\"Total probability: {sum(gene_deg_dist.values()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compositional Null Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_edge_probability(source_deg, target_deg, model_data, model_type='rf'):\n",
    "    \"\"\"\n",
    "    Predict edge probability given source and target degrees.\n",
    "    \n",
    "    Args:\n",
    "        source_deg: Source node degree\n",
    "        target_deg: Target node degree\n",
    "        model_data: Dictionary containing model and scaler\n",
    "        model_type: 'rf' or 'poly_logreg'\n",
    "    \n",
    "    Returns:\n",
    "        float: Predicted edge probability\n",
    "    \"\"\"\n",
    "    model = model_data['model']\n",
    "    scaler = model_data.get('scaler', None)\n",
    "    \n",
    "    # Prepare features\n",
    "    features = np.array([[source_deg, target_deg]])\n",
    "    \n",
    "    if scaler is not None:\n",
    "        features = scaler.transform(features)\n",
    "    \n",
    "    # Predict probability\n",
    "    if model_type == 'rf':\n",
    "        prob = model.predict_proba(features)[0, 1]\n",
    "    else:\n",
    "        prob = model.predict_proba(features)[0, 1]\n",
    "    \n",
    "    return float(prob)\n",
    "\n",
    "def compute_metapath_null_2edge(source_degrees, target_degrees, \n",
    "                                edge1_type, edge2_type,\n",
    "                                intermediate_degree_freq,\n",
    "                                model_type='rf'):\n",
    "    \"\"\"\n",
    "    Compute compositional null for 2-edge metapath.\n",
    "    \n",
    "    P(source -> target via metapath) = Σ_i P(source -> i) * P(i -> target) * freq(i)\n",
    "    \n",
    "    Args:\n",
    "        source_degrees: Array of source node degrees\n",
    "        target_degrees: Array of target node degrees\n",
    "        edge1_type: First edge type abbreviation\n",
    "        edge2_type: Second edge type abbreviation\n",
    "        intermediate_degree_freq: Dict of {degree: frequency} for intermediate nodes\n",
    "        model_type: 'rf' or 'poly_logreg'\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Predicted null probabilities\n",
    "    \"\"\"\n",
    "    # Load models\n",
    "    edge1_models = load_null_models(edge1_type, model_type)\n",
    "    edge2_models = load_null_models(edge2_type, model_type)\n",
    "    \n",
    "    null_probs = []\n",
    "    \n",
    "    for source_deg, target_deg in zip(source_degrees, target_degrees):\n",
    "        total_prob = 0.0\n",
    "        \n",
    "        # Sum over all intermediate node degrees\n",
    "        for inter_deg, freq in intermediate_degree_freq.items():\n",
    "            # P(source → intermediate)\n",
    "            p1 = predict_edge_probability(source_deg, inter_deg, edge1_models, model_type)\n",
    "            \n",
    "            # P(intermediate → target)\n",
    "            p2 = predict_edge_probability(inter_deg, target_deg, edge2_models, model_type)\n",
    "            \n",
    "            # Compositional multiplication weighted by frequency\n",
    "            total_prob += p1 * p2 * freq\n",
    "        \n",
    "        null_probs.append(total_prob)\n",
    "    \n",
    "    return np.array(null_probs)\n",
    "\n",
    "print(\"Compositional null calculator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hetionet Metapath Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hetionet_metapath_data(metapath):\n",
    "    \"\"\"\n",
    "    Load metapath data from Hetionet.\n",
    "    \n",
    "    Args:\n",
    "        metapath: Metapath abbreviation (e.g., 'CbGpPW')\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Metapath data with source_degree, target_degree, path_count\n",
    "    \"\"\"\n",
    "    metapath_file = results_dir.parent / 'degree_aware_compositionality' / f'{metapath}_hetionet_paths.csv'\n",
    "    \n",
    "    if not metapath_file.exists():\n",
    "        # Try loading from edge frequency file\n",
    "        print(f\"Metapath file not found: {metapath_file}\")\n",
    "        print(f\"Attempting to compute from scratch...\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(metapath_file)\n",
    "    return df\n",
    "\n",
    "def compute_metapath_from_edges(edge1_type, edge2_type, intermediate_node_type):\n",
    "    \"\"\"\n",
    "    Compute metapath counts directly from edge matrices.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: With columns source_id, target_id, source_degree, target_degree, path_count\n",
    "    \"\"\"\n",
    "    # Load edge matrices\n",
    "    edge1_file = data_dir / 'hetionet-v1.0' / 'hetmat' / 'edges' / f'{edge1_type}.sparse.npz'\n",
    "    edge2_file = data_dir / 'hetionet-v1.0' / 'hetmat' / 'edges' / f'{edge2_type}.sparse.npz'\n",
    "    \n",
    "    edge1_matrix = sp.load_npz(str(edge1_file))\n",
    "    edge2_matrix = sp.load_npz(str(edge2_file))\n",
    "    \n",
    "    # Compute metapath: edge1 @ edge2\n",
    "    metapath_matrix = edge1_matrix @ edge2_matrix\n",
    "    \n",
    "    # Get degrees\n",
    "    source_degrees = np.array(edge1_matrix.sum(axis=1)).flatten()\n",
    "    target_degrees = np.array(edge2_matrix.sum(axis=0)).flatten()\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    metapath_coo = metapath_matrix.tocoo()\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'source_id': metapath_coo.row,\n",
    "        'target_id': metapath_coo.col,\n",
    "        'path_count': metapath_coo.data,\n",
    "        'source_degree': source_degrees[metapath_coo.row],\n",
    "        'target_degree': target_degrees[metapath_coo.col]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Metapath loading functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metapaths to Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metapaths with their component edges\n",
    "metapaths_to_analyze = [\n",
    "    {\n",
    "        'name': 'CbGpPW',\n",
    "        'edge1': 'CbG',\n",
    "        'edge2': 'GpPW',\n",
    "        'intermediate_node': 'Gene',\n",
    "        'description': 'Compound binds Gene participates in Pathway'\n",
    "    },\n",
    "    {\n",
    "        'name': 'CtDaG',\n",
    "        'edge1': 'CtD',\n",
    "        'edge2': 'DaG',\n",
    "        'intermediate_node': 'Disease',\n",
    "        'description': 'Compound treats Disease associates with Gene'\n",
    "    },\n",
    "    {\n",
    "        'name': 'CbGaD',\n",
    "        'edge1': 'CbG',\n",
    "        'edge2': 'GaD',\n",
    "        'intermediate_node': 'Gene',\n",
    "        'description': 'Compound binds Gene associates with Disease'\n",
    "    },\n",
    "    {\n",
    "        'name': 'CrCbG',\n",
    "        'edge1': 'CrC',\n",
    "        'edge2': 'CbG',\n",
    "        'intermediate_node': 'Compound',\n",
    "        'description': 'Compound resembles Compound binds Gene'\n",
    "    },\n",
    "    {\n",
    "        'name': 'GuGiG',\n",
    "        'edge1': 'GuG',\n",
    "        'edge2': 'GiG',\n",
    "        'intermediate_node': 'Gene',\n",
    "        'description': 'Gene upregulates Gene interacts with Gene'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Will analyze {len(metapaths_to_analyze)} metapaths\")\n",
    "for mp in metapaths_to_analyze:\n",
    "    print(f\"  {mp['name']}: {mp['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Each Metapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_metapath(metapath_info, model_type='rf'):\n",
    "    \"\"\"\n",
    "    Analyze a single metapath: compute null, compare to Hetionet, generate visualizations.\n",
    "    \n",
    "    Args:\n",
    "        metapath_info: Dictionary with metapath information\n",
    "        model_type: 'rf' or 'poly_logreg'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    metapath_name = metapath_info['name']\n",
    "    edge1 = metapath_info['edge1']\n",
    "    edge2 = metapath_info['edge2']\n",
    "    intermediate_node = metapath_info['intermediate_node']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing {metapath_name}: {metapath_info['description']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load or compute Hetionet metapath data\n",
    "    print(f\"Loading Hetionet metapath data...\")\n",
    "    hetionet_df = compute_metapath_from_edges(edge1, edge2, intermediate_node)\n",
    "    print(f\"  Found {len(hetionet_df):,} source-target pairs with paths\")\n",
    "    \n",
    "    # Get intermediate node degree distribution\n",
    "    print(f\"Computing {intermediate_node} degree distribution...\")\n",
    "    inter_deg_freq = get_intermediate_node_degree_distribution(intermediate_node)\n",
    "    print(f\"  {len(inter_deg_freq)} unique degrees\")\n",
    "    \n",
    "    # Compute compositional null predictions\n",
    "    print(f\"Computing compositional null predictions using {model_type}...\")\n",
    "    null_probs = compute_metapath_null_2edge(\n",
    "        hetionet_df['source_degree'].values,\n",
    "        hetionet_df['target_degree'].values,\n",
    "        edge1, edge2,\n",
    "        inter_deg_freq,\n",
    "        model_type=model_type\n",
    "    )\n",
    "    \n",
    "    hetionet_df['null_prediction'] = null_probs\n",
    "    \n",
    "    # Normalize path counts to probabilities\n",
    "    total_paths = hetionet_df['path_count'].sum()\n",
    "    hetionet_df['path_probability'] = hetionet_df['path_count'] / total_paths\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(f\"\\nStatistical Analysis:\")\n",
    "    \n",
    "    # Correlation\n",
    "    corr, corr_pval = pearsonr(hetionet_df['null_prediction'], hetionet_df['path_probability'])\n",
    "    print(f\"  Correlation: {corr:.4f} (p={corr_pval:.2e})\")\n",
    "    \n",
    "    # RMSE\n",
    "    rmse = np.sqrt(np.mean((hetionet_df['null_prediction'] - hetionet_df['path_probability'])**2))\n",
    "    print(f\"  RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    # KS test\n",
    "    ks_stat, ks_pval = ks_2samp(hetionet_df['null_prediction'], hetionet_df['path_probability'])\n",
    "    print(f\"  KS test: statistic={ks_stat:.4f}, p={ks_pval:.2e}\")\n",
    "    \n",
    "    # Identify anomalous paths (high residuals)\n",
    "    hetionet_df['residual'] = hetionet_df['path_probability'] - hetionet_df['null_prediction']\n",
    "    hetionet_df['abs_residual'] = np.abs(hetionet_df['residual'])\n",
    "    \n",
    "    # Top overrepresented paths (positive residuals)\n",
    "    top_over = hetionet_df.nlargest(10, 'residual')\n",
    "    print(f\"\\n  Top 10 overrepresented paths (actual > null):\")\n",
    "    for idx, row in top_over.iterrows():\n",
    "        print(f\"    Source {row['source_id']} -> Target {row['target_id']}: \"\n",
    "              f\"actual={row['path_probability']:.6f}, null={row['null_prediction']:.6f}, \"\n",
    "              f\"residual={row['residual']:.6f}\")\n",
    "    \n",
    "    # Top underrepresented paths (negative residuals)\n",
    "    top_under = hetionet_df.nsmallest(10, 'residual')\n",
    "    print(f\"\\n  Top 10 underrepresented paths (actual < null):\")\n",
    "    for idx, row in top_under.iterrows():\n",
    "        print(f\"    Source {row['source_id']} -> Target {row['target_id']}: \"\n",
    "              f\"actual={row['path_probability']:.6f}, null={row['null_prediction']:.6f}, \"\n",
    "              f\"residual={row['residual']:.6f}\")\n",
    "    \n",
    "    # Save results\n",
    "    output_file = results_dir / f'{metapath_name}_{model_type}_analysis.csv'\n",
    "    hetionet_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # 1. Scatter: Null vs Actual\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(hetionet_df['null_prediction'], hetionet_df['path_probability'], \n",
    "               alpha=0.3, s=10)\n",
    "    ax.plot([0, hetionet_df[['null_prediction', 'path_probability']].max().max()],\n",
    "            [0, hetionet_df[['null_prediction', 'path_probability']].max().max()],\n",
    "            'r--', alpha=0.5, label='y=x')\n",
    "    ax.set_xlabel('Null Prediction')\n",
    "    ax.set_ylabel('Hetionet Path Probability')\n",
    "    ax.set_title(f'{metapath_name}: Null vs Actual\\nr={corr:.4f}, RMSE={rmse:.6f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Residual distribution\n",
    "    ax = axes[0, 1]\n",
    "    ax.hist(hetionet_df['residual'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "    ax.set_xlabel('Residual (Actual - Null)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Residual Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual vs source degree\n",
    "    ax = axes[1, 0]\n",
    "    ax.scatter(hetionet_df['source_degree'], hetionet_df['residual'], \n",
    "               alpha=0.3, s=10)\n",
    "    ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "    ax.set_xlabel('Source Degree')\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_title('Residual vs Source Degree')\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Residual vs target degree\n",
    "    ax = axes[1, 1]\n",
    "    ax.scatter(hetionet_df['target_degree'], hetionet_df['residual'], \n",
    "               alpha=0.3, s=10)\n",
    "    ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "    ax.set_xlabel('Target Degree')\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_title('Residual vs Target Degree')\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_file = results_dir / f'{metapath_name}_{model_type}_analysis.png'\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plots saved to: {plot_file}\")\n",
    "    \n",
    "    # Return summary\n",
    "    return {\n",
    "        'metapath': metapath_name,\n",
    "        'model_type': model_type,\n",
    "        'n_pairs': len(hetionet_df),\n",
    "        'correlation': corr,\n",
    "        'correlation_pval': corr_pval,\n",
    "        'rmse': rmse,\n",
    "        'ks_statistic': ks_stat,\n",
    "        'ks_pval': ks_pval,\n",
    "        'mean_residual': hetionet_df['residual'].mean(),\n",
    "        'std_residual': hetionet_df['residual'].std()\n",
    "    }\n",
    "\n",
    "print(\"Analysis function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis for All Metapaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze with Random Forest models\n",
    "results_rf = []\n",
    "\n",
    "for metapath_info in metapaths_to_analyze:\n",
    "    try:\n",
    "        result = analyze_metapath(metapath_info, model_type='rf')\n",
    "        results_rf.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR analyzing {metapath_info['name']}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_rf = pd.DataFrame(results_rf)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Random Forest Models\")\n",
    "print(\"=\"*60)\n",
    "print(summary_rf.to_string(index=False))\n",
    "\n",
    "summary_rf.to_csv(results_dir / 'summary_rf.csv', index=False)\n",
    "print(f\"\\nSummary saved to: {results_dir / 'summary_rf.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze with Polynomial Logistic Regression models\n",
    "results_poly = []\n",
    "\n",
    "for metapath_info in metapaths_to_analyze:\n",
    "    try:\n",
    "        result = analyze_metapath(metapath_info, model_type='poly_logreg')\n",
    "        results_poly.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR analyzing {metapath_info['name']}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_poly = pd.DataFrame(results_poly)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Polynomial Logistic Regression Models\")\n",
    "print(\"=\"*60)\n",
    "print(summary_poly.to_string(index=False))\n",
    "\n",
    "summary_poly.to_csv(results_dir / 'summary_poly_logreg.csv', index=False)\n",
    "print(f\"\\nSummary saved to: {results_dir / 'summary_poly_logreg.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge summaries for comparison\n",
    "comparison = summary_rf.merge(summary_poly, on='metapath', suffixes=('_rf', '_poly'))\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Correlation comparison\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, comparison['correlation_rf'], width, label='Random Forest', alpha=0.8)\n",
    "ax.bar(x + width/2, comparison['correlation_poly'], width, label='Poly LogReg', alpha=0.8)\n",
    "ax.set_xlabel('Metapath')\n",
    "ax.set_ylabel('Correlation')\n",
    "ax.set_title('Model Correlation Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['metapath'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. RMSE comparison\n",
    "ax = axes[0, 1]\n",
    "ax.bar(x - width/2, comparison['rmse_rf'], width, label='Random Forest', alpha=0.8)\n",
    "ax.bar(x + width/2, comparison['rmse_poly'], width, label='Poly LogReg', alpha=0.8)\n",
    "ax.set_xlabel('Metapath')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Model RMSE Comparison (lower is better)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['metapath'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Scatter: RF vs Poly correlation\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(comparison['correlation_rf'], comparison['correlation_poly'], s=100)\n",
    "for idx, row in comparison.iterrows():\n",
    "    ax.annotate(row['metapath'], (row['correlation_rf'], row['correlation_poly']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "lims = [min(comparison['correlation_rf'].min(), comparison['correlation_poly'].min()) - 0.05,\n",
    "        max(comparison['correlation_rf'].max(), comparison['correlation_poly'].max()) + 0.05]\n",
    "ax.plot(lims, lims, 'r--', alpha=0.5, label='y=x')\n",
    "ax.set_xlabel('Random Forest Correlation')\n",
    "ax.set_ylabel('Poly LogReg Correlation')\n",
    "ax.set_title('Correlation: RF vs Poly LogReg')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scatter: RF vs Poly RMSE\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(comparison['rmse_rf'], comparison['rmse_poly'], s=100)\n",
    "for idx, row in comparison.iterrows():\n",
    "    ax.annotate(row['metapath'], (row['rmse_rf'], row['rmse_poly']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "lims = [min(comparison['rmse_rf'].min(), comparison['rmse_poly'].min()) - 0.01,\n",
    "        max(comparison['rmse_rf'].max(), comparison['rmse_poly'].max()) + 0.01]\n",
    "ax.plot(lims, lims, 'r--', alpha=0.5, label='y=x')\n",
    "ax.set_xlabel('Random Forest RMSE')\n",
    "ax.set_ylabel('Poly LogReg RMSE')\n",
    "ax.set_title('RMSE: RF vs Poly LogReg')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_plot_file = results_dir / 'model_comparison.png'\n",
    "plt.savefig(comparison_plot_file, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nModel comparison plot saved to: {comparison_plot_file}\")\n",
    "\n",
    "# Print winner for each metapath\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST MODEL BY METAPATH (by correlation)\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in comparison.iterrows():\n",
    "    if row['correlation_rf'] > row['correlation_poly']:\n",
    "        winner = 'Random Forest'\n",
    "        winner_corr = row['correlation_rf']\n",
    "        winner_rmse = row['rmse_rf']\n",
    "    else:\n",
    "        winner = 'Poly LogReg'\n",
    "        winner_corr = row['correlation_poly']\n",
    "        winner_rmse = row['rmse_poly']\n",
    "    print(f\"{row['metapath']:15s} -> {winner:20s} (r={winner_corr:.4f}, RMSE={winner_rmse:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nAnalyzed {len(metapaths_to_analyze)} metapaths with 2 model types\")\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "\n",
    "print(\"\\nOutput files:\")\n",
    "for f in sorted(results_dir.glob('*')):\n",
    "    print(f\"  {f.name}\")\n",
    "\n",
    "print(\"\\nOverall Statistics:\")\n",
    "print(f\"  RF - Mean correlation: {summary_rf['correlation'].mean():.4f} ± {summary_rf['correlation'].std():.4f}\")\n",
    "print(f\"  RF - Mean RMSE: {summary_rf['rmse'].mean():.6f} ± {summary_rf['rmse'].std():.6f}\")\n",
    "print(f\"  Poly - Mean correlation: {summary_poly['correlation'].mean():.4f} ± {summary_poly['correlation'].std():.4f}\")\n",
    "print(f\"  Poly - Mean RMSE: {summary_poly['rmse'].mean():.6f} ± {summary_poly['rmse'].std():.6f}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review top over/underrepresented paths for biological insights\")\n",
    "print(\"  2. Extend to longer metapaths (3+ edges) using compositional approach\")\n",
    "print(\"  3. Integrate with downstream analyses (e.g., drug repurposing)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
