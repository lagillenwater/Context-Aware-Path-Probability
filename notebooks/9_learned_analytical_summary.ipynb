{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learned Analytical Formula - Summary Across All Edge Types\n",
    "\n",
    "This notebook summarizes the learned analytical formula results across all edge types.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Load results from all edge types\n",
    "2. Compare minimum permutations needed (N_min)\n",
    "3. Analyze learned parameters across graphs\n",
    "4. Performance comparison: Learned vs Current Analytical\n",
    "5. Relationship between graph properties and N_min\n",
    "6. Recommendations for future graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd().parent\n",
    "results_dir = repo_dir / 'results' / 'learned_analytical'\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results from All Edge Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all edge type results\n",
    "edge_type_dirs = [d for d in results_dir.glob('*_results') if d.is_dir()]\n",
    "edge_types = [d.name.replace('_results', '') for d in edge_type_dirs]\n",
    "\n",
    "print(f\"Found {len(edge_types)} edge types with results:\")\n",
    "print(f\"{edge_types}\")\n",
    "\n",
    "# Load all results\n",
    "all_results = []\n",
    "\n",
    "for edge_type in sorted(edge_types):\n",
    "    edge_dir = results_dir / f'{edge_type}_results'\n",
    "    \n",
    "    # Load parameters\n",
    "    params_file = edge_dir / f'{edge_type}_learned_parameters.json'\n",
    "    metrics_file = edge_dir / f'{edge_type}_metrics.json'\n",
    "    \n",
    "    if not params_file.exists() or not metrics_file.exists():\n",
    "        print(f\"⚠ Warning: Missing files for {edge_type}\")\n",
    "        continue\n",
    "    \n",
    "    with open(params_file, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    with open(metrics_file, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    # Combine into summary\n",
    "    result = {\n",
    "        'edge_type': edge_type,\n",
    "        'N_min': params['N_min'],\n",
    "        'edges': params['graph_stats']['m'],\n",
    "        'density': params['graph_stats']['density'],\n",
    "        'n_sources': params['graph_stats']['n_sources'],\n",
    "        'n_targets': params['graph_stats']['n_targets'],\n",
    "        # Learned parameters\n",
    "        'α': params['α'],\n",
    "        'β': params['β'],\n",
    "        'γ': params['γ'],\n",
    "        'δ': params['δ'],\n",
    "        'ε': params['ε'],\n",
    "        'ζ': params['ζ'],\n",
    "        'η': params['η'],\n",
    "        'θ': params['θ'],\n",
    "        'κ': params['κ'],\n",
    "        # Performance metrics\n",
    "        'learned_mae': metrics['final_metrics']['mae'],\n",
    "        'learned_correlation': metrics['final_metrics']['correlation'],\n",
    "        'baseline_mae': metrics['baseline_metrics']['mae'],\n",
    "        'baseline_correlation': metrics['baseline_metrics']['correlation']\n",
    "    }\n",
    "    \n",
    "    # Calculate improvements\n",
    "    result['mae_improvement'] = (result['baseline_mae'] - result['learned_mae']) / result['baseline_mae'] * 100\n",
    "    result['corr_improvement'] = (result['learned_correlation'] - result['baseline_correlation']) / result['baseline_correlation'] * 100\n",
    "    \n",
    "    all_results.append(result)\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"\\nLoaded {len(summary_df)} complete results\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(summary_df[['edge_type', 'N_min', 'edges', 'density', 'learned_correlation', 'baseline_correlation']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LEARNED ANALYTICAL FORMULA - SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nEdge types analyzed: {len(summary_df)}\")\n",
    "\n",
    "print(f\"\\nMinimum Permutations (N_min):\")\n",
    "print(f\"  Range: {summary_df['N_min'].min()} - {summary_df['N_min'].max()}\")\n",
    "print(f\"  Mean: {summary_df['N_min'].mean():.1f}\")\n",
    "print(f\"  Median: {summary_df['N_min'].median():.0f}\")\n",
    "\n",
    "print(f\"\\nGraph Characteristics:\")\n",
    "print(f\"  Edges: {summary_df['edges'].min():,} - {summary_df['edges'].max():,}\")\n",
    "print(f\"  Density: {summary_df['density'].min():.4f} - {summary_df['density'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nPerformance (Correlation vs 200-perm empirical):\")\n",
    "print(f\"  Learned:    {summary_df['learned_correlation'].min():.4f} - {summary_df['learned_correlation'].max():.4f} (mean: {summary_df['learned_correlation'].mean():.4f})\")\n",
    "print(f\"  Analytical: {summary_df['baseline_correlation'].min():.4f} - {summary_df['baseline_correlation'].max():.4f} (mean: {summary_df['baseline_correlation'].mean():.4f})\")\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  MAE: {summary_df['mae_improvement'].mean():+.1f}% (range: {summary_df['mae_improvement'].min():+.1f}% to {summary_df['mae_improvement'].max():+.1f}%)\")\n",
    "print(f\"  Correlation: {summary_df['corr_improvement'].mean():+.1f}% (range: {summary_df['corr_improvement'].min():+.1f}% to {summary_df['corr_improvement'].max():+.1f}%)\")\n",
    "\n",
    "# Count improvements\n",
    "n_improved = (summary_df['corr_improvement'] > 0).sum()\n",
    "print(f\"\\nEdge types with improved correlation: {n_improved}/{len(summary_df)} ({n_improved/len(summary_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship: Graph Properties vs N_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# N_min vs Density\n",
    "axes[0, 0].scatter(summary_df['density'], summary_df['N_min'], s=100, alpha=0.6)\n",
    "axes[0, 0].set_xlabel('Edge Density', fontsize=12)\n",
    "axes[0, 0].set_ylabel('N_min (Minimum Permutations)', fontsize=12)\n",
    "axes[0, 0].set_title('Minimum Permutations vs Density', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xscale('log')\n",
    "\n",
    "# Add trendline\n",
    "z = np.polyfit(np.log(summary_df['density']), summary_df['N_min'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_trend = np.sort(summary_df['density'])\n",
    "axes[0, 0].plot(x_trend, p(np.log(x_trend)), \"r--\", alpha=0.5, linewidth=2, label='Trend')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# N_min vs Number of Edges\n",
    "axes[0, 1].scatter(summary_df['edges'], summary_df['N_min'], s=100, alpha=0.6, color='green')\n",
    "axes[0, 1].set_xlabel('Number of Edges', fontsize=12)\n",
    "axes[0, 1].set_ylabel('N_min (Minimum Permutations)', fontsize=12)\n",
    "axes[0, 1].set_title('Minimum Permutations vs Graph Size', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xscale('log')\n",
    "\n",
    "# Density distribution\n",
    "axes[1, 0].hist(summary_df['density'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Edge Density', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].set_title('Distribution of Graph Densities', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axvline(0.03, color='red', linestyle='--', linewidth=2, label='Sparse threshold')\n",
    "axes[1, 0].axvline(0.05, color='orange', linestyle='--', linewidth=2, label='Dense threshold')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# N_min distribution\n",
    "axes[1, 1].hist(summary_df['N_min'], bins=15, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 1].set_xlabel('N_min (Minimum Permutations)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Distribution of Minimum Permutations', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axvline(summary_df['N_min'].median(), color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Median = {summary_df[\"N_min\"].median():.0f}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'summary_graph_properties_vs_N_min.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Graph properties plot saved to: {results_dir / 'summary_graph_properties_vs_N_min.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: Learned vs Analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Correlation comparison\n",
    "x_pos = np.arange(len(summary_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, summary_df['baseline_correlation'], width, \n",
    "            label='Current Analytical', alpha=0.7, color='orange')\n",
    "axes[0].bar(x_pos + width/2, summary_df['learned_correlation'], width, \n",
    "            label='Learned Formula', alpha=0.7, color='blue')\n",
    "axes[0].set_xlabel('Edge Type', fontsize=12)\n",
    "axes[0].set_ylabel('Correlation', fontsize=12)\n",
    "axes[0].set_title('Correlation with 200-Perm Empirical', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(summary_df['edge_type'], rotation=90, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Improvement scatter\n",
    "axes[1].scatter(summary_df['baseline_correlation'], summary_df['corr_improvement'], \n",
    "                s=100, alpha=0.6, c=summary_df['N_min'], cmap='viridis')\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "axes[1].set_xlabel('Baseline Correlation', fontsize=12)\n",
    "axes[1].set_ylabel('Improvement (%)', fontsize=12)\n",
    "axes[1].set_title('Improvement vs Baseline Performance', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(axes[1].collections[0], ax=axes[1])\n",
    "cbar.set_label('N_min', fontsize=10)\n",
    "\n",
    "# Add labels for edge types with large improvements\n",
    "for idx, row in summary_df.iterrows():\n",
    "    if abs(row['corr_improvement']) > 5:  # Label if >5% improvement\n",
    "        axes[1].annotate(row['edge_type'], \n",
    "                        (row['baseline_correlation'], row['corr_improvement']),\n",
    "                        fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'summary_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Performance comparison plot saved to: {results_dir / 'summary_performance_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Parameters Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze learned parameters\n",
    "param_cols = ['α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'κ']\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, param in enumerate(param_cols):\n",
    "    axes[i].hist(summary_df[param], bins=15, edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_xlabel(param, fontsize=12, fontweight='bold')\n",
    "    axes[i].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[i].set_title(f'Distribution of {param}', fontsize=12)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].axvline(summary_df[param].mean(), color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean={summary_df[param].mean():.3f}')\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'summary_learned_parameters.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Parameter distributions saved to: {results_dir / 'summary_learned_parameters.png'}\")\n",
    "\n",
    "# Print parameter statistics\n",
    "print(\"\\nLearned Parameter Statistics:\")\n",
    "print(\"=\"*80)\n",
    "for param in param_cols:\n",
    "    print(f\"{param}: mean={summary_df[param].mean():.4f}, std={summary_df[param].std():.4f}, range=[{summary_df[param].min():.4f}, {summary_df[param].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize by Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize graphs\n",
    "summary_df['category'] = pd.cut(summary_df['density'], \n",
    "                                 bins=[0, 0.01, 0.03, 0.05, 1.0],\n",
    "                                 labels=['Very Sparse (<1%)', 'Sparse (1-3%)', \n",
    "                                        'Medium (3-5%)', 'Dense (>5%)'])\n",
    "\n",
    "# Summary by category\n",
    "category_summary = summary_df.groupby('category').agg({\n",
    "    'edge_type': 'count',\n",
    "    'N_min': ['mean', 'median', 'std'],\n",
    "    'learned_correlation': ['mean', 'min', 'max'],\n",
    "    'corr_improvement': ['mean', 'min', 'max']\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY BY GRAPH DENSITY CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "print(category_summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Visualize by category\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# N_min by category\n",
    "category_summary['N_min']['mean'].plot(kind='bar', ax=axes[0], color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Graph Category', fontsize=12)\n",
    "axes[0].set_ylabel('Mean N_min', fontsize=12)\n",
    "axes[0].set_title('Average Minimum Permutations by Graph Density', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Improvement by category\n",
    "category_summary['corr_improvement']['mean'].plot(kind='bar', ax=axes[1], color='green', alpha=0.7)\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Graph Category', fontsize=12)\n",
    "axes[1].set_ylabel('Mean Correlation Improvement (%)', fontsize=12)\n",
    "axes[1].set_title('Average Improvement by Graph Density', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'summary_by_density_category.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Category summary plot saved to: {results_dir / 'summary_by_density_category.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full summary\n",
    "summary_file = results_dir / 'learned_analytical_summary.csv'\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "print(f\"Full summary saved to: {summary_file}\")\n",
    "\n",
    "# Save category summary\n",
    "category_file = results_dir / 'summary_by_category.csv'\n",
    "category_summary.to_csv(category_file)\n",
    "print(f\"Category summary saved to: {category_file}\")\n",
    "\n",
    "# Create top performers table\n",
    "top_improved = summary_df.nlargest(10, 'corr_improvement')[[\n",
    "    'edge_type', 'density', 'N_min', 'baseline_correlation', \n",
    "    'learned_correlation', 'corr_improvement'\n",
    "]].round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 EDGE TYPES WITH LARGEST IMPROVEMENT\")\n",
    "print(\"=\"*80)\n",
    "print(top_improved.to_string(index=False))\n",
    "\n",
    "top_improved_file = results_dir / 'top_improved_edge_types.csv'\n",
    "top_improved.to_csv(top_improved_file, index=False)\n",
    "print(f\"\\nTop improved edge types saved to: {top_improved_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS FOR FUTURE GRAPHS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Recommendations based on density\n",
    "very_sparse = summary_df[summary_df['density'] < 0.01]\n",
    "sparse = summary_df[(summary_df['density'] >= 0.01) & (summary_df['density'] < 0.03)]\n",
    "medium = summary_df[(summary_df['density'] >= 0.03) & (summary_df['density'] < 0.05)]\n",
    "dense = summary_df[summary_df['density'] >= 0.05]\n",
    "\n",
    "print(\"\\nMinimum Permutations by Graph Type:\")\n",
    "print(\"-\" * 80)\n",
    "if len(very_sparse) > 0:\n",
    "    print(f\"Very Sparse (<1% density): N_min ≈ {very_sparse['N_min'].median():.0f} (range: {very_sparse['N_min'].min()}-{very_sparse['N_min'].max()})\")\n",
    "if len(sparse) > 0:\n",
    "    print(f\"Sparse (1-3% density):     N_min ≈ {sparse['N_min'].median():.0f} (range: {sparse['N_min'].min()}-{sparse['N_min'].max()})\")\n",
    "if len(medium) > 0:\n",
    "    print(f\"Medium (3-5% density):     N_min ≈ {medium['N_min'].median():.0f} (range: {medium['N_min'].min()}-{medium['N_min'].max()})\")\n",
    "if len(dense) > 0:\n",
    "    print(f\"Dense (>5% density):       N_min ≈ {dense['N_min'].median():.0f} (range: {dense['N_min'].min()}-{dense['N_min'].max()})\")\n",
    "\n",
    "print(\"\\nExpected Performance:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Correlation with 200-perm empirical: {summary_df['learned_correlation'].mean():.4f} ± {summary_df['learned_correlation'].std():.4f}\")\n",
    "print(f\"Improvement over current analytical: {summary_df['corr_improvement'].mean():+.1f}% ± {summary_df['corr_improvement'].std():.1f}%\")\n",
    "\n",
    "print(\"\\nRecommended Strategy:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"1. For NEW graph with unknown density:\")\n",
    "print(f\"   - Start with N = {int(summary_df['N_min'].median())} permutations (median across all graphs)\")\n",
    "print(f\"   - Check convergence: if improvement < 2%, N is sufficient\")\n",
    "print(f\"   - If needed, increase to N = {int(summary_df['N_min'].quantile(0.75))} (75th percentile)\")\n",
    "print(\"\\n2. For graphs similar to existing:\")\n",
    "print(\"   - Use N_min from most similar graph by density\")\n",
    "print(\"   - Expected accuracy: r > 0.95 with 200-perm empirical\")\n",
    "print(\"\\n3. For production use:\")\n",
    "print(f\"   - Conservative estimate: N = {summary_df['N_min'].max()} (maximum observed)\")\n",
    "print(\"   - Ensures high confidence across all graph types\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
