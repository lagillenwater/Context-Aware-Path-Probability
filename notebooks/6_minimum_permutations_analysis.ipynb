{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Permutations Analysis for ML Models\n",
    "\n",
    "This notebook determines the minimum number of permutations needed for ML models (from **Notebook 4**) to accurately learn empirical edge frequency distributions.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Progressive Training**: Train models using 1, 2, 3, 5, 7, 10, ... up to max_permutations\n",
    "2. **Models Tested**: All models from Notebook 4:\n",
    "   - Simple Neural Network\n",
    "   - Random Forest\n",
    "   - Logistic Regression\n",
    "   - Polynomial Logistic Regression\n",
    "3. **Evaluation**: Compare predictions against 200-permutation empirical frequencies\n",
    "4. **Convergence Detection**: Identify when additional permutations provide diminishing returns\n",
    "\n",
    "## Usage with Papermill\n",
    "\n",
    "```bash\n",
    "papermill 6_minimum_permutations_analysis.ipynb output.ipynb \\\n",
    "  -p edge_type \"CtD\" \\\n",
    "  -p max_permutations 50 \\\n",
    "  -p convergence_threshold 0.02 \\\n",
    "  -p target_metric \"correlation\" \\\n",
    "  -p min_metric_value 0.90\n",
    "```\n",
    "\n",
    "## Output\n",
    "\n",
    "- Minimum N for each model to achieve target performance\n",
    "- Convergence curves showing learning progression\n",
    "- Model comparison: which learns fastest from limited data?\n",
    "- Saved results for cross-edge-type analysis in Notebook 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill parameters\n",
    "edge_type = \"CtD\"  # Edge type to analyze\n",
    "max_permutations = 50  # Maximum number of permutations to test\n",
    "convergence_threshold = 0.02  # Improvement threshold for convergence (2%)\n",
    "target_metric = \"correlation\"  # Metric to optimize: 'correlation', 'mae', or 'rmse'\n",
    "min_metric_value = 0.90  # Target minimum correlation (or max MAE/RMSE)\n",
    "random_seed = 42  # Random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd().parent\n",
    "src_dir = repo_dir / 'src'\n",
    "data_dir = repo_dir / 'data'\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "# Import from notebook 4 pipeline\n",
    "from model_comparison import ModelCollection, prepare_edge_features_and_labels, filter_zero_degree_nodes\n",
    "from model_training import ModelTrainer, predict_with_model\n",
    "from model_evaluation import ModelEvaluator\n",
    "\n",
    "print(f\"Repository directory: {repo_dir}\")\n",
    "print(f\"Edge type: {edge_type}\")\n",
    "print(f\"Max permutations: {max_permutations}\")\n",
    "print(f\"Target: {target_metric} >= {min_metric_value}\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Setup output directory\n",
    "output_dir = repo_dir / 'results' / 'minimum_permutations_ml' / f'{edge_type}_results'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Empirical Frequencies (200-perm Gold Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load empirical frequencies from notebook 3\n",
    "empirical_file = repo_dir / 'results' / 'empirical_edge_frequencies' / f'edge_frequency_by_degree_{edge_type}.csv'\n",
    "\n",
    "if not empirical_file.exists():\n",
    "    raise FileNotFoundError(f\"Empirical frequency file not found: {empirical_file}\\nRun notebook 3 first.\")\n",
    "\n",
    "empirical_df = pd.read_csv(empirical_file)\n",
    "print(f\"Loaded {len(empirical_df)} empirical frequency records\")\n",
    "print(f\"Degree ranges: source {empirical_df['source_degree'].min()}-{empirical_df['source_degree'].max()}, \"\n",
    "      f\"target {empirical_df['target_degree'].min()}-{empirical_df['target_degree'].max()}\")\n",
    "print(f\"Frequency range: {empirical_df['frequency'].min():.6f} - {empirical_df['frequency'].max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discover Available Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all permutation directories\n",
    "permutations_dir = data_dir / 'permutations'\n",
    "permutation_dirs = sorted([p for p in permutations_dir.iterdir() if p.is_dir() and p.name.endswith('.hetmat')])\n",
    "\n",
    "print(f\"Found {len(permutation_dirs)} permutation directories\")\n",
    "if len(permutation_dirs) < max_permutations:\n",
    "    print(f\"WARNING: Only {len(permutation_dirs)} available, but max_permutations={max_permutations}\")\n",
    "    print(f\"Will test up to {len(permutation_dirs)} permutations\")\n",
    "    max_permutations = min(max_permutations, len(permutation_dirs))\n",
    "\n",
    "# Verify edge files exist\n",
    "for perm_dir in permutation_dirs[:3]:\n",
    "    edge_file = perm_dir / 'edges' / f'{edge_type}.sparse.npz'\n",
    "    print(f\"  {perm_dir.name}: {'✓' if edge_file.exists() else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Progressive Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_permutations(edge_type: str, num_permutations: int, permutation_dirs: List[Path]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load and combine data from N permutations.\n",
    "    \n",
    "    Returns:\n",
    "        features: Combined features from all permutations\n",
    "        labels: Combined labels from all permutations\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(f\"Loading {num_permutations} permutations...\")\n",
    "    \n",
    "    for i in range(num_permutations):\n",
    "        perm_dir = permutation_dirs[i]\n",
    "        edge_file = perm_dir / 'edges' / f'{edge_type}.sparse.npz'\n",
    "        \n",
    "        if not edge_file.exists():\n",
    "            print(f\"  WARNING: {edge_file} not found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Load edge matrix and prepare features\n",
    "        features, labels = prepare_edge_features_and_labels(\n",
    "            str(edge_file),\n",
    "            sample_ratio=0.01,  # Use same sampling as notebook 4\n",
    "            adaptive_sampling=True,\n",
    "            enhanced_features=False  # Use basic 2D features\n",
    "        )\n",
    "        \n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "        print(f\"  Loaded {perm_dir.name}: {len(features)} samples\")\n",
    "    \n",
    "    # Combine all permutations\n",
    "    combined_features = np.vstack(all_features)\n",
    "    combined_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    print(f\"Combined: {len(combined_features)} total samples\")\n",
    "    print(f\"  Positive: {np.sum(combined_labels)} ({np.mean(combined_labels):.1%})\")\n",
    "    \n",
    "    return combined_features, combined_labels\n",
    "\n",
    "\n",
    "def train_models_with_n_permutations(edge_type: str, num_permutations: int, permutation_dirs: List[Path]) -> Dict:\n",
    "    \"\"\"\n",
    "    Train all models from notebook 4 using N permutations.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with trained models and metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with {num_permutations} permutation(s)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load and combine data\n",
    "    features, labels = load_and_combine_permutations(edge_type, num_permutations, permutation_dirs)\n",
    "    \n",
    "    # Get edge file for model parameter adaptation\n",
    "    ref_edge_file = permutation_dirs[0] / 'edges' / f'{edge_type}.sparse.npz'\n",
    "    \n",
    "    # Create models (same as notebook 4)\n",
    "    model_collection = ModelCollection(random_state=random_seed)\n",
    "    models = model_collection.create_models(\n",
    "        use_class_weights=True,\n",
    "        input_dim=features.shape[1],\n",
    "        edge_file_path=str(ref_edge_file)\n",
    "    )\n",
    "    \n",
    "    # Train all models\n",
    "    trainer = ModelTrainer(random_state=random_seed)\n",
    "    training_results = trainer.train_all_models(\n",
    "        models, features, labels,\n",
    "        test_size=0.2,\n",
    "        val_size=0.1\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'num_permutations': num_permutations,\n",
    "        'training_results': training_results,\n",
    "        'n_samples': len(features)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_against_empirical(training_results: Dict, empirical_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate model predictions against 200-perm empirical frequencies.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics for each model\n",
    "    \"\"\"\n",
    "    # Prepare empirical features\n",
    "    empirical_features = empirical_df[['source_degree', 'target_degree']].values\n",
    "    empirical_freqs = empirical_df['frequency'].values\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model_result in training_results.items():\n",
    "        if model_name == 'data_splits':\n",
    "            continue\n",
    "        \n",
    "        model = model_result['model']\n",
    "        scaler = model_result['training_result'].get('scaler')\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = predict_with_model(model, empirical_features, model_name, scaler)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = np.mean(np.abs(predictions - empirical_freqs))\n",
    "        rmse = np.sqrt(np.mean((predictions - empirical_freqs) ** 2))\n",
    "        correlation = np.corrcoef(predictions, empirical_freqs)[0, 1]\n",
    "        \n",
    "        if np.isnan(correlation):\n",
    "            correlation = 0.0\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'correlation': correlation,\n",
    "            'predictions': predictions\n",
    "        }\n",
    "        \n",
    "        print(f\"{model_name:30} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, Corr: {correlation:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Progressive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define N values to test\n",
    "N_candidates = [1, 2, 3, 5, 7, 10, 15, 20, 30, 40, 50]\n",
    "N_candidates = [n for n in N_candidates if n <= max_permutations]\n",
    "\n",
    "print(f\"Testing N values: {N_candidates}\")\n",
    "\n",
    "# Storage for results\n",
    "all_results = []\n",
    "model_names = None\n",
    "\n",
    "for N in N_candidates:\n",
    "    # Train models with N permutations\n",
    "    training_output = train_models_with_n_permutations(edge_type, N, permutation_dirs)\n",
    "    \n",
    "    # Evaluate against empirical\n",
    "    print(\"\\nEvaluating against 200-perm empirical frequencies:\")\n",
    "    eval_results = evaluate_against_empirical(training_output['training_results'], empirical_df)\n",
    "    \n",
    "    # Store model names on first iteration\n",
    "    if model_names is None:\n",
    "        model_names = list(eval_results.keys())\n",
    "    \n",
    "    # Store results\n",
    "    result_entry = {\n",
    "        'N': N,\n",
    "        'n_samples': training_output['n_samples']\n",
    "    }\n",
    "    \n",
    "    for model_name, metrics in eval_results.items():\n",
    "        result_entry[f'{model_name}_mae'] = metrics['mae']\n",
    "        result_entry[f'{model_name}_rmse'] = metrics['rmse']\n",
    "        result_entry[f'{model_name}_correlation'] = metrics['correlation']\n",
    "    \n",
    "    all_results.append(result_entry)\n",
    "    \n",
    "    # Check convergence for each model\n",
    "    if len(all_results) >= 2:\n",
    "        print(\"\\nConvergence check:\")\n",
    "        prev_result = all_results[-2]\n",
    "        curr_result = all_results[-1]\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            metric_key = f'{model_name}_{target_metric}'\n",
    "            improvement = abs(curr_result[metric_key] - prev_result[metric_key])\n",
    "            \n",
    "            if target_metric in ['mae', 'rmse']:\n",
    "                improvement_pct = improvement / (prev_result[metric_key] + 1e-10)\n",
    "            else:\n",
    "                improvement_pct = improvement\n",
    "            \n",
    "            status = \"✓ CONVERGED\" if improvement_pct < convergence_threshold else \"  continuing\"\n",
    "            print(f\"  {model_name:30} improvement: {improvement:.4f} ({improvement_pct:.1%}) {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"\\nProgressive analysis complete!\")\n",
    "print(f\"Tested N = {results_df['N'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Convergence and Find N_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_N_min_for_model(results_df: pd.DataFrame, model_name: str, target_metric: str, min_value: float) -> Dict:\n",
    "    \"\"\"\n",
    "    Find minimum N where model achieves target performance.\n",
    "    \"\"\"\n",
    "    metric_col = f'{model_name}_{target_metric}'\n",
    "    \n",
    "    if target_metric in ['mae', 'rmse']:\n",
    "        # Lower is better\n",
    "        passing = results_df[results_df[metric_col] <= min_value]\n",
    "    else:\n",
    "        # Higher is better (correlation)\n",
    "        passing = results_df[results_df[metric_col] >= min_value]\n",
    "    \n",
    "    if len(passing) > 0:\n",
    "        N_min = passing['N'].min()\n",
    "        achieved_value = passing[passing['N'] == N_min].iloc[0][metric_col]\n",
    "        return {\n",
    "            'N_min': int(N_min),\n",
    "            'achieved': achieved_value,\n",
    "            'target_met': True\n",
    "        }\n",
    "    else:\n",
    "        # Target not met, return best\n",
    "        if target_metric in ['mae', 'rmse']:\n",
    "            best_idx = results_df[metric_col].idxmin()\n",
    "        else:\n",
    "            best_idx = results_df[metric_col].idxmax()\n",
    "        \n",
    "        return {\n",
    "            'N_min': int(results_df.loc[best_idx, 'N']),\n",
    "            'achieved': results_df.loc[best_idx, metric_col],\n",
    "            'target_met': False\n",
    "        }\n",
    "\n",
    "# Find N_min for each model\n",
    "convergence_summary = {}\n",
    "\n",
    "print(f\"\\nFinding N_min for each model (target: {target_metric} {'≤' if target_metric in ['mae', 'rmse'] else '≥'} {min_metric_value}):\\n\")\n",
    "\n",
    "for model_name in model_names:\n",
    "    result = find_N_min_for_model(results_df, model_name, target_metric, min_metric_value)\n",
    "    convergence_summary[model_name] = result\n",
    "    \n",
    "    status = \"✓ TARGET MET\" if result['target_met'] else \"✗ Target not met\"\n",
    "    print(f\"{model_name:30} N_min = {result['N_min']:3d}  ({target_metric} = {result['achieved']:.4f})  {status}\")\n",
    "\n",
    "# Overall minimum N (most data-efficient model)\n",
    "min_N_overall = min(r['N_min'] for r in convergence_summary.values())\n",
    "best_models = [m for m, r in convergence_summary.items() if r['N_min'] == min_N_overall]\n",
    "\n",
    "print(f\"\\nMost data-efficient model(s): {', '.join(best_models)} (N_min = {min_N_overall})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create convergence curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics_to_plot = ['mae', 'rmse', 'correlation']\n",
    "titles = ['MAE vs N Permutations', 'RMSE vs N Permutations', 'Correlation vs N Permutations']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        col = f'{model_name}_{metric}'\n",
    "        ax.plot(results_df['N'], results_df[col], marker='o', label=model_name, linewidth=2)\n",
    "    \n",
    "    # Add target line if this is the target metric\n",
    "    if metric == target_metric:\n",
    "        ax.axhline(min_metric_value, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Target ({min_metric_value})', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Number of Permutations', fontsize=12)\n",
    "    ax.set_ylabel(metric.upper(), fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'convergence_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved convergence curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_min comparison bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "n_mins = [convergence_summary[m]['N_min'] for m in model_names]\n",
    "colors = ['green' if convergence_summary[m]['target_met'] else 'orange' for m in model_names]\n",
    "\n",
    "bars = ax.bar(range(len(model_names)), n_mins, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(range(len(model_names)))\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.set_ylabel('Minimum N Permutations', fontsize=12)\n",
    "ax.set_title(f'N_min by Model (Target: {target_metric} {\"≤\" if target_metric in [\"mae\", \"rmse\"] else \"≥\"} {min_metric_value})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, n_min) in enumerate(zip(bars, n_mins)):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "            str(n_min), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', alpha=0.7, label='Target Met'),\n",
    "    Patch(facecolor='orange', alpha=0.7, label='Target Not Met')\n",
    "]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'N_min_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved N_min comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results_df.to_csv(output_dir / f'{edge_type}_convergence_data.csv', index=False)\n",
    "print(f\"Saved convergence data: {output_dir / f'{edge_type}_convergence_data.csv'}\")\n",
    "\n",
    "# Save convergence summary\n",
    "summary_data = {\n",
    "    'edge_type': edge_type,\n",
    "    'target_metric': target_metric,\n",
    "    'min_metric_value': min_metric_value,\n",
    "    'convergence_threshold': convergence_threshold,\n",
    "    'N_tested': results_df['N'].tolist(),\n",
    "    'models': {}\n",
    "}\n",
    "\n",
    "for model_name, result in convergence_summary.items():\n",
    "    summary_data['models'][model_name] = {\n",
    "        'N_min': result['N_min'],\n",
    "        'achieved_value': result['achieved'],\n",
    "        'target_met': result['target_met']\n",
    "    }\n",
    "\n",
    "with open(output_dir / f'{edge_type}_summary.json', 'w') as f:\n",
    "    json.dump(summary_data, f, indent=2)\n",
    "print(f\"Saved summary: {output_dir / f'{edge_type}_summary.json'}\")\n",
    "\n",
    "# Save text report\n",
    "with open(output_dir / f'{edge_type}_report.txt', 'w') as f:\n",
    "    f.write(f\"Minimum Permutations Analysis - {edge_type}\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Target: {target_metric} {'≤' if target_metric in ['mae', 'rmse'] else '≥'} {min_metric_value}\\n\")\n",
    "    f.write(f\"Convergence threshold: {convergence_threshold}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Results by Model:\\n\")\n",
    "    f.write(\"-\"*60 + \"\\n\")\n",
    "    for model_name, result in convergence_summary.items():\n",
    "        status = \"✓ TARGET MET\" if result['target_met'] else \"✗ Target not met\"\n",
    "        f.write(f\"{model_name:30} N_min = {result['N_min']:3d}  \"\n",
    "                f\"({target_metric} = {result['achieved']:.4f})  {status}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Most data-efficient: {', '.join(best_models)} (N_min = {min_N_overall})\\n\")\n",
    "\n",
    "print(f\"Saved report: {output_dir / f'{edge_type}_report.txt'}\")\n",
    "print(\"\\nAll results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"MINIMUM PERMUTATIONS ANALYSIS - {edge_type}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTarget: {target_metric} {'≤' if target_metric in ['mae', 'rmse'] else '≥'} {min_metric_value}\")\n",
    "print(f\"N values tested: {results_df['N'].tolist()}\")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for model_name, result in convergence_summary.items():\n",
    "    status = \"✓\" if result['target_met'] else \"✗\"\n",
    "    print(f\"  {status} {model_name:30} N_min = {result['N_min']:3d}  ({target_metric} = {result['achieved']:.4f})\")\n",
    "\n",
    "print(f\"\\nMost data-efficient model: {', '.join(best_models)} (N_min = {min_N_overall})\")\n",
    "print(f\"\\nOutput directory: {output_dir}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
