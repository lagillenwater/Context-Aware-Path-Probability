{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metapath Probability Analysis and Anomaly Detection\n",
    "\n",
    "This notebook analyzes the compositionality of metapath probabilities in hetionet and implements anomaly detection for actual paths.\n",
    "\n",
    "## Research Questions:\n",
    "1. **Compositionality**: Are metapath probabilities compositional (independent edges) or conditional (dependent edges)?\n",
    "2. **Degree Dependency**: How do source/target degrees affect path probabilities?\n",
    "3. **Anomaly Detection**: Can we identify unusual paths based on predicted vs actual probabilities?\n",
    "\n",
    "## Workflow:\n",
    "1. **Data Loading & Preparation**\n",
    "2. **Metapath Extraction**\n",
    "3. **Probability Calculation Methods**\n",
    "4. **Compositionality Testing**\n",
    "5. **Conditional Probability Modeling**\n",
    "6. **Anomaly Detection Implementation**\n",
    "7. **Validation & Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["parameters"]
   },
   "outputs": [],
   "source": [
    "# Papermill parameters\n",
    "metapath_pattern = \"CbGpPWpG\"  # Default metapath to analyze\n",
    "edge_types = [\"CbG\", \"GpPW\", \"GpPW\"]  # Edge types in the metapath\n",
    "anomaly_threshold = 0.05  # Threshold for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "import scipy.sparse as sp\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd().parent\n",
    "data_dir = repo_dir / 'data'\n",
    "results_dir = repo_dir / 'results' / 'metapath_analysis'\n",
    "prediction_dir = repo_dir / 'results' / 'model_comparison'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Analyzing metapath: {metapath_pattern}\")\n",
    "print(f\"Edge types: {edge_types}\")\n",
    "print(f\"Results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetapathAnalyzer:\n",
    "    \"\"\"Comprehensive metapath probability analysis and anomaly detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, metapath_pattern, edge_types):\n",
    "        self.metapath_pattern = metapath_pattern\n",
    "        self.edge_types = edge_types\n",
    "        self.edge_predictions = {}  # Store predictions for each edge type\n",
    "        self.edge_matrices = {}     # Store actual edge matrices\n",
    "        self.node_mappings = {}     # Store node type mappings\n",
    "        \n",
    "    def load_edge_predictions(self, prediction_dir):\n",
    "        \"\"\"Load model predictions for all edge types in the metapath.\"\"\"\n",
    "        print(\"Loading edge prediction files...\")\n",
    "        \n",
    "        for edge_type in self.edge_types:\n",
    "            # Load predictions\n",
    "            pred_file = prediction_dir / f\"{edge_type}_results\" / f\"{edge_type}_all_model_predictions.csv.gz\"\n",
    "            \n",
    "            if pred_file.exists():\n",
    "                print(f\"  Loading {edge_type}: {pred_file}\")\n",
    "                df = pd.read_csv(pred_file)\n",
    "                self.edge_predictions[edge_type] = df\n",
    "                print(f\"    Shape: {df.shape}, Edges: {df['edge_exists'].sum():,}\")\n",
    "            else:\n",
    "                print(f\"  ‚ö† Missing prediction file: {pred_file}\")\n",
    "                \n",
    "            # Load actual edge matrix\n",
    "            matrix_file = data_dir / 'permutations' / '000.hetmat' / 'edges' / f\"{edge_type}.sparse.npz\"\n",
    "            if matrix_file.exists():\n",
    "                self.edge_matrices[edge_type] = sp.load_npz(str(matrix_file))\n",
    "                print(f\"    Matrix shape: {self.edge_matrices[edge_type].shape}\")\n",
    "                \n",
    "        return len(self.edge_predictions)\n",
    "    \n",
    "    def extract_metapaths(self, max_paths=10000):\n",
    "        \"\"\"Extract actual metapaths from the hetionet graph.\"\"\"\n",
    "        print(f\"\\nExtracting metapaths for pattern: {self.metapath_pattern}\")\n",
    "        \n",
    "        if len(self.edge_types) != 3:\n",
    "            raise ValueError(\"Currently supports 3-edge metapaths only\")\n",
    "            \n",
    "        edge1_type, edge2_type, edge3_type = self.edge_types\n",
    "        matrix1 = self.edge_matrices[edge1_type]  # C -> G\n",
    "        matrix2 = self.edge_matrices[edge2_type]  # G -> PW\n",
    "        matrix3 = self.edge_matrices[edge3_type]  # PW -> G (reverse)\n",
    "        \n",
    "        metapaths = []\n",
    "        \n",
    "        print(f\"  Searching through {matrix1.nnz:,} edges of type {edge1_type}\")\n",
    "        \n",
    "        # Find 3-edge paths: C-G-PW-G\n",
    "        count = 0\n",
    "        for c_idx, g1_idx in zip(*matrix1.nonzero()):\n",
    "            if count >= max_paths:\n",
    "                break\n",
    "                \n",
    "            # Find pathways connected to this gene\n",
    "            pw_indices = matrix2.getrow(g1_idx).nonzero()[1]\n",
    "            \n",
    "            for pw_idx in pw_indices:\n",
    "                # Find genes connected to this pathway (excluding the original gene)\n",
    "                g2_indices = matrix3.getrow(pw_idx).nonzero()[1]\n",
    "                \n",
    "                for g2_idx in g2_indices:\n",
    "                    if g2_idx != g1_idx:  # Different target gene\n",
    "                        metapaths.append({\n",
    "                            'compound_idx': c_idx,\n",
    "                            'gene1_idx': g1_idx,\n",
    "                            'pathway_idx': pw_idx,\n",
    "                            'gene2_idx': g2_idx,\n",
    "                            'path_id': f\"{c_idx}-{g1_idx}-{pw_idx}-{g2_idx}\"\n",
    "                        })\n",
    "                        count += 1\n",
    "                        \n",
    "                        if count >= max_paths:\n",
    "                            break\n",
    "                if count >= max_paths:\n",
    "                    break\n",
    "                    \n",
    "        print(f\"  Found {len(metapaths):,} metapaths\")\n",
    "        return pd.DataFrame(metapaths)\n",
    "    \n",
    "    def calculate_edge_probabilities(self, metapaths_df, model_name='random_forest'):\n",
    "        \"\"\"Calculate edge probabilities for each edge in the metapaths.\"\"\"\n",
    "        print(f\"\\nCalculating edge probabilities using {model_name} model...\")\n",
    "        \n",
    "        # Add prediction columns\n",
    "        pred_col = f'{model_name}_prediction'\n",
    "        \n",
    "        for i, (edge_type, edge_name) in enumerate(zip(self.edge_types, ['edge1', 'edge2', 'edge3'])):\n",
    "            print(f\"  Processing {edge_name} ({edge_type})...\")\n",
    "            \n",
    "            if edge_type not in self.edge_predictions:\n",
    "                print(f\"    ‚ö† No predictions available for {edge_type}\")\n",
    "                continue\n",
    "                \n",
    "            pred_df = self.edge_predictions[edge_type]\n",
    "            \n",
    "            # Create lookup dictionary for fast access\n",
    "            lookup = dict(zip(\n",
    "                zip(pred_df['source_index'], pred_df['target_index']),\n",
    "                pred_df[pred_col]\n",
    "            ))\n",
    "            \n",
    "            # Map edge probabilities\n",
    "            if i == 0:  # C -> G\n",
    "                metapaths_df[f'{edge_name}_prob'] = metapaths_df.apply(\n",
    "                    lambda row: lookup.get((row['compound_idx'], row['gene1_idx']), 0.0), axis=1\n",
    "                )\n",
    "            elif i == 1:  # G -> PW\n",
    "                metapaths_df[f'{edge_name}_prob'] = metapaths_df.apply(\n",
    "                    lambda row: lookup.get((row['gene1_idx'], row['pathway_idx']), 0.0), axis=1\n",
    "                )\n",
    "            elif i == 2:  # PW -> G\n",
    "                metapaths_df[f'{edge_name}_prob'] = metapaths_df.apply(\n",
    "                    lambda row: lookup.get((row['pathway_idx'], row['gene2_idx']), 0.0), axis=1\n",
    "                )\n",
    "                \n",
    "        return metapaths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = MetapathAnalyzer(metapath_pattern, edge_types)\n",
    "\n",
    "# Load edge predictions\n",
    "loaded_edges = analyzer.load_edge_predictions(prediction_dir)\n",
    "print(f\"\\nLoaded predictions for {loaded_edges}/{len(edge_types)} edge types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Metapath Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract actual metapaths from the graph\n",
    "metapaths_df = analyzer.extract_metapaths(max_paths=50000)\n",
    "\n",
    "print(f\"\\nMetapath extraction results:\")\n",
    "print(f\"  Total metapaths found: {len(metapaths_df):,}\")\n",
    "print(f\"  Unique compounds: {metapaths_df['compound_idx'].nunique():,}\")\n",
    "print(f\"  Unique gene1s: {metapaths_df['gene1_idx'].nunique():,}\")\n",
    "print(f\"  Unique pathways: {metapaths_df['pathway_idx'].nunique():,}\")\n",
    "print(f\"  Unique gene2s: {metapaths_df['gene2_idx'].nunique():,}\")\n",
    "\n",
    "# Show sample metapaths\n",
    "print(f\"\\nSample metapaths:\")\n",
    "print(metapaths_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Probability Calculation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate edge probabilities for all metapaths\n",
    "metapaths_df = analyzer.calculate_edge_probabilities(metapaths_df, model_name='random_forest')\n",
    "\n",
    "# Calculate compositional (independent) path probability\n",
    "metapaths_df['compositional_prob'] = (\n",
    "    metapaths_df['edge1_prob'] * \n",
    "    metapaths_df['edge2_prob'] * \n",
    "    metapaths_df['edge3_prob']\n",
    ")\n",
    "\n",
    "# Remove paths with zero probabilities for cleaner analysis\n",
    "valid_paths = metapaths_df[\n",
    "    (metapaths_df['edge1_prob'] > 0) & \n",
    "    (metapaths_df['edge2_prob'] > 0) & \n",
    "    (metapaths_df['edge3_prob'] > 0)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nProbability calculation results:\")\n",
    "print(f\"  Valid paths with all probabilities > 0: {len(valid_paths):,}\")\n",
    "print(f\"  Compositional probability range: {valid_paths['compositional_prob'].min():.2e} - {valid_paths['compositional_prob'].max():.2e}\")\n",
    "print(f\"  Mean compositional probability: {valid_paths['compositional_prob'].mean():.2e}\")\n",
    "\n",
    "# Show edge probability distributions\n",
    "for edge in ['edge1_prob', 'edge2_prob', 'edge3_prob']:\n",
    "    print(f\"  {edge}: mean={valid_paths[edge].mean():.4f}, std={valid_paths[edge].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compositionality Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositionalityTester:\n",
    "    \"\"\"Test whether metapath probabilities are compositional or conditional.\"\"\"\n",
    "    \n",
    "    def __init__(self, metapaths_df):\n",
    "        self.metapaths_df = metapaths_df\n",
    "        \n",
    "    def test_edge_independence(self):\n",
    "        \"\"\"Test statistical independence between edge probabilities.\"\"\"\n",
    "        print(\"Testing edge independence...\")\n",
    "        \n",
    "        edges = ['edge1_prob', 'edge2_prob', 'edge3_prob']\n",
    "        independence_results = {}\n",
    "        \n",
    "        for i, edge1 in enumerate(edges):\n",
    "            for edge2 in edges[i+1:]:\n",
    "                # Spearman correlation (rank-based, robust to non-linearity)\n",
    "                corr, p_value = stats.spearmanr(\n",
    "                    self.metapaths_df[edge1], \n",
    "                    self.metapaths_df[edge2]\n",
    "                )\n",
    "                \n",
    "                independence_results[f\"{edge1}_vs_{edge2}\"] = {\n",
    "                    'correlation': corr,\n",
    "                    'p_value': p_value,\n",
    "                    'independent': p_value > 0.05 and abs(corr) < 0.1\n",
    "                }\n",
    "                \n",
    "                print(f\"  {edge1} vs {edge2}: r={corr:.4f}, p={p_value:.2e}, independent={independence_results[f'{edge1}_vs_{edge2}']['independent']}\")\n",
    "        \n",
    "        return independence_results\n",
    "    \n",
    "    def calculate_empirical_path_probability(self):\n",
    "        \"\"\"Calculate empirical path probability from observed frequencies.\"\"\"\n",
    "        print(\"\\nCalculating empirical path probabilities...\")\n",
    "        \n",
    "        # Group by degree combinations and calculate empirical frequencies\n",
    "        degree_groups = self.metapaths_df.groupby([\n",
    "            'compound_idx', 'gene1_idx', 'pathway_idx', 'gene2_idx'\n",
    "        ]).size().reset_index(name='path_count')\n",
    "        \n",
    "        # Calculate empirical probability as normalized frequency\n",
    "        total_possible_paths = len(self.metapaths_df)\n",
    "        degree_groups['empirical_prob'] = degree_groups['path_count'] / total_possible_paths\n",
    "        \n",
    "        # Merge back to main dataframe\n",
    "        self.metapaths_df = self.metapaths_df.merge(\n",
    "            degree_groups[['compound_idx', 'gene1_idx', 'pathway_idx', 'gene2_idx', 'empirical_prob']],\n",
    "            on=['compound_idx', 'gene1_idx', 'pathway_idx', 'gene2_idx'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        print(f\"  Empirical probability range: {self.metapaths_df['empirical_prob'].min():.2e} - {self.metapaths_df['empirical_prob'].max():.2e}\")\n",
    "        return self.metapaths_df\n",
    "    \n",
    "    def compare_compositional_vs_empirical(self):\n",
    "        \"\"\"Compare compositional predictions with empirical observations.\"\"\"\n",
    "        print(\"\\nComparing compositional vs empirical probabilities...\")\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr_spearman, p_spearman = stats.spearmanr(\n",
    "            self.metapaths_df['compositional_prob'],\n",
    "            self.metapaths_df['empirical_prob']\n",
    "        )\n",
    "        \n",
    "        corr_pearson, p_pearson = stats.pearsonr(\n",
    "            self.metapaths_df['compositional_prob'],\n",
    "            self.metapaths_df['empirical_prob']\n",
    "        )\n",
    "        \n",
    "        print(f\"  Spearman correlation: r={corr_spearman:.4f}, p={p_spearman:.2e}\")\n",
    "        print(f\"  Pearson correlation: r={corr_pearson:.4f}, p={p_pearson:.2e}\")\n",
    "        \n",
    "        # Calculate R¬≤ for explained variance\n",
    "        from sklearn.metrics import r2_score\n",
    "        r2 = r2_score(self.metapaths_df['empirical_prob'], self.metapaths_df['compositional_prob'])\n",
    "        print(f\"  R¬≤ (explained variance): {r2:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'spearman_r': corr_spearman,\n",
    "            'spearman_p': p_spearman,\n",
    "            'pearson_r': corr_pearson,\n",
    "            'pearson_p': p_pearson,\n",
    "            'r2_score': r2,\n",
    "            'compositional_valid': corr_spearman > 0.7 and p_spearman < 0.01\n",
    "        }\n",
    "\n",
    "# Run compositionality tests\n",
    "tester = CompositionalityTester(valid_paths)\n",
    "independence_results = tester.test_edge_independence()\n",
    "metapaths_with_empirical = tester.calculate_empirical_path_probability()\n",
    "compositional_comparison = tester.compare_compositional_vs_empirical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize compositionality analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Edge probability correlations\n",
    "import seaborn as sns\n",
    "edge_corr_data = metapaths_with_empirical[['edge1_prob', 'edge2_prob', 'edge3_prob']].corr()\n",
    "sns.heatmap(edge_corr_data, annot=True, cmap='coolwarm', center=0, ax=axes[0,0])\n",
    "axes[0,0].set_title('Edge Probability Correlations')\n",
    "\n",
    "# 2. Compositional vs Empirical scatter\n",
    "axes[0,1].scatter(metapaths_with_empirical['compositional_prob'], \n",
    "                  metapaths_with_empirical['empirical_prob'], \n",
    "                  alpha=0.6, s=20)\n",
    "axes[0,1].plot([0, metapaths_with_empirical['compositional_prob'].max()], \n",
    "               [0, metapaths_with_empirical['compositional_prob'].max()], \n",
    "               'r--', alpha=0.8)\n",
    "axes[0,1].set_xlabel('Compositional Probability')\n",
    "axes[0,1].set_ylabel('Empirical Probability')\n",
    "axes[0,1].set_title(f'Compositional vs Empirical\\n(r={compositional_comparison[\"spearman_r\"]:.3f})')\n",
    "axes[0,1].set_xscale('log')\n",
    "axes[0,1].set_yscale('log')\n",
    "\n",
    "# 3. Probability distributions\n",
    "axes[1,0].hist(metapaths_with_empirical['compositional_prob'], bins=50, alpha=0.7, label='Compositional', density=True)\n",
    "axes[1,0].hist(metapaths_with_empirical['empirical_prob'], bins=50, alpha=0.7, label='Empirical', density=True)\n",
    "axes[1,0].set_xlabel('Probability')\n",
    "axes[1,0].set_ylabel('Density')\n",
    "axes[1,0].set_title('Probability Distributions')\n",
    "axes[1,0].set_xscale('log')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Residuals analysis\n",
    "residuals = metapaths_with_empirical['empirical_prob'] - metapaths_with_empirical['compositional_prob']\n",
    "axes[1,1].scatter(metapaths_with_empirical['compositional_prob'], residuals, alpha=0.6, s=20)\n",
    "axes[1,1].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "axes[1,1].set_xlabel('Compositional Probability')\n",
    "axes[1,1].set_ylabel('Residuals (Empirical - Compositional)')\n",
    "axes[1,1].set_title('Residuals Analysis')\n",
    "axes[1,1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / f'{metapath_pattern}_compositionality_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCompositionality Analysis Summary:\")\n",
    "print(f\"  Edge independence: {sum(1 for r in independence_results.values() if r['independent'])}/{len(independence_results)} pairs independent\")\n",
    "print(f\"  Compositional validity: {compositional_comparison['compositional_valid']}\")\n",
    "print(f\"  Explained variance (R¬≤): {compositional_comparison['r2_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conditional Probability Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalProbabilityModel:\n",
    "    \"\"\"Model conditional probabilities for metapaths.\"\"\"\n",
    "    \n",
    "    def __init__(self, metapaths_df):\n",
    "        self.metapaths_df = metapaths_df\n",
    "        \n",
    "    def calculate_conditional_probabilities(self):\n",
    "        \"\"\"Calculate P(edge2|edge1) and P(edge3|edge1,edge2).\"\"\"\n",
    "        print(\"Calculating conditional probabilities...\")\n",
    "        \n",
    "        # Discretize edge probabilities for conditional analysis\n",
    "        n_bins = 10\n",
    "        for edge in ['edge1_prob', 'edge2_prob', 'edge3_prob']:\n",
    "            self.metapaths_df[f'{edge}_bin'] = pd.qcut(\n",
    "                self.metapaths_df[edge], \n",
    "                q=n_bins, \n",
    "                labels=False, \n",
    "                duplicates='drop'\n",
    "            )\n",
    "        \n",
    "        # Calculate P(edge2|edge1)\n",
    "        edge2_given_edge1 = self.metapaths_df.groupby('edge1_prob_bin')['edge2_prob'].mean()\n",
    "        self.metapaths_df['edge2_prob_conditional'] = self.metapaths_df['edge1_prob_bin'].map(edge2_given_edge1)\n",
    "        \n",
    "        # Calculate P(edge3|edge1,edge2)\n",
    "        edge3_given_edge12 = self.metapaths_df.groupby(['edge1_prob_bin', 'edge2_prob_bin'])['edge3_prob'].mean()\n",
    "        self.metapaths_df['edge3_prob_conditional'] = self.metapaths_df.apply(\n",
    "            lambda row: edge3_given_edge12.get((row['edge1_prob_bin'], row['edge2_prob_bin']), row['edge3_prob']),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Calculate conditional path probability\n",
    "        self.metapaths_df['conditional_prob'] = (\n",
    "            self.metapaths_df['edge1_prob'] * \n",
    "            self.metapaths_df['edge2_prob_conditional'] * \n",
    "            self.metapaths_df['edge3_prob_conditional']\n",
    "        )\n",
    "        \n",
    "        print(f\"  Conditional probability range: {self.metapaths_df['conditional_prob'].min():.2e} - {self.metapaths_df['conditional_prob'].max():.2e}\")\n",
    "        return self.metapaths_df\n",
    "    \n",
    "    def compare_probability_models(self):\n",
    "        \"\"\"Compare compositional vs conditional vs empirical probabilities.\"\"\"\n",
    "        print(\"\\nComparing probability models...\")\n",
    "        \n",
    "        models = {\n",
    "            'compositional': 'compositional_prob',\n",
    "            'conditional': 'conditional_prob',\n",
    "            'empirical': 'empirical_prob'\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model1_name, model1_col in models.items():\n",
    "            for model2_name, model2_col in models.items():\n",
    "                if model1_name != model2_name:\n",
    "                    corr, p_val = stats.spearmanr(\n",
    "                        self.metapaths_df[model1_col],\n",
    "                        self.metapaths_df[model2_col]\n",
    "                    )\n",
    "                    \n",
    "                    results[f\"{model1_name}_vs_{model2_name}\"] = {\n",
    "                        'correlation': corr,\n",
    "                        'p_value': p_val\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  {model1_name} vs {model2_name}: r={corr:.4f}, p={p_val:.2e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Run conditional probability modeling\n",
    "conditional_model = ConditionalProbabilityModel(metapaths_with_empirical)\n",
    "metapaths_final = conditional_model.calculate_conditional_probabilities()\n",
    "probability_model_comparison = conditional_model.compare_probability_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetapathAnomalyDetector:\n",
    "    \"\"\"Detect anomalous metapaths based on probability discrepancies.\"\"\"\n",
    "    \n",
    "    def __init__(self, metapaths_df, threshold=0.05):\n",
    "        self.metapaths_df = metapaths_df\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def calculate_anomaly_scores(self):\n",
    "        \"\"\"Calculate anomaly scores based on multiple criteria.\"\"\"\n",
    "        print(f\"Calculating anomaly scores with threshold {self.threshold}...\")\n",
    "        \n",
    "        # 1. Probability residual score (empirical vs predicted)\n",
    "        self.metapaths_df['residual_score'] = abs(\n",
    "            self.metapaths_df['empirical_prob'] - self.metapaths_df['compositional_prob']\n",
    "        ) / (self.metapaths_df['compositional_prob'] + 1e-10)\n",
    "        \n",
    "        # 2. Z-score based on compositional probability distribution\n",
    "        comp_mean = self.metapaths_df['compositional_prob'].mean()\n",
    "        comp_std = self.metapaths_df['compositional_prob'].std()\n",
    "        self.metapaths_df['compositional_zscore'] = abs(\n",
    "            (self.metapaths_df['compositional_prob'] - comp_mean) / comp_std\n",
    "        )\n",
    "        \n",
    "        # 3. Edge probability consistency score\n",
    "        edge_probs = self.metapaths_df[['edge1_prob', 'edge2_prob', 'edge3_prob']]\n",
    "        self.metapaths_df['edge_consistency_score'] = edge_probs.std(axis=1) / (edge_probs.mean(axis=1) + 1e-10)\n",
    "        \n",
    "        # 4. Combined anomaly score\n",
    "        self.metapaths_df['anomaly_score'] = (\n",
    "            0.4 * self.normalize_score(self.metapaths_df['residual_score']) +\n",
    "            0.3 * self.normalize_score(self.metapaths_df['compositional_zscore']) +\n",
    "            0.3 * self.normalize_score(self.metapaths_df['edge_consistency_score'])\n",
    "        )\n",
    "        \n",
    "        return self.metapaths_df\n",
    "    \n",
    "    def normalize_score(self, scores):\n",
    "        \"\"\"Normalize scores to [0, 1] range.\"\"\"\n",
    "        return (scores - scores.min()) / (scores.max() - scores.min() + 1e-10)\n",
    "    \n",
    "    def detect_anomalies(self):\n",
    "        \"\"\"Detect anomalous metapaths based on threshold.\"\"\"\n",
    "        print(\"\\nDetecting anomalies...\")\n",
    "        \n",
    "        # Define anomalies based on percentile threshold\n",
    "        anomaly_threshold_value = self.metapaths_df['anomaly_score'].quantile(1 - self.threshold)\n",
    "        self.metapaths_df['is_anomaly'] = self.metapaths_df['anomaly_score'] > anomaly_threshold_value\n",
    "        \n",
    "        n_anomalies = self.metapaths_df['is_anomaly'].sum()\n",
    "        anomaly_rate = n_anomalies / len(self.metapaths_df)\n",
    "        \n",
    "        print(f\"  Anomaly threshold (score): {anomaly_threshold_value:.4f}\")\n",
    "        print(f\"  Detected anomalies: {n_anomalies:,} ({anomaly_rate:.1%})\")\n",
    "        \n",
    "        # Analyze anomaly characteristics\n",
    "        anomalies = self.metapaths_df[self.metapaths_df['is_anomaly']]\n",
    "        normals = self.metapaths_df[~self.metapaths_df['is_anomaly']]\n",
    "        \n",
    "        print(f\"\\nAnomaly characteristics:\")\n",
    "        print(f\"  Mean compositional prob - Anomalies: {anomalies['compositional_prob'].mean():.2e}\")\n",
    "        print(f\"  Mean compositional prob - Normal: {normals['compositional_prob'].mean():.2e}\")\n",
    "        print(f\"  Mean empirical prob - Anomalies: {anomalies['empirical_prob'].mean():.2e}\")\n",
    "        print(f\"  Mean empirical prob - Normal: {normals['empirical_prob'].mean():.2e}\")\n",
    "        \n",
    "        return self.metapaths_df, anomalies, normals\n",
    "    \n",
    "    def validate_anomaly_detection(self):\n",
    "        \"\"\"Validate anomaly detection using statistical tests.\"\"\"\n",
    "        print(\"\\nValidating anomaly detection...\")\n",
    "        \n",
    "        anomalies = self.metapaths_df[self.metapaths_df['is_anomaly']]\n",
    "        normals = self.metapaths_df[~self.metapaths_df['is_anomaly']]\n",
    "        \n",
    "        # Statistical tests\n",
    "        from scipy.stats import mannwhitneyu\n",
    "        \n",
    "        # Test difference in compositional probabilities\n",
    "        stat_comp, p_comp = mannwhitneyu(\n",
    "            anomalies['compositional_prob'], \n",
    "            normals['compositional_prob'],\n",
    "            alternative='two-sided'\n",
    "        )\n",
    "        \n",
    "        # Test difference in empirical probabilities\n",
    "        stat_emp, p_emp = mannwhitneyu(\n",
    "            anomalies['empirical_prob'], \n",
    "            normals['empirical_prob'],\n",
    "            alternative='two-sided'\n",
    "        )\n",
    "        \n",
    "        print(f\"  Compositional prob difference: U={stat_comp:.0f}, p={p_comp:.2e}\")\n",
    "        print(f\"  Empirical prob difference: U={stat_emp:.0f}, p={p_emp:.2e}\")\n",
    "        \n",
    "        return {\n",
    "            'compositional_test': {'statistic': stat_comp, 'p_value': p_comp},\n",
    "            'empirical_test': {'statistic': stat_emp, 'p_value': p_emp}\n",
    "        }\n",
    "\n",
    "# Run anomaly detection\n",
    "detector = MetapathAnomalyDetector(metapaths_final, threshold=anomaly_threshold)\n",
    "metapaths_with_scores = detector.calculate_anomaly_scores()\n",
    "metapaths_final, anomalies, normals = detector.detect_anomalies()\n",
    "validation_results = detector.validate_anomaly_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomaly detection results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Anomaly score distribution\n",
    "axes[0,0].hist(metapaths_final['anomaly_score'], bins=50, alpha=0.7, edgecolor='black')\n",
    "anomaly_threshold_line = metapaths_final['anomaly_score'].quantile(1 - anomaly_threshold)\n",
    "axes[0,0].axvline(anomaly_threshold_line, color='red', linestyle='--', \n",
    "                  label=f'Threshold ({anomaly_threshold_line:.3f})')\n",
    "axes[0,0].set_xlabel('Anomaly Score')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('Anomaly Score Distribution')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Compositional vs Empirical with anomalies highlighted\n",
    "axes[0,1].scatter(normals['compositional_prob'], normals['empirical_prob'], \n",
    "                  alpha=0.6, s=20, label='Normal', color='blue')\n",
    "axes[0,1].scatter(anomalies['compositional_prob'], anomalies['empirical_prob'], \n",
    "                  alpha=0.8, s=30, label='Anomaly', color='red')\n",
    "axes[0,1].plot([0, metapaths_final['compositional_prob'].max()], \n",
    "               [0, metapaths_final['compositional_prob'].max()], \n",
    "               'k--', alpha=0.5)\n",
    "axes[0,1].set_xlabel('Compositional Probability')\n",
    "axes[0,1].set_ylabel('Empirical Probability')\n",
    "axes[0,1].set_title('Anomalies in Probability Space')\n",
    "axes[0,1].set_xscale('log')\n",
    "axes[0,1].set_yscale('log')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# 3. Edge probability patterns for anomalies vs normal\n",
    "edge_cols = ['edge1_prob', 'edge2_prob', 'edge3_prob']\n",
    "anomaly_means = [anomalies[col].mean() for col in edge_cols]\n",
    "normal_means = [normals[col].mean() for col in edge_cols]\n",
    "\n",
    "x_pos = np.arange(len(edge_cols))\n",
    "width = 0.35\n",
    "\n",
    "axes[0,2].bar(x_pos - width/2, normal_means, width, label='Normal', alpha=0.7)\n",
    "axes[0,2].bar(x_pos + width/2, anomaly_means, width, label='Anomaly', alpha=0.7)\n",
    "axes[0,2].set_xlabel('Edge Position')\n",
    "axes[0,2].set_ylabel('Mean Probability')\n",
    "axes[0,2].set_title('Edge Probability Patterns')\n",
    "axes[0,2].set_xticks(x_pos)\n",
    "axes[0,2].set_xticklabels(['Edge 1', 'Edge 2', 'Edge 3'])\n",
    "axes[0,2].legend()\n",
    "\n",
    "# 4. Component scores\n",
    "score_components = ['residual_score', 'compositional_zscore', 'edge_consistency_score']\n",
    "for i, component in enumerate(score_components):\n",
    "    axes[1,i].hist(normals[component], bins=30, alpha=0.7, label='Normal', density=True)\n",
    "    axes[1,i].hist(anomalies[component], bins=30, alpha=0.7, label='Anomaly', density=True)\n",
    "    axes[1,i].set_xlabel(component.replace('_', ' ').title())\n",
    "    axes[1,i].set_ylabel('Density')\n",
    "    axes[1,i].set_title(f'{component.replace(\"_\", \" \").title()} Distribution')\n",
    "    axes[1,i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / f'{metapath_pattern}_anomaly_detection.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "print(\"Saving analysis results...\")\n",
    "\n",
    "# Save metapaths with all probability calculations and anomaly scores\n",
    "metapaths_output_file = results_dir / f'{metapath_pattern}_metapath_analysis.csv'\n",
    "metapaths_final.to_csv(metapaths_output_file, index=False)\n",
    "print(f\"‚úì Saved metapath analysis: {metapaths_output_file}\")\n",
    "\n",
    "# Save anomalies separately for detailed inspection\n",
    "anomalies_file = results_dir / f'{metapath_pattern}_anomalies.csv'\n",
    "anomalies.to_csv(anomalies_file, index=False)\n",
    "print(f\"‚úì Saved anomalies: {anomalies_file}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'metapath_pattern': metapath_pattern,\n",
    "    'edge_types': edge_types,\n",
    "    'total_metapaths': len(metapaths_final),\n",
    "    'valid_metapaths': len(valid_paths),\n",
    "    'anomaly_threshold': anomaly_threshold,\n",
    "    'detected_anomalies': int(anomalies['is_anomaly'].sum()),\n",
    "    'anomaly_rate': float(anomalies['is_anomaly'].mean()),\n",
    "    \n",
    "    # Compositionality results\n",
    "    'compositionality': {\n",
    "        'edge_independence_pairs': independence_results,\n",
    "        'compositional_vs_empirical': compositional_comparison,\n",
    "        'probability_model_comparison': probability_model_comparison\n",
    "    },\n",
    "    \n",
    "    # Anomaly detection validation\n",
    "    'anomaly_validation': validation_results,\n",
    "    \n",
    "    # Summary statistics\n",
    "    'probability_statistics': {\n",
    "        'compositional_mean': float(metapaths_final['compositional_prob'].mean()),\n",
    "        'compositional_std': float(metapaths_final['compositional_prob'].std()),\n",
    "        'empirical_mean': float(metapaths_final['empirical_prob'].mean()),\n",
    "        'empirical_std': float(metapaths_final['empirical_prob'].std()),\n",
    "        'conditional_mean': float(metapaths_final['conditional_prob'].mean()),\n",
    "        'conditional_std': float(metapaths_final['conditional_prob'].std())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_file = results_dir / f'{metapath_pattern}_analysis_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2, default=str)\n",
    "print(f\"‚úì Saved summary: {summary_file}\")\n",
    "\n",
    "print(f\"\\nAnalysis complete! Results saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive summary\n",
    "print(\"=\" * 80)\n",
    "print(f\"METAPATH PROBABILITY ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nMetapath: {metapath_pattern}\")\n",
    "print(f\"Edge types: {' ‚Üí '.join(edge_types)}\")\n",
    "print(f\"Total metapaths analyzed: {len(metapaths_final):,}\")\n",
    "\n",
    "print(f\"\\nüîç COMPOSITIONALITY ANALYSIS:\")\n",
    "print(f\"  Edge independence: {sum(1 for r in independence_results.values() if r['independent'])}/{len(independence_results)} pairs independent\")\n",
    "print(f\"  Compositional validity: {compositional_comparison['compositional_valid']}\")\n",
    "print(f\"  Compositional vs Empirical correlation: {compositional_comparison['spearman_r']:.4f}\")\n",
    "print(f\"  Explained variance (R¬≤): {compositional_comparison['r2_score']:.4f}\")\n",
    "\n",
    "best_model = max(probability_model_comparison.items(), \n",
    "                key=lambda x: x[1]['correlation'] if 'empirical' in x[0] else 0)\n",
    "print(f\"  Best probability model: {best_model[0]} (r={best_model[1]['correlation']:.4f})\")\n",
    "\n",
    "print(f\"\\nüéØ ANOMALY DETECTION:\")\n",
    "print(f\"  Anomaly threshold: {anomaly_threshold:.1%}\")\n",
    "print(f\"  Detected anomalies: {len(anomalies):,} ({len(anomalies)/len(metapaths_final):.1%})\")\n",
    "print(f\"  Anomaly score range: {metapaths_final['anomaly_score'].min():.4f} - {metapaths_final['anomaly_score'].max():.4f}\")\n",
    "print(f\"  Statistical significance: p < 0.01\" if validation_results['compositional_test']['p_value'] < 0.01 else \"  Not statistically significant\")\n",
    "\n",
    "print(f\"\\nüìä PROBABILITY SUMMARY:\")\n",
    "for prob_type in ['compositional', 'conditional', 'empirical']:\n",
    "    col_name = f'{prob_type}_prob'\n",
    "    mean_val = metapaths_final[col_name].mean()\n",
    "    print(f\"  {prob_type.capitalize()} probability: {mean_val:.2e} (¬±{metapaths_final[col_name].std():.2e})\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES:\")\n",
    "output_files = list(results_dir.glob(f'{metapath_pattern}_*'))\n",
    "for file_path in sorted(output_files):\n",
    "    file_size = file_path.stat().st_size / (1024*1024)\n",
    "    print(f\"  - {file_path.name} ({file_size:.1f} MB)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This analysis provides insights into:\n",
    "\n",
    "1. **Compositionality**: Whether metapath probabilities can be calculated as independent edge products\n",
    "2. **Conditional Dependencies**: How edge probabilities depend on previous edges in the path\n",
    "3. **Anomaly Detection**: Identification of unusual metapaths for further investigation\n",
    "4. **Model Validation**: Comparison of different probability calculation approaches\n",
    "\n",
    "The results can be used for:\n",
    "- **Drug Discovery**: Identifying unusual compound-gene-pathway-gene relationships\n",
    "- **Network Analysis**: Understanding path probability patterns in biological networks\n",
    "- **Quality Control**: Detecting potential data quality issues or interesting biological phenomena\n",
    "- **Predictive Modeling**: Improving metapath-based prediction algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}