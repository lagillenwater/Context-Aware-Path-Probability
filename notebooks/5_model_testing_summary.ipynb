{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing Summary Across Edge Types\n",
    "\n",
    "This notebook aggregates and analyzes the results from **Notebook 4** (model testing) across all 24 edge types.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "- Summarize ML model performance (RF, LogReg, PolyLogReg, Simple NN) across all edge types\n",
    "- Identify best-performing models by edge type\n",
    "- Analyze relationship between graph characteristics and model performance\n",
    "- Provide recommendations for model selection based on graph properties\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load results from all edge types\n",
    "2. Aggregate performance metrics\n",
    "3. Analyze by graph characteristics (density, size, degree distributions)\n",
    "4. Create visualizations and summary tables\n",
    "5. Generate recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Papermill parameters (optional)\n",
    "edge_types = None  # None = use all 24 edge types, or provide list like ['CtD', 'AeG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import scipy.sparse as sp\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd().parent\n",
    "data_dir = repo_dir / 'data'\n",
    "results_dir = repo_dir / 'results' / 'model_comparison'\n",
    "summary_dir = repo_dir / 'results' / 'model_comparison_summary'\n",
    "summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repository directory: {repo_dir}\")\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Summary output directory: {summary_dir}\")\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discover and Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default edge types (all 24)\n",
    "DEFAULT_EDGE_TYPES = [\n",
    "    \"AdG\", \"AeG\", \"AuG\", \"CbG\", \"CcSE\", \"CdG\", \"CpD\", \"CrC\", \"CtD\", \"CuG\",\n",
    "    \"DaG\", \"DdG\", \"DlA\", \"DpS\", \"DrD\", \"DuG\", \"GcG\", \"GiG\", \"GpBP\", \"GpCC\",\n",
    "    \"GpMF\", \"GpPW\", \"Gr>G\", \"PCiC\"\n",
    "]\n",
    "\n",
    "# Use provided edge_types or default to all\n",
    "if edge_types is None:\n",
    "    edge_types = DEFAULT_EDGE_TYPES\n",
    "\n",
    "print(f\"Analyzing {len(edge_types)} edge types\")\n",
    "\n",
    "def load_edge_type_results(edge_type: str) -> Dict:\n",
    "    \"\"\"Load all result files for a given edge type.\"\"\"\n",
    "    edge_results_dir = results_dir / f\"{edge_type}_results\"\n",
    "    \n",
    "    if not edge_results_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    results = {'edge_type': edge_type}\n",
    "    \n",
    "    # Load model comparison metrics\n",
    "    comparison_file = edge_results_dir / 'model_comparison.csv'\n",
    "    if comparison_file.exists():\n",
    "        results['model_comparison'] = pd.read_csv(comparison_file)\n",
    "    \n",
    "    # Load analytical comparison\n",
    "    analytical_file = edge_results_dir / 'models_vs_analytical_comparison.csv'\n",
    "    if analytical_file.exists():\n",
    "        results['analytical_comparison'] = pd.read_csv(analytical_file)\n",
    "    \n",
    "    # Load empirical comparison\n",
    "    empirical_file = edge_results_dir / 'test_vs_empirical_comparison.csv'\n",
    "    if empirical_file.exists():\n",
    "        results['empirical_comparison'] = pd.read_csv(empirical_file)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load all results\n",
    "all_results = {}\n",
    "successful_loads = 0\n",
    "failed_loads = []\n",
    "\n",
    "for edge_type in edge_types:\n",
    "    result = load_edge_type_results(edge_type)\n",
    "    if result is not None:\n",
    "        all_results[edge_type] = result\n",
    "        successful_loads += 1\n",
    "    else:\n",
    "        failed_loads.append(edge_type)\n",
    "\n",
    "print(f\"\\nSuccessfully loaded results for {successful_loads} edge types\")\n",
    "if failed_loads:\n",
    "    print(f\"Failed to load results for {len(failed_loads)} edge types: {failed_loads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Graph Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_characteristics(edge_type: str) -> Dict:\n",
    "    \"\"\"Extract graph characteristics from edge matrix.\"\"\"\n",
    "    edge_file = data_dir / 'permutations' / '000.hetmat' / 'edges' / f\"{edge_type}.sparse.npz\"\n",
    "    \n",
    "    if not edge_file.exists():\n",
    "        return None\n",
    "    \n",
    "    # Load edge matrix\n",
    "    edge_matrix = sp.load_npz(edge_file)\n",
    "    n_sources, n_targets = edge_matrix.shape\n",
    "    n_edges = edge_matrix.nnz\n",
    "    \n",
    "    # Calculate characteristics\n",
    "    density = n_edges / (n_sources * n_targets)\n",
    "    source_degrees = np.array(edge_matrix.sum(axis=1)).flatten()\n",
    "    target_degrees = np.array(edge_matrix.sum(axis=0)).flatten()\n",
    "    \n",
    "    # Filter zero degrees\n",
    "    source_degrees_nz = source_degrees[source_degrees > 0]\n",
    "    target_degrees_nz = target_degrees[target_degrees > 0]\n",
    "    \n",
    "    return {\n",
    "        'edge_type': edge_type,\n",
    "        'n_sources': n_sources,\n",
    "        'n_targets': n_targets,\n",
    "        'n_edges': n_edges,\n",
    "        'density': density,\n",
    "        'mean_source_degree': source_degrees_nz.mean() if len(source_degrees_nz) > 0 else 0,\n",
    "        'mean_target_degree': target_degrees_nz.mean() if len(target_degrees_nz) > 0 else 0,\n",
    "        'max_source_degree': source_degrees.max(),\n",
    "        'max_target_degree': target_degrees.max(),\n",
    "        'n_sources_nz': len(source_degrees_nz),\n",
    "        'n_targets_nz': len(target_degrees_nz)\n",
    "    }\n",
    "\n",
    "# Load graph characteristics\n",
    "graph_chars = []\n",
    "for edge_type in all_results.keys():\n",
    "    chars = get_graph_characteristics(edge_type)\n",
    "    if chars is not None:\n",
    "        graph_chars.append(chars)\n",
    "\n",
    "graph_chars_df = pd.DataFrame(graph_chars)\n",
    "print(f\"\\nLoaded graph characteristics for {len(graph_chars_df)} edge types\")\n",
    "print(f\"\\nDensity range: {graph_chars_df['density'].min():.6f} - {graph_chars_df['density'].max():.6f}\")\n",
    "print(f\"Edge count range: {graph_chars_df['n_edges'].min()} - {graph_chars_df['n_edges'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregate Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate model comparison metrics across all edge types\n",
    "model_performance = []\n",
    "\n",
    "for edge_type, results in all_results.items():\n",
    "    if 'model_comparison' not in results:\n",
    "        continue\n",
    "    \n",
    "    df = results['model_comparison'].copy()\n",
    "    df['edge_type'] = edge_type\n",
    "    model_performance.append(df)\n",
    "\n",
    "if model_performance:\n",
    "    model_perf_df = pd.concat(model_performance, ignore_index=True)\n",
    "    print(f\"Aggregated performance metrics: {len(model_perf_df)} records\")\n",
    "    print(f\"Models: {model_perf_df['Model'].unique().tolist()}\")\n",
    "else:\n",
    "    print(\"No model performance data available\")\n",
    "    model_perf_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with graph characteristics\n",
    "if not model_perf_df.empty:\n",
    "    model_perf_df = model_perf_df.merge(graph_chars_df, on='edge_type', how='left')\n",
    "    \n",
    "    # Categorize by density\n",
    "    model_perf_df['density_category'] = pd.cut(\n",
    "        model_perf_df['density'],\n",
    "        bins=[0, 0.01, 0.03, 0.05, 1.0],\n",
    "        labels=['Very Sparse (<1%)', 'Sparse (1-3%)', 'Medium (3-5%)', 'Dense (>5%)']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDensity category distribution:\")\n",
    "    print(model_perf_df.groupby('density_category')['edge_type'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model for each edge type by different metrics\n",
    "best_models = []\n",
    "\n",
    "for edge_type in model_perf_df['edge_type'].unique():\n",
    "    edge_data = model_perf_df[model_perf_df['edge_type'] == edge_type]\n",
    "    \n",
    "    best_model_entry = {\n",
    "        'edge_type': edge_type,\n",
    "        'best_auc': edge_data.loc[edge_data['AUC'].idxmax(), 'Model'],\n",
    "        'best_correlation': edge_data.loc[edge_data['Correlation'].idxmax(), 'Model'],\n",
    "        'best_f1': edge_data.loc[edge_data['F1 Score'].idxmax(), 'Model'],\n",
    "        'best_rmse': edge_data.loc[edge_data['RMSE'].idxmin(), 'Model'],\n",
    "        'density': edge_data.iloc[0]['density'],\n",
    "        'n_edges': edge_data.iloc[0]['n_edges']\n",
    "    }\n",
    "    best_models.append(best_model_entry)\n",
    "\n",
    "best_models_df = pd.DataFrame(best_models)\n",
    "\n",
    "print(\"\\nBest models by metric:\")\n",
    "print(\"\\nAUC:\")\n",
    "print(best_models_df['best_auc'].value_counts())\n",
    "print(\"\\nCorrelation:\")\n",
    "print(best_models_df['best_correlation'].value_counts())\n",
    "print(\"\\nF1 Score:\")\n",
    "print(best_models_df['best_f1'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance by model\n",
    "if not model_perf_df.empty:\n",
    "    model_stats = model_perf_df.groupby('Model').agg({\n",
    "        'AUC': ['mean', 'std', 'min', 'max'],\n",
    "        'Accuracy': ['mean', 'std', 'min', 'max'],\n",
    "        'F1 Score': ['mean', 'std', 'min', 'max'],\n",
    "        'Correlation': ['mean', 'std', 'min', 'max'],\n",
    "        'RMSE': ['mean', 'std', 'min', 'max']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE STATISTICS ACROSS ALL EDGE TYPES\")\n",
    "    print(\"=\"*80)\n",
    "    print(model_stats)\n",
    "    \n",
    "    # Save to CSV\n",
    "    model_stats.to_csv(summary_dir / 'model_statistics_summary.csv')\n",
    "    print(f\"\\nSaved model statistics to {summary_dir / 'model_statistics_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Performance heatmap by edge type and model\n",
    "if not model_perf_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    metrics = ['AUC', 'Correlation', 'F1 Score', 'RMSE']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        # Pivot for heatmap\n",
    "        pivot = model_perf_df.pivot_table(\n",
    "            values=metric, \n",
    "            index='edge_type', \n",
    "            columns='Model',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Sort by mean performance\n",
    "        pivot = pivot.loc[pivot.mean(axis=1).sort_values(ascending=(metric=='RMSE')).index]\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(\n",
    "            pivot, \n",
    "            annot=True, \n",
    "            fmt='.3f', \n",
    "            cmap='RdYlGn' if metric != 'RMSE' else 'RdYlGn_r',\n",
    "            ax=ax,\n",
    "            cbar_kws={'label': metric}\n",
    "        )\n",
    "        ax.set_title(f'{metric} by Edge Type and Model', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Model', fontsize=12)\n",
    "        ax.set_ylabel('Edge Type', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(summary_dir / 'performance_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved performance heatmaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Best model distribution\n",
    "if not best_models_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    metrics = ['best_auc', 'best_correlation', 'best_f1', 'best_rmse']\n",
    "    titles = ['Best by AUC', 'Best by Correlation', 'Best by F1 Score', 'Best by RMSE']\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        counts = best_models_df[metric].value_counts()\n",
    "        counts.plot(kind='bar', ax=ax, color=sns.color_palette('Set2', len(counts)))\n",
    "        \n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Model', fontsize=12)\n",
    "        ax.set_ylabel('Number of Edge Types', fontsize=12)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for i, v in enumerate(counts.values):\n",
    "            ax.text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(summary_dir / 'best_model_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved best model distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Performance vs Graph Density\n",
    "if not model_perf_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    metrics = ['AUC', 'Correlation', 'F1 Score', 'RMSE']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        for model in model_perf_df['Model'].unique():\n",
    "            model_data = model_perf_df[model_perf_df['Model'] == model]\n",
    "            ax.scatter(\n",
    "                model_data['density'], \n",
    "                model_data[metric],\n",
    "                label=model,\n",
    "                alpha=0.6,\n",
    "                s=100\n",
    "            )\n",
    "        \n",
    "        ax.set_xlabel('Graph Density', fontsize=12)\n",
    "        ax.set_ylabel(metric, fontsize=12)\n",
    "        ax.set_title(f'{metric} vs Graph Density', fontsize=14, fontweight='bold')\n",
    "        ax.set_xscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(summary_dir / 'performance_vs_density.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved performance vs density plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Average performance by density category\n",
    "if not model_perf_df.empty and 'density_category' in model_perf_df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    metrics = ['AUC', 'Correlation', 'F1 Score', 'RMSE']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        # Prepare data for grouped bar chart\n",
    "        grouped = model_perf_df.groupby(['density_category', 'Model'])[metric].mean().unstack()\n",
    "        \n",
    "        grouped.plot(kind='bar', ax=ax, width=0.8)\n",
    "        \n",
    "        ax.set_title(f'Average {metric} by Density Category', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Density Category', fontsize=12)\n",
    "        ax.set_ylabel(f'Mean {metric}', fontsize=12)\n",
    "        ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(summary_dir / 'performance_by_density_category.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved performance by density category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation with Analytical and Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate analytical and empirical comparisons\n",
    "analytical_data = []\n",
    "empirical_data = []\n",
    "\n",
    "for edge_type, results in all_results.items():\n",
    "    if 'analytical_comparison' in results:\n",
    "        df = results['analytical_comparison'].copy()\n",
    "        df['edge_type'] = edge_type\n",
    "        analytical_data.append(df)\n",
    "    \n",
    "    if 'empirical_comparison' in results:\n",
    "        df = results['empirical_comparison'].copy()\n",
    "        df['edge_type'] = edge_type\n",
    "        empirical_data.append(df)\n",
    "\n",
    "if analytical_data:\n",
    "    analytical_df = pd.concat(analytical_data, ignore_index=True)\n",
    "    print(f\"\\nAnalytical comparison data: {len(analytical_df)} records\")\n",
    "else:\n",
    "    analytical_df = pd.DataFrame()\n",
    "\n",
    "if empirical_data:\n",
    "    empirical_df = pd.concat(empirical_data, ignore_index=True)\n",
    "    print(f\"Empirical comparison data: {len(empirical_df)} records\")\n",
    "else:\n",
    "    empirical_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation distributions\n",
    "if not empirical_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Analytical correlation\n",
    "    if not analytical_df.empty:\n",
    "        for model in analytical_df['Model'].unique():\n",
    "            model_data = analytical_df[analytical_df['Model'] == model]\n",
    "            axes[0].hist(\n",
    "                model_data['Correlation vs Analytical'],\n",
    "                alpha=0.6,\n",
    "                label=model,\n",
    "                bins=20\n",
    "            )\n",
    "        \n",
    "        axes[0].set_xlabel('Correlation with Analytical', fontsize=12)\n",
    "        axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "        axes[0].set_title('Model vs Analytical Correlation Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Empirical correlation\n",
    "    for model in empirical_df['Model'].unique():\n",
    "        model_data = empirical_df[empirical_df['Model'] == model]\n",
    "        axes[1].hist(\n",
    "            model_data['Correlation vs Empirical'],\n",
    "            alpha=0.6,\n",
    "            label=model,\n",
    "            bins=20\n",
    "        )\n",
    "    \n",
    "    axes[1].set_xlabel('Correlation with Empirical', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1].set_title('Model vs Empirical Correlation Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(summary_dir / 'correlation_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved correlation distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "if not model_perf_df.empty:\n",
    "    model_perf_df.to_csv(summary_dir / 'aggregate_model_performance.csv', index=False)\n",
    "    print(f\"Saved aggregate performance to {summary_dir / 'aggregate_model_performance.csv'}\")\n",
    "\n",
    "if not best_models_df.empty:\n",
    "    best_models_df.to_csv(summary_dir / 'best_models_by_edge_type.csv', index=False)\n",
    "    print(f\"Saved best models to {summary_dir / 'best_models_by_edge_type.csv'}\")\n",
    "\n",
    "if not analytical_df.empty:\n",
    "    analytical_df.to_csv(summary_dir / 'aggregate_analytical_comparison.csv', index=False)\n",
    "    print(f\"Saved analytical comparison to {summary_dir / 'aggregate_analytical_comparison.csv'}\")\n",
    "\n",
    "if not empirical_df.empty:\n",
    "    empirical_df.to_csv(summary_dir / 'aggregate_empirical_comparison.csv', index=False)\n",
    "    print(f\"Saved empirical comparison to {summary_dir / 'aggregate_empirical_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL SELECTION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not model_perf_df.empty:\n",
    "    # Overall champion\n",
    "    overall_best = model_perf_df.groupby('Model')['Correlation'].mean().idxmax()\n",
    "    print(f\"\\n1. OVERALL CHAMPION MODEL: {overall_best}\")\n",
    "    print(f\"   Average correlation across all edge types: {model_perf_df.groupby('Model')['Correlation'].mean()[overall_best]:.4f}\")\n",
    "    \n",
    "    # Best by density category\n",
    "    if 'density_category' in model_perf_df.columns:\n",
    "        print(\"\\n2. RECOMMENDATIONS BY GRAPH DENSITY:\")\n",
    "        for category in model_perf_df['density_category'].dropna().unique():\n",
    "            cat_data = model_perf_df[model_perf_df['density_category'] == category]\n",
    "            best_model = cat_data.groupby('Model')['Correlation'].mean().idxmax()\n",
    "            best_corr = cat_data.groupby('Model')['Correlation'].mean()[best_model]\n",
    "            print(f\"   {category}: {best_model} (avg correlation: {best_corr:.4f})\")\n",
    "    \n",
    "    # Best for small vs large graphs\n",
    "    print(\"\\n3. RECOMMENDATIONS BY GRAPH SIZE:\")\n",
    "    median_edges = model_perf_df['n_edges'].median()\n",
    "    \n",
    "    small_graphs = model_perf_df[model_perf_df['n_edges'] <= median_edges]\n",
    "    large_graphs = model_perf_df[model_perf_df['n_edges'] > median_edges]\n",
    "    \n",
    "    if not small_graphs.empty:\n",
    "        best_small = small_graphs.groupby('Model')['Correlation'].mean().idxmax()\n",
    "        print(f\"   Small graphs (≤{median_edges:.0f} edges): {best_small}\")\n",
    "    \n",
    "    if not large_graphs.empty:\n",
    "        best_large = large_graphs.groupby('Model')['Correlation'].mean().idxmax()\n",
    "        print(f\"   Large graphs (>{median_edges:.0f} edges): {best_large}\")\n",
    "    \n",
    "    # Most consistent model\n",
    "    print(\"\\n4. MOST CONSISTENT MODEL (lowest std in correlation):\")\n",
    "    consistency = model_perf_df.groupby('Model')['Correlation'].std().idxmin()\n",
    "    consistency_std = model_perf_df.groupby('Model')['Correlation'].std()[consistency]\n",
    "    print(f\"   {consistency} (std: {consistency_std:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll results saved to: {summary_dir}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for file in sorted(summary_dir.glob('*')):\n",
    "    print(f\"  - {file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
