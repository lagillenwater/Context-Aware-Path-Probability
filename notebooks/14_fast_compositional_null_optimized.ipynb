{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Compositional Null Calculator (Optimized Version)\n",
    "\n",
    "This notebook is an **optimized version** that addresses HPC performance issues:\n",
    "\n",
    "## Key Optimizations\n",
    "\n",
    "1. **Vectorized batch predictions** - Process many degree pairs simultaneously\n",
    "2. **Lookup table caching** - Pre-compute predictions for common degree pairs\n",
    "3. **Memory-efficient processing** - Process in chunks to avoid memory issues\n",
    "4. **Error handling** - Handle missing models and data gracefully\n",
    "5. **Checkpointing** - Save intermediate results for recovery\n",
    "\n",
    "## Performance Target\n",
    "\n",
    "- Original: ~10 pairs/second\n",
    "- Target: 1000+ pairs/second (100x speedup)\n",
    "- HPC time: Complete within 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Papermill parameters\n",
    "test_metapath = \"CbGpPW\"  # Test metapath for validation\n",
    "test_edge_types = [\"CbG\", \"GpPW\"]  # Edge types in test metapath\n",
    "validation_perm_range = (21, 31)  # Perms 21-30 for validation\n",
    "model_type = \"rf\"  # 'rf' or 'poly' or 'ensemble'\n",
    "chunk_size = 10000  # Process predictions in chunks\n",
    "use_cache = True  # Use lookup table caching\n",
    "save_checkpoints = True  # Save intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.sparse as sp\nfrom scipy.stats import pearsonr, ks_2samp\nimport joblib\nimport time\nfrom collections import Counter\nimport warnings\nimport pickle\nimport gc\nwarnings.filterwarnings('ignore')\n\n# Setup paths\nrepo_dir = Path.cwd()\ndata_dir = repo_dir / 'data'\nnull_models_dir = repo_dir / 'results' / 'null_models'\nresults_dir = repo_dir / 'results' / 'compositional_null'\nresults_dir.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Repository directory: {repo_dir}\")\nprint(f\"Null models directory: {null_models_dir}\")\nprint(f\"Results will be saved to: {results_dir}\")\nprint(f\"\\nOptimization settings:\")\nprint(f\"  Chunk size: {chunk_size:,}\")\nprint(f\"  Use cache: {use_cache}\")\nprint(f\"  Save checkpoints: {save_checkpoints}\")\n\n# Set plot style\nsns.set_style('whitegrid')\nplt.rcParams['figure.dpi'] = 100"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Null Models with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_null_model(edge_type, model_type='rf'):\n",
    "    \"\"\"\n",
    "    Load trained null model for edge type with error handling.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    try:\n",
    "        if model_type in ['rf', 'ensemble']:\n",
    "            rf_file = null_models_dir / f'{edge_type}_rf_null.pkl'\n",
    "            if rf_file.exists():\n",
    "                print(f\"    Loading {rf_file.name}...\")\n",
    "                models['rf'] = joblib.load(rf_file)\n",
    "            else:\n",
    "                print(f\"    âš ï¸ Warning: {rf_file.name} not found\")\n",
    "        \n",
    "        if model_type in ['poly', 'ensemble']:\n",
    "            poly_file = null_models_dir / f'{edge_type}_poly_null.pkl'\n",
    "            poly_features_file = null_models_dir / f'{edge_type}_poly_features.pkl'\n",
    "            if poly_file.exists() and poly_features_file.exists():\n",
    "                print(f\"    Loading {poly_file.name}...\")\n",
    "                models['poly'] = joblib.load(poly_file)\n",
    "                models['poly_features'] = joblib.load(poly_features_file)\n",
    "            else:\n",
    "                print(f\"    âš ï¸ Warning: Polynomial model files not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ Error loading models for {edge_type}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Test loading models for test metapath\n",
    "print(f\"Loading null models for metapath {test_metapath}...\")\n",
    "null_models = {}\n",
    "\n",
    "for edge_type in test_edge_types:\n",
    "    print(f\"\\n  {edge_type}:\")\n",
    "    models = load_null_model(edge_type, model_type)\n",
    "    if models:\n",
    "        null_models[edge_type] = models\n",
    "        print(f\"    âœ… Loaded: {list(models.keys())}\")\n",
    "    else:\n",
    "        print(f\"    âŒ Failed to load models\")\n",
    "        raise FileNotFoundError(f\"Required models for {edge_type} not found\")\n",
    "\n",
    "print(f\"\\nâœ“ Loaded models for {len(null_models)} edge types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Degree Distributions (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_distribution(node_type, edge_types_in_metapath, perm_id=0):\n",
    "    \"\"\"\n",
    "    Get degree distribution for intermediate nodes in metapath.\n",
    "    Optimized version with better memory management.\n",
    "    \"\"\"\n",
    "    degrees = []\n",
    "    \n",
    "    for edge_type in edge_types_in_metapath:\n",
    "        edge_file = data_dir / 'permutations' / f'{perm_id:03d}.hetmat' / 'edges' / f'{edge_type}.sparse.npz'\n",
    "        \n",
    "        if not edge_file.exists():\n",
    "            # Try alternative path structure\n",
    "            edge_file = data_dir / 'edges' / f'{edge_type}.sparse.npz'\n",
    "        \n",
    "        if edge_file.exists():\n",
    "            print(f\"    Loading {edge_file.name}...\")\n",
    "            matrix = sp.load_npz(str(edge_file))\n",
    "            \n",
    "            # Determine if node type is source or target\n",
    "            if 'G' in edge_type:  # Gene-related edges\n",
    "                if edge_type.endswith('G'):  # Gene is target\n",
    "                    node_degrees = np.array(matrix.sum(axis=0)).flatten()\n",
    "                else:  # Gene is source\n",
    "                    node_degrees = np.array(matrix.sum(axis=1)).flatten()\n",
    "                \n",
    "                # Only keep non-zero degrees\n",
    "                nonzero_degrees = node_degrees[node_degrees > 0]\n",
    "                degrees.extend(nonzero_degrees.tolist())\n",
    "            \n",
    "            # Clear matrix to free memory\n",
    "            del matrix\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"    âš ï¸ Warning: {edge_file} not found\")\n",
    "    \n",
    "    if not degrees:\n",
    "        print(f\"    âš ï¸ Warning: No degrees found, using default distribution\")\n",
    "        # Use a reasonable default distribution if no data found\n",
    "        degrees = [1] * 100 + [2] * 50 + [3] * 30 + [5] * 20 + [10] * 10\n",
    "    \n",
    "    # Count degree frequencies\n",
    "    degree_counts = Counter(degrees)\n",
    "    total = sum(degree_counts.values())\n",
    "    degree_freq = {deg: count/total for deg, count in degree_counts.items()}\n",
    "    \n",
    "    return degree_freq\n",
    "\n",
    "# Get gene degree distribution\n",
    "print(\"\\nComputing degree distributions for intermediate nodes...\")\n",
    "gene_degree_freq = get_degree_distribution('Gene', test_edge_types, perm_id=0)\n",
    "\n",
    "if gene_degree_freq:\n",
    "    print(f\"  Gene degrees: {len(gene_degree_freq)} unique degrees\")\n",
    "    if len(gene_degree_freq) > 0:\n",
    "        print(f\"  Degree range: {min(gene_degree_freq.keys())}-{max(gene_degree_freq.keys())}\")\n",
    "        print(f\"  Sample frequencies: {dict(list(gene_degree_freq.items())[:5])}\")\n",
    "else:\n",
    "    print(f\"  âš ï¸ No degree distribution found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimized Compositional Null Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCompositionalCalculator:\n",
    "    \"\"\"\n",
    "    Optimized compositional null calculator with caching and vectorization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, edge1_models, edge2_models, intermediate_degree_freq, \n",
    "                 model_type='rf', use_cache=True):\n",
    "        self.edge1_models = edge1_models\n",
    "        self.edge2_models = edge2_models\n",
    "        self.intermediate_degree_freq = intermediate_degree_freq\n",
    "        self.model_type = model_type\n",
    "        self.use_cache = use_cache\n",
    "        self.cache = {} if use_cache else None\n",
    "        \n",
    "        # Pre-compute intermediate degrees and frequencies as arrays for vectorization\n",
    "        self.inter_degrees = np.array(list(intermediate_degree_freq.keys()))\n",
    "        self.inter_freqs = np.array([intermediate_degree_freq[d] for d in self.inter_degrees])\n",
    "    \n",
    "    def _batch_predict(self, degrees_array, models, edge_num):\n",
    "        \"\"\"\n",
    "        Batch predict edge probabilities for many degree pairs at once.\n",
    "        \"\"\"\n",
    "        if self.model_type == 'rf' and 'rf' in models:\n",
    "            preds = models['rf'].predict(degrees_array)\n",
    "        elif self.model_type == 'poly' and 'poly' in models:\n",
    "            X_poly = models['poly_features'].transform(degrees_array)\n",
    "            preds = models['poly'].predict(X_poly)\n",
    "        else:\n",
    "            preds = np.zeros(len(degrees_array))\n",
    "        \n",
    "        return np.clip(preds, 0, 1)\n",
    "    \n",
    "    def compute_metapath_null_vectorized(self, source_degrees, target_degrees, \n",
    "                                        chunk_size=10000):\n",
    "        \"\"\"\n",
    "        Compute null probabilities using vectorized operations.\n",
    "        Process in chunks to manage memory.\n",
    "        \"\"\"\n",
    "        n_pairs = len(source_degrees)\n",
    "        null_probs = np.zeros(n_pairs)\n",
    "        \n",
    "        # Process in chunks\n",
    "        for chunk_start in range(0, n_pairs, chunk_size):\n",
    "            chunk_end = min(chunk_start + chunk_size, n_pairs)\n",
    "            chunk_sources = source_degrees[chunk_start:chunk_end]\n",
    "            chunk_targets = target_degrees[chunk_start:chunk_end]\n",
    "            chunk_size_actual = len(chunk_sources)\n",
    "            \n",
    "            # Initialize chunk results\n",
    "            chunk_probs = np.zeros(chunk_size_actual)\n",
    "            \n",
    "            # For each source-target pair in chunk\n",
    "            for i, (src_deg, tgt_deg) in enumerate(zip(chunk_sources, chunk_targets)):\n",
    "                # Check cache\n",
    "                if self.use_cache:\n",
    "                    cache_key = (src_deg, tgt_deg)\n",
    "                    if cache_key in self.cache:\n",
    "                        chunk_probs[i] = self.cache[cache_key]\n",
    "                        continue\n",
    "                \n",
    "                # Create degree pairs for all intermediate nodes\n",
    "                src_inter_pairs = np.column_stack([\n",
    "                    np.full(len(self.inter_degrees), src_deg),\n",
    "                    self.inter_degrees\n",
    "                ])\n",
    "                \n",
    "                inter_tgt_pairs = np.column_stack([\n",
    "                    self.inter_degrees,\n",
    "                    np.full(len(self.inter_degrees), tgt_deg)\n",
    "                ])\n",
    "                \n",
    "                # Batch predict all edge probabilities\n",
    "                p1_vec = self._batch_predict(src_inter_pairs, self.edge1_models, 1)\n",
    "                p2_vec = self._batch_predict(inter_tgt_pairs, self.edge2_models, 2)\n",
    "                \n",
    "                # Compositional calculation: sum over intermediate nodes\n",
    "                total_prob = np.sum(p1_vec * p2_vec * self.inter_freqs)\n",
    "                chunk_probs[i] = total_prob\n",
    "                \n",
    "                # Update cache\n",
    "                if self.use_cache:\n",
    "                    self.cache[cache_key] = total_prob\n",
    "            \n",
    "            null_probs[chunk_start:chunk_end] = chunk_probs\n",
    "            \n",
    "            # Progress update\n",
    "            if chunk_end % (chunk_size * 10) == 0 or chunk_end == n_pairs:\n",
    "                progress = chunk_end / n_pairs * 100\n",
    "                print(f\"    Processed {chunk_end:,} / {n_pairs:,} pairs ({progress:.1f}%)\")\n",
    "        \n",
    "        return null_probs\n",
    "\n",
    "# Create optimized calculator\n",
    "print(\"\\nCreating optimized compositional calculator...\")\n",
    "calculator = OptimizedCompositionalCalculator(\n",
    "    null_models[test_edge_types[0]],\n",
    "    null_models[test_edge_types[1]],\n",
    "    gene_degree_freq,\n",
    "    model_type=model_type,\n",
    "    use_cache=use_cache\n",
    ")\n",
    "print(\"âœ… Calculator ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract True Null with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metapath_frequencies(metapath_edges, perm_id):\n",
    "    \"\"\"\n",
    "    Extract observed metapath frequencies from a permutation.\n",
    "    \"\"\"\n",
    "    edge1_type, edge2_type = metapath_edges\n",
    "    \n",
    "    # Try multiple path structures\n",
    "    edge1_file = data_dir / 'permutations' / f'{perm_id:03d}.hetmat' / 'edges' / f'{edge1_type}.sparse.npz'\n",
    "    edge2_file = data_dir / 'permutations' / f'{perm_id:03d}.hetmat' / 'edges' / f'{edge2_type}.sparse.npz'\n",
    "    \n",
    "    # Alternative paths if not found\n",
    "    if not edge1_file.exists():\n",
    "        edge1_file = data_dir / 'edges' / f'{edge1_type}.sparse.npz'\n",
    "    if not edge2_file.exists():\n",
    "        edge2_file = data_dir / 'edges' / f'{edge2_type}.sparse.npz'\n",
    "    \n",
    "    if not edge1_file.exists() or not edge2_file.exists():\n",
    "        print(f\"    âš ï¸ Edge files not found for perm {perm_id}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        matrix1 = sp.load_npz(str(edge1_file))  # C Ã— G\n",
    "        matrix2 = sp.load_npz(str(edge2_file))  # G Ã— P\n",
    "        \n",
    "        # Compute metapath matrix\n",
    "        metapath_matrix = matrix1 @ matrix2\n",
    "        \n",
    "        # Get degrees\n",
    "        source_degrees = np.array(matrix1.sum(axis=1)).flatten()\n",
    "        target_degrees = np.array(matrix2.sum(axis=0)).flatten()\n",
    "        \n",
    "        # Extract metapath frequencies\n",
    "        data = []\n",
    "        for i, j in zip(*metapath_matrix.nonzero()):\n",
    "            data.append({\n",
    "                'source_idx': i,\n",
    "                'target_idx': j,\n",
    "                'source_degree': int(source_degrees[i]),\n",
    "                'target_degree': int(target_degrees[j]),\n",
    "                'metapath_count': int(metapath_matrix[i, j]),\n",
    "                'perm_id': perm_id\n",
    "            })\n",
    "        \n",
    "        # Clean up\n",
    "        del matrix1, matrix2, metapath_matrix\n",
    "        gc.collect()\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ Error processing perm {perm_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Check if checkpoint exists\n",
    "checkpoint_file = results_dir / f'{test_metapath}_true_null_checkpoint.pkl'\n",
    "\n",
    "if checkpoint_file.exists() and not save_checkpoints:\n",
    "    print(f\"Loading checkpoint from {checkpoint_file}...\")\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        true_null_agg = pickle.load(f)\n",
    "    print(f\"  âœ… Loaded {len(true_null_agg)} metapath pairs from checkpoint\")\n",
    "else:\n",
    "    # Extract true null from validation permutations\n",
    "    print(f\"\\nExtracting true null from permutations {validation_perm_range[0]}-{validation_perm_range[1]-1}...\")\n",
    "    all_true_null = []\n",
    "    \n",
    "    for perm_id in range(validation_perm_range[0], validation_perm_range[1]):\n",
    "        print(f\"  Processing permutation {perm_id}...\")\n",
    "        df = extract_metapath_frequencies(test_edge_types, perm_id)\n",
    "        if df is not None and len(df) > 0:\n",
    "            all_true_null.append(df)\n",
    "            print(f\"    Found {len(df):,} metapath pairs\")\n",
    "    \n",
    "    if all_true_null:\n",
    "        true_null_df = pd.concat(all_true_null, ignore_index=True)\n",
    "        \n",
    "        # Aggregate across permutations\n",
    "        true_null_agg = true_null_df.groupby(['source_idx', 'target_idx', 'source_degree', 'target_degree']).agg({\n",
    "            'metapath_count': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Normalize to probabilities\n",
    "        total_paths = true_null_agg['metapath_count'].sum()\n",
    "        if total_paths > 0:\n",
    "            true_null_agg['true_null_prob'] = true_null_agg['metapath_count'] / total_paths\n",
    "        else:\n",
    "            true_null_agg['true_null_prob'] = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if save_checkpoints:\n",
    "            with open(checkpoint_file, 'wb') as f:\n",
    "                pickle.dump(true_null_agg, f)\n",
    "            print(f\"  ðŸ’¾ Saved checkpoint to {checkpoint_file}\")\n",
    "    else:\n",
    "        print(\"  âŒ No validation data found!\")\n",
    "        true_null_agg = pd.DataFrame()\n",
    "\n",
    "if len(true_null_agg) > 0:\n",
    "    print(f\"\\nðŸ“Š Extracted {len(true_null_agg)} metapath pairs\")\n",
    "    print(f\"  Source degree range: {true_null_agg['source_degree'].min()}-{true_null_agg['source_degree'].max()}\")\n",
    "    print(f\"  Target degree range: {true_null_agg['target_degree'].min()}-{true_null_agg['target_degree'].max()}\")\n",
    "    print(f\"  Mean null probability: {true_null_agg['true_null_prob'].mean():.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute ML-Compositional Null (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(true_null_agg) > 0:\n",
    "    print(f\"\\nComputing ML-compositional null using {model_type} model...\")\n",
    "    print(f\"  Processing {len(true_null_agg):,} pairs in chunks of {chunk_size:,}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Compute null for each (source, target) pair\n",
    "    ml_null_probs = calculator.compute_metapath_null_vectorized(\n",
    "        true_null_agg['source_degree'].values,\n",
    "        true_null_agg['target_degree'].values,\n",
    "        chunk_size=chunk_size\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    true_null_agg['ml_null_prob'] = ml_null_probs\n",
    "    \n",
    "    print(f\"\\nâœ… Computation complete!\")\n",
    "    print(f\"  Computed {len(ml_null_probs):,} null probabilities\")\n",
    "    print(f\"  Computation time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"  Speed: {len(ml_null_probs)/elapsed_time:.0f} pairs/second\")\n",
    "    print(f\"  Mean ML-null probability: {ml_null_probs.mean():.6e}\")\n",
    "    \n",
    "    if use_cache:\n",
    "        print(f\"  Cache size: {len(calculator.cache):,} unique degree pairs\")\n",
    "        cache_hit_rate = len(calculator.cache) / len(ml_null_probs) * 100\n",
    "        print(f\"  Cache coverage: {cache_hit_rate:.1f}%\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data to compute null probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate Against True Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(true_null_agg) > 0 and 'ml_null_prob' in true_null_agg.columns:\n",
    "    # Remove any zero or invalid values\n",
    "    valid_mask = (true_null_agg['true_null_prob'] > 0) & (true_null_agg['ml_null_prob'] > 0)\n",
    "    valid_data = true_null_agg[valid_mask].copy()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Validation Results:\")\n",
    "    print(f\"  Valid pairs: {len(valid_data)} / {len(true_null_agg)}\")\n",
    "    \n",
    "    if len(valid_data) > 1:\n",
    "        # Correlation\n",
    "        corr, p_val = pearsonr(valid_data['true_null_prob'], valid_data['ml_null_prob'])\n",
    "        print(f\"  Pearson correlation: r = {corr:.4f} (p = {p_val:.2e})\")\n",
    "        \n",
    "        # MAE and RMSE\n",
    "        mae = np.abs(valid_data['true_null_prob'] - valid_data['ml_null_prob']).mean()\n",
    "        rmse = np.sqrt(((valid_data['true_null_prob'] - valid_data['ml_null_prob'])**2).mean())\n",
    "        print(f\"  MAE: {mae:.6e}\")\n",
    "        print(f\"  RMSE: {rmse:.6e}\")\n",
    "        \n",
    "        # RÂ²\n",
    "        from sklearn.metrics import r2_score\n",
    "        r2 = r2_score(valid_data['true_null_prob'], valid_data['ml_null_prob'])\n",
    "        print(f\"  RÂ²: {r2:.4f}\")\n",
    "        \n",
    "        # Success criteria\n",
    "        print(f\"\\nâœ… Success Criteria:\")\n",
    "        print(f\"  Correlation > 0.75: {'âœ“' if corr > 0.75 else 'âœ—'} ({corr:.4f})\")\n",
    "        print(f\"  RMSE < 0.20: {'âœ“' if rmse < 0.20 else 'âœ—'} ({rmse:.4f})\")\n",
    "        print(f\"  RÂ² > 0.50: {'âœ“' if r2 > 0.50 else 'âœ—'} ({r2:.4f})\")\n",
    "        \n",
    "        # Performance improvement\n",
    "        print(f\"\\nðŸš€ Performance Improvement:\")\n",
    "        speedup = (len(ml_null_probs)/elapsed_time) / 10  # Original was ~10 pairs/sec\n",
    "        print(f\"  Speedup: {speedup:.1f}x faster than original\")\n",
    "        print(f\"  Time to process 1M pairs: {1000000/(len(ml_null_probs)/elapsed_time)/60:.1f} minutes\")\n",
    "else:\n",
    "    print(\"âš ï¸ Insufficient data for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(true_null_agg) > 0 and 'ml_null_prob' in true_null_agg.columns:\n",
    "    # Save validation data\n",
    "    output_file = results_dir / f'{test_metapath}_null_validation_optimized.csv'\n",
    "    true_null_agg.to_csv(output_file, index=False)\n",
    "    print(f\"\\nðŸ’¾ Saved validation data to {output_file}\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'metapath': test_metapath,\n",
    "        'edge_types': test_edge_types,\n",
    "        'model_type': model_type,\n",
    "        'n_pairs': len(valid_data) if 'valid_data' in locals() else 0,\n",
    "        'correlation': corr if 'corr' in locals() else None,\n",
    "        'mae': mae if 'mae' in locals() else None,\n",
    "        'rmse': rmse if 'rmse' in locals() else None,\n",
    "        'r2': r2 if 'r2' in locals() else None,\n",
    "        'computation_time_sec': elapsed_time,\n",
    "        'pairs_per_second': len(ml_null_probs)/elapsed_time,\n",
    "        'optimization_settings': {\n",
    "            'chunk_size': chunk_size,\n",
    "            'use_cache': use_cache,\n",
    "            'cache_size': len(calculator.cache) if use_cache else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_file = results_dir / f'{test_metapath}_summary_optimized.json'\n",
    "    import json\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"ðŸ’¾ Saved summary to {summary_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZED COMPOSITIONAL NULL COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}