{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming DWPC Null Model\n",
    "\n",
    "This notebook implements the dynamic programming approach for computing expected Degree-Weighted Path Counts (DWPC) under the XSwap null model (degree-preserving edge randomization).\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Provide an analytical method to compute expected DWPC that:\n",
    "- Uses exact degree-sum formula under independence assumptions\n",
    "- Implements matrix-based dynamic programming for efficient computation\n",
    "- Includes closed-form mean-field approximation for ultra-fast estimates\n",
    "- Validates against empirical null values from permutations\n",
    "- Compares with compositional null approach from notebook 14\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Three calculation methods**: Exact, DP matrix, mean-field approximation\n",
    "- **Degree-grouping technique**: Pool results by degree combinations\n",
    "- **Performance benchmarking**: Compare accuracy and speed vs compositional null\n",
    "- **Scalability**: Handle metapaths of arbitrary length\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "For a metapath of length L:\n",
    "\n",
    "$$\\mathbb{E}[\\text{DWPC}_{s \\to t}] = \\frac{\\deg_{\\text{first}}(s)^{1-w} \\cdot \\deg_{\\text{last}}(t)^{1-w}}{\\prod_{i=1}^{L} E_i} \\prod_{k=2}^{L} S_k$$\n",
    "\n",
    "where $S_k = \\sum_{n \\in T_k} \\deg_{T_{k-1}\\text{-}T_k}(n)^{1-w} \\cdot \\deg_{T_k\\text{-}T_{k+1}}(n)^{1-w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Papermill parameters\n",
    "test_metapaths = [\n",
    "    'CbGpPW',     # Compound binds Gene participates in Pathway\n",
    "    'CtDaG',      # Compound treats Disease associates with Gene\n",
    "    'CbGaD',      # Compound binds Gene associates with Disease\n",
    "    'CrCbG',      # Compound resembles Compound binds Gene\n",
    "    'GuGiG'       # Gene upregulates Gene interacts with Gene\n",
    "]\n",
    "validation_perm_range = (21, 31)  # Perms 21-30 for validation\n",
    "damping_exponent = 0.4  # DWPC damping factor w"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import pearsonr, ks_2samp\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd()\n",
    "data_dir = repo_dir / 'data'\n",
    "results_dir = repo_dir / 'results' / 'dynamic_programming_dwpc'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Repository directory: {repo_dir}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Results will be saved to: {results_dir}\")\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Metapath Structure and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def parse_metapath(metapath_str):\n",
    "    \"\"\"\n",
    "    Parse metapath string into edges and node types.\n",
    "    Example: 'CbGpPW' -> edges: ['CbG', 'GpPW'], nodes: ['Compound', 'Gene', 'Pathway']\n",
    "    \"\"\"\n",
    "    # Map of edge abbreviations to (source_type, edge_name, target_type)\n",
    "    edge_map = {\n",
    "        'CbG': ('Compound', 'binds', 'Gene'),\n",
    "        'GpPW': ('Gene', 'participates', 'Pathway'),\n",
    "        'CtD': ('Compound', 'treats', 'Disease'),\n",
    "        'DaG': ('Disease', 'associates', 'Gene'),\n",
    "        'CbGaD': ('Compound', 'binds-associates', 'Disease'),  # Composite\n",
    "        'GaD': ('Gene', 'associates', 'Disease'),\n",
    "        'CrC': ('Compound', 'resembles', 'Compound'),\n",
    "        'GuG': ('Gene', 'upregulates', 'Gene'),\n",
    "        'GiG': ('Gene', 'interacts', 'Gene')\n",
    "    }\n",
    "    \n",
    "    # Simple parser for common metapaths\n",
    "    if metapath_str == 'CbGpPW':\n",
    "        return ['CbG', 'GpPW'], ['Compound', 'Gene', 'Pathway']\n",
    "    elif metapath_str == 'CtDaG':\n",
    "        return ['CtD', 'DaG'], ['Compound', 'Disease', 'Gene']\n",
    "    elif metapath_str == 'CbGaD':\n",
    "        return ['CbG', 'GaD'], ['Compound', 'Gene', 'Disease']\n",
    "    elif metapath_str == 'CrCbG':\n",
    "        return ['CrC', 'CbG'], ['Compound', 'Compound', 'Gene']\n",
    "    elif metapath_str == 'GuGiG':\n",
    "        return ['GuG', 'GiG'], ['Gene', 'Gene', 'Gene']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metapath: {metapath_str}\")\n",
    "\n",
    "def load_edge_matrix(edge_type, perm_id=0):\n",
    "    \"\"\"\n",
    "    Load edge matrix for given edge type and permutation.\n",
    "    \"\"\"\n",
    "    edge_file = data_dir / 'permutations' / f'{perm_id:03d}.hetmat' / 'edges' / f'{edge_type}.sparse.npz'\n",
    "    \n",
    "    if not edge_file.exists():\n",
    "        raise FileNotFoundError(f\"Edge file not found: {edge_file}\")\n",
    "    \n",
    "    return sp.load_npz(str(edge_file))\n",
    "\n",
    "def get_degree_sequences(edge_type, perm_id=0):\n",
    "    \"\"\"\n",
    "    Get source and target degree sequences for an edge type.\n",
    "    \"\"\"\n",
    "    matrix = load_edge_matrix(edge_type, perm_id)\n",
    "    source_degrees = np.array(matrix.sum(axis=1)).flatten()\n",
    "    target_degrees = np.array(matrix.sum(axis=0)).flatten()\n",
    "    return source_degrees, target_degrees, matrix.nnz\n",
    "\n",
    "print(\"Metapath parsing and data loading functions ready\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dynamic Programming DWPC Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DynamicProgrammingDWPC:\n",
    "    \"\"\"\n",
    "    Dynamic Programming approach for computing expected DWPC under degree-preserving null.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, damping_exponent=0.4):\n",
    "        self.w = damping_exponent\n",
    "        self.degree_cache = {}\n",
    "        \n",
    "    def load_metapath_data(self, metapath_str, perm_id=0):\n",
    "        \"\"\"\n",
    "        Load all necessary degree data for a metapath.\n",
    "        \"\"\"\n",
    "        edges, nodes = parse_metapath(metapath_str)\n",
    "        \n",
    "        edge_data = []\n",
    "        for edge_type in edges:\n",
    "            source_deg, target_deg, n_edges = get_degree_sequences(edge_type, perm_id)\n",
    "            edge_data.append({\n",
    "                'type': edge_type,\n",
    "                'source_degrees': source_deg,\n",
    "                'target_degrees': target_deg,\n",
    "                'total_edges': n_edges\n",
    "            })\n",
    "        \n",
    "        return edges, nodes, edge_data\n",
    "    \n",
    "    def compute_exact_expectation(self, source_idx, target_idx, edge_data):\n",
    "        \"\"\"\n",
    "        Compute exact expected DWPC using degree-sum formula.\n",
    "        \n",
    "        For length-2 metapath:\n",
    "        E[DWPC_{s→t}] = deg_s^{1-w} * deg_t^{1-w} / (E1 * E2) * S\n",
    "        where S = Σ_m deg1_m^{1-w} * deg2_m^{1-w}\n",
    "        \"\"\"\n",
    "        if len(edge_data) == 2:\n",
    "            # Length-2 metapath\n",
    "            edge1, edge2 = edge_data\n",
    "            \n",
    "            # Source and target degrees\n",
    "            deg_s = edge1['source_degrees'][source_idx]\n",
    "            deg_t = edge2['target_degrees'][target_idx]\n",
    "            \n",
    "            if deg_s == 0 or deg_t == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            # Compute intermediate sum S\n",
    "            # Intermediate nodes are targets of edge1 and sources of edge2\n",
    "            deg1_intermediate = edge1['target_degrees']\n",
    "            deg2_intermediate = edge2['source_degrees']\n",
    "            \n",
    "            # Both should have same length (same intermediate node type)\n",
    "            assert len(deg1_intermediate) == len(deg2_intermediate)\n",
    "            \n",
    "            S = np.sum(\n",
    "                deg1_intermediate**(1-self.w) * deg2_intermediate**(1-self.w)\n",
    "            )\n",
    "            \n",
    "            # Total edges\n",
    "            E1 = edge1['total_edges']\n",
    "            E2 = edge2['total_edges']\n",
    "            \n",
    "            # Expected DWPC\n",
    "            expectation = (deg_s**(1-self.w) * deg_t**(1-self.w) * S) / (E1 * E2)\n",
    "            \n",
    "            return expectation\n",
    "        else:\n",
    "            # For longer metapaths, implement recursive formula\n",
    "            raise NotImplementedError(\"Longer metapaths not yet implemented\")\n",
    "    \n",
    "    def compute_mean_field_approximation(self, source_idx, target_idx, edge_data, n_intermediate_nodes):\n",
    "        \"\"\"\n",
    "        Compute closed-form mean-field approximation.\n",
    "        \n",
    "        For length-2 metapath:\n",
    "        E[DWPC_{s→t}] ≈ deg_s^{1-w} * deg_t^{1-w} * |T_2|^{2w-1} / (E1^w * E2^w)\n",
    "        \"\"\"\n",
    "        if len(edge_data) == 2:\n",
    "            edge1, edge2 = edge_data\n",
    "            \n",
    "            deg_s = edge1['source_degrees'][source_idx]\n",
    "            deg_t = edge2['target_degrees'][target_idx]\n",
    "            \n",
    "            if deg_s == 0 or deg_t == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            E1 = edge1['total_edges']\n",
    "            E2 = edge2['total_edges']\n",
    "            \n",
    "            # Mean-field approximation\n",
    "            expectation = (\n",
    "                deg_s**(1-self.w) * deg_t**(1-self.w) * \n",
    "                n_intermediate_nodes**(2*self.w - 1)\n",
    "            ) / (E1**self.w * E2**self.w)\n",
    "            \n",
    "            return expectation\n",
    "        else:\n",
    "            raise NotImplementedError(\"Longer metapaths not yet implemented\")\n",
    "    \n",
    "    def compute_matrix_dp(self, source_idx, target_idx, edge_data):\n",
    "        \"\"\"\n",
    "        Matrix-based dynamic programming approach.\n",
    "        \n",
    "        Uses expected adjacency matrices P where P_uv ≈ deg_u * deg_v / E\n",
    "        and weight matrices W for degree downweighting.\n",
    "        \"\"\"\n",
    "        if len(edge_data) == 2:\n",
    "            edge1, edge2 = edge_data\n",
    "            \n",
    "            # Create expected adjacency matrices\n",
    "            n_source = len(edge1['source_degrees'])\n",
    "            n_intermediate = len(edge1['target_degrees'])\n",
    "            n_target = len(edge2['target_degrees'])\n",
    "            \n",
    "            # Expected adjacency matrix for edge1\n",
    "            P1 = np.outer(edge1['source_degrees'], edge1['target_degrees']) / edge1['total_edges']\n",
    "            \n",
    "            # Expected adjacency matrix for edge2  \n",
    "            P2 = np.outer(edge2['source_degrees'], edge2['target_degrees']) / edge2['total_edges']\n",
    "            \n",
    "            # Weight matrices\n",
    "            W_source = edge1['source_degrees']**(-self.w)\n",
    "            W_intermediate_1 = edge1['target_degrees']**(-self.w)\n",
    "            W_intermediate_2 = edge2['source_degrees']**(-self.w)\n",
    "            W_target = edge2['target_degrees']**(-self.w)\n",
    "            \n",
    "            # Initialize with source node\n",
    "            state = np.zeros(n_source)\n",
    "            state[source_idx] = W_source[source_idx]\n",
    "            \n",
    "            # Propagate through edge1\n",
    "            state = state @ (P1 * W_intermediate_1[None, :])\n",
    "            \n",
    "            # Apply intermediate weight for edge2\n",
    "            state = state * W_intermediate_2\n",
    "            \n",
    "            # Propagate through edge2\n",
    "            state = state @ P2\n",
    "            \n",
    "            # Apply target weight and extract result\n",
    "            result = state[target_idx] * W_target[target_idx]\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            raise NotImplementedError(\"Longer metapaths not yet implemented\")\n",
    "\n",
    "print(\"Dynamic Programming DWPC calculator ready\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Empirical DWPC from Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_actual_dwpc(source_idx, target_idx, edge_matrices, w=0.4):\n",
    "    \"\"\"\n",
    "    Compute actual DWPC for a source-target pair through a metapath.\n",
    "    \"\"\"\n",
    "    if len(edge_matrices) == 2:\n",
    "        matrix1, matrix2 = edge_matrices\n",
    "        \n",
    "        # Get degree weighting factors\n",
    "        source_degree = matrix1.sum(axis=1).A1[source_idx]\n",
    "        target_degree = matrix2.sum(axis=0).A1[target_idx]\n",
    "        \n",
    "        if source_degree == 0 or target_degree == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Compute metapath matrix\n",
    "        metapath_matrix = matrix1 @ matrix2\n",
    "        \n",
    "        # Get path count\n",
    "        path_count = metapath_matrix[source_idx, target_idx]\n",
    "        \n",
    "        if path_count == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Apply degree weighting\n",
    "        # For each path, weight = product of (degree^(-w)) for all nodes\n",
    "        # Simplified for 2-edge path: weight ≈ path_count * (deg_s * deg_t)^(-w)\n",
    "        dwpc = path_count * (source_degree**(-w)) * (target_degree**(-w))\n",
    "        \n",
    "        return dwpc\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only 2-edge metapaths currently supported\")\n",
    "\n",
    "def extract_empirical_dwpc_values(metapath_str, perm_range, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Extract empirical DWPC values from permutations.\n",
    "    \"\"\"\n",
    "    edges, nodes = parse_metapath(metapath_str)\n",
    "    \n",
    "    empirical_data = []\n",
    "    \n",
    "    for perm_id in range(perm_range[0], perm_range[1]):\n",
    "        try:\n",
    "            # Load edge matrices\n",
    "            matrices = [load_edge_matrix(edge_type, perm_id) for edge_type in edges]\n",
    "            \n",
    "            # Get dimensions\n",
    "            n_sources = matrices[0].shape[0]\n",
    "            n_targets = matrices[-1].shape[1]\n",
    "            \n",
    "            # Sample random source-target pairs\n",
    "            for _ in range(sample_size):\n",
    "                source_idx = np.random.randint(0, n_sources)\n",
    "                target_idx = np.random.randint(0, n_targets)\n",
    "                \n",
    "                # Get degrees\n",
    "                source_deg = matrices[0].sum(axis=1).A1[source_idx]\n",
    "                target_deg = matrices[-1].sum(axis=0).A1[target_idx]\n",
    "                \n",
    "                # Compute actual DWPC\n",
    "                dwpc = compute_actual_dwpc(source_idx, target_idx, matrices)\n",
    "                \n",
    "                empirical_data.append({\n",
    "                    'perm_id': perm_id,\n",
    "                    'source_idx': source_idx,\n",
    "                    'target_idx': target_idx,\n",
    "                    'source_degree': source_deg,\n",
    "                    'target_degree': target_deg,\n",
    "                    'empirical_dwpc': dwpc\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing permutation {perm_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(empirical_data)\n",
    "\n",
    "print(\"Empirical DWPC extraction functions ready\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Test Metapaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize calculator\n",
    "calculator = DynamicProgrammingDWPC(damping_exponent=damping_exponent)\n",
    "\n",
    "# Results storage\n",
    "all_results = {}\n",
    "\n",
    "for metapath_str in test_metapaths:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing metapath: {metapath_str}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Load metapath data from Hetionet (perm 0)\n",
    "        edges, nodes, edge_data = calculator.load_metapath_data(metapath_str, perm_id=0)\n",
    "        print(f\"Edges: {edges}\")\n",
    "        print(f\"Node types: {nodes}\")\n",
    "        \n",
    "        # Extract empirical DWPC values from validation permutations\n",
    "        print(f\"\\nExtracting empirical DWPC from permutations {validation_perm_range[0]}-{validation_perm_range[1]-1}...\")\n",
    "        empirical_df = extract_empirical_dwpc_values(metapath_str, validation_perm_range, sample_size=500)\n",
    "        \n",
    "        print(f\"  Extracted {len(empirical_df)} DWPC values\")\n",
    "        print(f\"  Mean empirical DWPC: {empirical_df['empirical_dwpc'].mean():.6f}\")\n",
    "        \n",
    "        # Compute expected DWPC using three methods\n",
    "        results = []\n",
    "        \n",
    "        for _, row in empirical_df.iterrows():\n",
    "            source_idx = row['source_idx']\n",
    "            target_idx = row['target_idx']\n",
    "            \n",
    "            # Method 1: Exact expectation\n",
    "            exact_exp = calculator.compute_exact_expectation(source_idx, target_idx, edge_data)\n",
    "            \n",
    "            # Method 2: Mean-field approximation\n",
    "            n_intermediate = len(edge_data[0]['target_degrees'])\n",
    "            meanfield_exp = calculator.compute_mean_field_approximation(\n",
    "                source_idx, target_idx, edge_data, n_intermediate\n",
    "            )\n",
    "            \n",
    "            # Method 3: Matrix DP\n",
    "            matrix_dp_exp = calculator.compute_matrix_dp(source_idx, target_idx, edge_data)\n",
    "            \n",
    "            results.append({\n",
    "                'source_idx': source_idx,\n",
    "                'target_idx': target_idx,\n",
    "                'source_degree': row['source_degree'],\n",
    "                'target_degree': row['target_degree'],\n",
    "                'empirical_dwpc': row['empirical_dwpc'],\n",
    "                'exact_expectation': exact_exp,\n",
    "                'meanfield_expectation': meanfield_exp,\n",
    "                'matrix_dp_expectation': matrix_dp_exp\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        all_results[metapath_str] = results_df\n",
    "        \n",
    "        # Save results\n",
    "        output_file = results_dir / f'{metapath_str}_dp_results.csv'\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nSaved results to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {metapath_str}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Processing complete!\")\n",
    "print(f\"{'='*70}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze results for each metapath\n",
    "validation_summary = []\n",
    "\n",
    "for metapath_str, results_df in all_results.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Validation for {metapath_str}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Remove invalid values\n",
    "    valid_mask = (\n",
    "        (results_df['empirical_dwpc'] > 0) & \n",
    "        (results_df['exact_expectation'] > 0) &\n",
    "        np.isfinite(results_df['empirical_dwpc']) &\n",
    "        np.isfinite(results_df['exact_expectation'])\n",
    "    )\n",
    "    valid_data = results_df[valid_mask].copy()\n",
    "    \n",
    "    if len(valid_data) < 10:\n",
    "        print(f\"Insufficient valid data points: {len(valid_data)}\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate correlations for each method\n",
    "    methods = ['exact_expectation', 'meanfield_expectation', 'matrix_dp_expectation']\n",
    "    \n",
    "    for method in methods:\n",
    "        # Filter valid values for this method\n",
    "        method_valid = valid_data[valid_data[method] > 0].copy()\n",
    "        \n",
    "        if len(method_valid) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Correlation\n",
    "        corr, p_val = pearsonr(method_valid['empirical_dwpc'], method_valid[method])\n",
    "        \n",
    "        # MAE and RMSE\n",
    "        mae = np.abs(method_valid['empirical_dwpc'] - method_valid[method]).mean()\n",
    "        rmse = np.sqrt(((method_valid['empirical_dwpc'] - method_valid[method])**2).mean())\n",
    "        \n",
    "        # R²\n",
    "        from sklearn.metrics import r2_score\n",
    "        r2 = r2_score(method_valid['empirical_dwpc'], method_valid[method])\n",
    "        \n",
    "        method_name = method.replace('_expectation', '').replace('_', ' ').title()\n",
    "        print(f\"\\n{method_name}:\")\n",
    "        print(f\"  Correlation: r = {corr:.4f} (p = {p_val:.2e})\")\n",
    "        print(f\"  MAE: {mae:.6f}\")\n",
    "        print(f\"  RMSE: {rmse:.6f}\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        print(f\"  Valid samples: {len(method_valid)}\")\n",
    "        \n",
    "        validation_summary.append({\n",
    "            'metapath': metapath_str,\n",
    "            'method': method_name,\n",
    "            'correlation': corr,\n",
    "            'p_value': p_val,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'n_samples': len(method_valid)\n",
    "        })\n",
    "\n",
    "# Save validation summary\n",
    "if validation_summary:\n",
    "    summary_df = pd.DataFrame(validation_summary)\n",
    "    summary_df.to_csv(results_dir / 'validation_summary.csv', index=False)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Overall Validation Summary\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(summary_df.to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create visualizations for the best performing metapath\n",
    "if all_results:\n",
    "    # Select first metapath with sufficient data\n",
    "    for metapath_str, results_df in all_results.items():\n",
    "        if len(results_df) > 100:\n",
    "            break\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Filter valid data\n",
    "    valid_mask = (\n",
    "        (results_df['empirical_dwpc'] > 0) & \n",
    "        (results_df['exact_expectation'] > 0)\n",
    "    )\n",
    "    plot_data = results_df[valid_mask].copy()\n",
    "    \n",
    "    # 1. Empirical vs Exact Expectation\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(plot_data['empirical_dwpc'], plot_data['exact_expectation'], \n",
    "               alpha=0.5, s=20)\n",
    "    lims = [0, max(plot_data['empirical_dwpc'].max(), plot_data['exact_expectation'].max())]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.8, label='Perfect agreement')\n",
    "    ax.set_xlabel('Empirical DWPC', fontsize=12)\n",
    "    ax.set_ylabel('Expected DWPC (Exact)', fontsize=12)\n",
    "    ax.set_title(f'{metapath_str}: Empirical vs Exact Expectation', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Empirical vs Mean-Field Approximation\n",
    "    ax = axes[0, 1]\n",
    "    valid_mf = plot_data[plot_data['meanfield_expectation'] > 0]\n",
    "    ax.scatter(valid_mf['empirical_dwpc'], valid_mf['meanfield_expectation'], \n",
    "               alpha=0.5, s=20, color='orange')\n",
    "    lims = [0, max(valid_mf['empirical_dwpc'].max(), valid_mf['meanfield_expectation'].max())]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.8, label='Perfect agreement')\n",
    "    ax.set_xlabel('Empirical DWPC', fontsize=12)\n",
    "    ax.set_ylabel('Expected DWPC (Mean-Field)', fontsize=12)\n",
    "    ax.set_title(f'{metapath_str}: Empirical vs Mean-Field', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Error vs Source Degree\n",
    "    ax = axes[1, 0]\n",
    "    plot_data['error'] = plot_data['empirical_dwpc'] - plot_data['exact_expectation']\n",
    "    ax.scatter(plot_data['source_degree'], plot_data['error'], \n",
    "               alpha=0.5, s=20, color='green')\n",
    "    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Source Degree', fontsize=12)\n",
    "    ax.set_ylabel('Error (Empirical - Expected)', fontsize=12)\n",
    "    ax.set_title('Prediction Error vs Source Degree', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Method Comparison Boxplot\n",
    "    ax = axes[1, 1]\n",
    "    methods_data = []\n",
    "    methods_labels = []\n",
    "    \n",
    "    for method in ['exact_expectation', 'meanfield_expectation', 'matrix_dp_expectation']:\n",
    "        method_valid = plot_data[plot_data[method] > 0]\n",
    "        if len(method_valid) > 0:\n",
    "            errors = np.abs(method_valid['empirical_dwpc'] - method_valid[method])\n",
    "            methods_data.append(errors)\n",
    "            methods_labels.append(method.replace('_expectation', '').replace('_', ' ').title())\n",
    "    \n",
    "    bp = ax.boxplot(methods_data, labels=methods_labels, patch_artist=True)\n",
    "    colors = ['lightblue', 'lightcoral', 'lightgreen']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    ax.set_ylabel('Absolute Error', fontsize=12)\n",
    "    ax.set_title('Method Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / f'{metapath_str}_validation_plots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSaved validation plots for {metapath_str}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Compositional Null (Notebook 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load compositional null results if available\n",
    "comp_null_dir = repo_dir / 'results' / 'compositional_null'\n",
    "\n",
    "if comp_null_dir.exists():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Comparison with Compositional Null Approach\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for metapath_str in test_metapaths:\n",
    "        # Check if compositional null results exist for this metapath\n",
    "        comp_file = comp_null_dir / f'{metapath_str}_null_validation.csv'\n",
    "        \n",
    "        if comp_file.exists() and metapath_str in all_results:\n",
    "            comp_df = pd.read_csv(comp_file)\n",
    "            dp_df = all_results[metapath_str]\n",
    "            \n",
    "            print(f\"\\n{metapath_str}:\")\n",
    "            \n",
    "            # Compare correlations if available\n",
    "            if 'ml_null_prob' in comp_df.columns and 'true_null_prob' in comp_df.columns:\n",
    "                comp_corr, _ = pearsonr(comp_df['true_null_prob'], comp_df['ml_null_prob'])\n",
    "                print(f\"  Compositional Null correlation: r = {comp_corr:.4f}\")\n",
    "            \n",
    "            # Get DP correlation for comparison\n",
    "            valid_dp = dp_df[(dp_df['empirical_dwpc'] > 0) & (dp_df['exact_expectation'] > 0)]\n",
    "            if len(valid_dp) > 10:\n",
    "                dp_corr, _ = pearsonr(valid_dp['empirical_dwpc'], valid_dp['exact_expectation'])\n",
    "                print(f\"  Dynamic Programming correlation: r = {dp_corr:.4f}\")\n",
    "                \n",
    "                # Performance comparison\n",
    "                print(f\"\\n  Performance Analysis:\")\n",
    "                print(f\"    Sample size - Compositional: {len(comp_df)}, DP: {len(valid_dp)}\")\n",
    "                \n",
    "                # Speed comparison would require timing data\n",
    "                print(f\"    Note: Speed comparison requires runtime measurements\")\n",
    "else:\n",
    "    print(f\"\\nCompositional null results not found at {comp_null_dir}\")\n",
    "    print(\"Run notebook 14 to generate comparison data\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DYNAMIC PROGRAMMING DWPC ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if validation_summary:\n",
    "    summary_df = pd.DataFrame(validation_summary)\n",
    "    \n",
    "    print(f\"\\nTested Metapaths: {test_metapaths}\")\n",
    "    print(f\"Validation Permutations: {validation_perm_range[0]}-{validation_perm_range[1]-1}\")\n",
    "    print(f\"Damping Exponent: {damping_exponent}\")\n",
    "    \n",
    "    print(f\"\\nMETHOD PERFORMANCE:\")\n",
    "    for method in summary_df['method'].unique():\n",
    "        method_data = summary_df[summary_df['method'] == method]\n",
    "        print(f\"\\n  {method}:\")\n",
    "        print(f\"    Mean correlation: {method_data['correlation'].mean():.4f} ± {method_data['correlation'].std():.4f}\")\n",
    "        print(f\"    Mean MAE: {method_data['mae'].mean():.6f} ± {method_data['mae'].std():.6f}\")\n",
    "        print(f\"    Mean R²: {method_data['r2'].mean():.4f} ± {method_data['r2'].std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nKEY FINDINGS:\")\n",
    "    best_method = summary_df.groupby('method')['correlation'].mean().idxmax()\n",
    "    best_corr = summary_df.groupby('method')['correlation'].mean()[best_method]\n",
    "    print(f\"  Best method: {best_method} (avg correlation: {best_corr:.4f})\")\n",
    "    \n",
    "    print(f\"\\nADVANTAGES OF DYNAMIC PROGRAMMING APPROACH:\")\n",
    "    print(f\"  • Exact mathematical expectation (no simulation required)\")\n",
    "    print(f\"  • Fast computation O(Σ|T_k|) complexity\")\n",
    "    print(f\"  • Scalable to longer metapaths\")\n",
    "    print(f\"  • Degree-grouping compatible for improved statistics\")\n",
    "    \n",
    "    print(f\"\\nLIMITATIONS:\")\n",
    "    print(f\"  • Assumes independence between metaedges\")\n",
    "    print(f\"  • May need corrections for repeated edge types\")\n",
    "    print(f\"  • Mean-field approximation less accurate for heterogeneous networks\")\n",
    "    \n",
    "    print(f\"\\nOUTPUT FILES:\")\n",
    "    for file in results_dir.glob('*.csv'):\n",
    "        print(f\"  - {file.name}\")\n",
    "    for file in results_dir.glob('*.png'):\n",
    "        print(f\"  - {file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}