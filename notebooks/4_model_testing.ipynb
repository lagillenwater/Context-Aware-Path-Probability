{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Model Comparison for Edge Probability Prediction\n\nThis notebook compares 4 different models for predicting edge probabilities based on source and target node degrees:\n\n1. **Simple Neural Network**: 2-layer NN with continuous probability output\n2. **Random Forest**: Ensemble method with 100 trees\n3. **Logistic Regression**: Standard logistic regression with L2 regularization\n4. **Polynomial Logistic Regression**: Quadratic features + logistic regression\n\n## Data\n\nWe use edge data from a single permutation: `data/permutations/000.hetmat/edges/AeG.sparse.npz`\n\n## Evaluation\n\nModels are evaluated using:\n- **Classification metrics**: AUC-ROC, Precision, Recall, F1-Score\n- **Regression metrics**: RMSE, MAE, RÂ², Correlation\n- **Empirical comparison**: Compare predictions with empirical frequencies from `results/edge_frequency_by_degree.csv`\n- **Visualizations**: ROC curves, Precision-Recall curves, probability heatmaps"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "repo_dir = Path.cwd().parent\n",
    "src_dir = repo_dir / 'src'\n",
    "data_dir = repo_dir / 'data'\n",
    "results_dir = repo_dir / 'results' / 'model_comparison'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "# Import our custom modules\n",
    "from model_comparison import ModelCollection, prepare_edge_features_and_labels, create_degree_grid\n",
    "from model_training import ModelTrainer\n",
    "from model_evaluation import ModelEvaluator, get_best_models\n",
    "from model_visualization import ModelVisualizer, create_comparison_table_plot\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"Repository directory: {repo_dir}\")\n",
    "print(f\"Results will be saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "EDGE_FILE_PATH = data_dir / 'permutations' / '000.hetmat' / 'edges' / 'AeG.sparse.npz'\n",
    "SAMPLE_RATIO = 0.1  # Ratio for negative sampling to balance dataset\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "print(f\"Loading edge data from: {EDGE_FILE_PATH}\")\n",
    "print(f\"File exists: {EDGE_FILE_PATH.exists()}\")\n",
    "\n",
    "if not EDGE_FILE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Edge data file not found: {EDGE_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "print(\"Preparing edge features and labels...\")\n",
    "features, labels = prepare_edge_features_and_labels(\n",
    "    str(EDGE_FILE_PATH), \n",
    "    sample_ratio=SAMPLE_RATIO\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total samples: {len(features)}\")\n",
    "print(f\"  Features shape: {features.shape}\")\n",
    "print(f\"  Positive examples: {np.sum(labels)} ({np.mean(labels):.1%})\")\n",
    "print(f\"  Negative examples: {len(labels) - np.sum(labels)} ({1-np.mean(labels):.1%})\")\n",
    "\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "print(f\"  Source degrees: {features[:, 0].min():.0f} - {features[:, 0].max():.0f} (mean: {features[:, 0].mean():.1f})\")\n",
    "print(f\"  Target degrees: {features[:, 1].min():.0f} - {features[:, 1].max():.0f} (mean: {features[:, 1].mean():.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Source degree distribution\n",
    "axes[0, 0].hist(features[:, 0], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Source Degree')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Source Degrees')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Target degree distribution\n",
    "axes[0, 1].hist(features[:, 1], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Target Degree')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Target Degrees')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot of degrees (colored by edge existence)\n",
    "positive_mask = labels == 1\n",
    "negative_mask = labels == 0\n",
    "\n",
    "# Sample for visualization (too many points otherwise)\n",
    "n_sample = min(10000, len(features))\n",
    "sample_idx = np.random.choice(len(features), n_sample, replace=False)\n",
    "\n",
    "axes[1, 0].scatter(features[sample_idx[negative_mask[sample_idx]], 0], \n",
    "                   features[sample_idx[negative_mask[sample_idx]], 1], \n",
    "                   alpha=0.3, s=1, label='No Edge', color='red')\n",
    "axes[1, 0].scatter(features[sample_idx[positive_mask[sample_idx]], 0], \n",
    "                   features[sample_idx[positive_mask[sample_idx]], 1], \n",
    "                   alpha=0.3, s=1, label='Edge Exists', color='blue')\n",
    "axes[1, 0].set_xlabel('Source Degree')\n",
    "axes[1, 0].set_ylabel('Target Degree')\n",
    "axes[1, 0].set_title('Degree Relationships (Sample)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Label distribution\n",
    "label_counts = pd.Series(labels).value_counts().sort_index()\n",
    "axes[1, 1].bar(['No Edge', 'Edge Exists'], label_counts.values, \n",
    "               color=['red', 'blue'], alpha=0.7)\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Edge Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, count in enumerate(label_counts.values):\n",
    "    axes[1, 1].text(i, count + len(features) * 0.01, str(count), \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Data distribution visualized and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all models\n",
    "model_collection = ModelCollection(random_state=RANDOM_STATE)\n",
    "models = model_collection.create_models()\n",
    "model_info = model_collection.get_model_info()\n",
    "\n",
    "print(\"Models created:\")\n",
    "print(\"=\" * 50)\n",
    "for name, description in model_info.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  {description}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "trainer = ModelTrainer(random_state=RANDOM_STATE)\n",
    "training_results = trainer.train_all_models(\n",
    "    models, features, labels, \n",
    "    test_size=TEST_SIZE, \n",
    "    val_size=VAL_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "evaluator = ModelEvaluator()\n",
    "X_test = training_results['data_splits']['X_test']\n",
    "y_test = training_results['data_splits']['y_test']\n",
    "\n",
    "evaluation_results = evaluator.evaluate_all_models(training_results, X_test, y_test)\n",
    "\n",
    "print(\"Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed evaluation results\n",
    "evaluator.print_detailed_results(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display comparison table\n",
    "comparison_df = evaluator.create_comparison_dataframe(evaluation_results)\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(results_dir / 'model_comparison.csv', index=False)\n",
    "print(f\"\\nComparison table saved to: {results_dir / 'model_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison table\n",
    "create_comparison_table_plot(comparison_df, save_path=results_dir / 'comparison_table.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best models for each metric\n",
    "best_models = get_best_models(evaluation_results)\n",
    "\n",
    "print(\"Best Models by Metric:\")\n",
    "print(\"=\" * 30)\n",
    "for metric, model_name in best_models.items():\n",
    "    print(f\"{metric.upper():20}: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4.5. Empirical Frequency Comparison\n\nIn this section, we compare model predictions with empirical edge frequencies from the full dataset. This provides insight into how well each model captures the true underlying edge probability distribution based on degree combinations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Reimport updated modules to get the new empirical comparison methods\nimport importlib\nimport model_evaluation\nimportlib.reload(model_evaluation)\nfrom model_evaluation import ModelEvaluator, get_best_models\n\n# Create a new evaluator instance with the updated methods\nevaluator = ModelEvaluator()\n\n# Re-run evaluation with the new evaluator instance to get fresh results\nevaluation_results = evaluator.evaluate_all_models(training_results, X_test, y_test)\n\nprint(\"Reloaded model_evaluation module with updated empirical comparison methods\")\nprint(\"Created new evaluator instance and re-ran evaluation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test Predictions vs Empirical Frequencies Comparison\nEMPIRICAL_FREQ_FILE = repo_dir / 'results' / 'edge_frequency_by_degree.csv'\n\nprint(f\"Empirical frequency file: {EMPIRICAL_FREQ_FILE}\")\nprint(f\"File exists: {EMPIRICAL_FREQ_FILE.exists()}\")\n\nif EMPIRICAL_FREQ_FILE.exists():\n    # Compare test predictions with empirical frequencies for the same degree combinations\n    test_empirical_comparison = evaluator.compare_test_predictions_with_empirical(\n        evaluation_results, training_results, X_test, str(EMPIRICAL_FREQ_FILE)\n    )\n    \n    # Print summary\n    evaluator.print_test_empirical_comparison_summary(test_empirical_comparison)\n    \n    # Create comparison dataframe\n    test_empirical_df = evaluator.create_test_empirical_comparison_dataframe(test_empirical_comparison)\n    print(\"\\nTest Predictions vs Empirical Frequencies Comparison:\")\n    print(\"=\" * 80)\n    print(test_empirical_df.to_string(index=False, float_format='%.6f'))\n    \n    # Save comparison results\n    test_empirical_df.to_csv(results_dir / 'test_vs_empirical_comparison.csv', index=False)\n    print(f\"\\nTest vs empirical comparison saved to: {results_dir / 'test_vs_empirical_comparison.csv'}\")\n    \n    # Create scatter plot comparing predictions vs empirical frequencies\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    axes = axes.flatten()\n    \n    for i, (model_name, results) in enumerate(test_empirical_comparison.items()):\n        if i >= 4:  # Only plot first 4 models\n            break\n            \n        predictions = results['matched_predictions']\n        empirical = results['matched_empirical']\n        correlation = results['correlation_vs_empirical']\n        \n        axes[i].scatter(empirical, predictions, alpha=0.6, s=20)\n        axes[i].plot([0, 1], [0, 1], 'r--', alpha=0.8)  # Perfect correlation line\n        axes[i].set_xlabel('Empirical Frequency')\n        axes[i].set_ylabel('Predicted Probability')\n        axes[i].set_title(f'{model_name}\\nr = {correlation:.4f}')\n        axes[i].grid(True, alpha=0.3)\n        axes[i].set_xlim(0, 1)\n        axes[i].set_ylim(0, 1)\n    \n    plt.tight_layout()\n    plt.savefig(results_dir / 'test_vs_empirical_scatter.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \nelse:\n    print(\"Empirical frequency file not found. Skipping empirical comparison.\")\n    print(\"Run the edge frequency analysis first to generate the empirical frequencies.\")\n    print(\"This should create the file: results/edge_frequency_by_degree.csv\")\n    test_empirical_comparison = None\n    test_empirical_df = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizer\n",
    "visualizer = ModelVisualizer()\n",
    "\n",
    "# Plot ROC curves\n",
    "print(\"Creating ROC curves...\")\n",
    "visualizer.plot_roc_curves(evaluation_results, save_path=results_dir / 'roc_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "print(\"Creating Precision-Recall curves...\")\n",
    "visualizer.plot_precision_recall_curves(evaluation_results, save_path=results_dir / 'precision_recall_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance comparison\n",
    "print(\"Creating performance comparison...\")\n",
    "visualizer.plot_performance_comparison(evaluation_results, save_path=results_dir / 'performance_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history (for neural networks)\n",
    "print(\"Creating training history plot...\")\n",
    "visualizer.plot_training_history(training_results, save_path=results_dir / 'training_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Edge Probability Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create degree grids for heatmap visualization\n",
    "source_degrees = features[:, 0]\n",
    "target_degrees = features[:, 1]\n",
    "\n",
    "source_bins, target_bins, grid_features = create_degree_grid(\n",
    "    source_degrees, target_degrees, n_bins=50\n",
    ")\n",
    "\n",
    "print(f\"Created degree grid for visualization:\")\n",
    "print(f\"  Source degree range: {source_bins.min():.0f} - {source_bins.max():.0f}\")\n",
    "print(f\"  Target degree range: {target_bins.min():.0f} - {target_bins.max():.0f}\")\n",
    "print(f\"  Grid size: {len(source_bins)} x {len(target_bins)} = {len(grid_features)} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual heatmaps for each model\n",
    "print(\"Creating individual prediction heatmaps...\")\n",
    "visualizer.create_all_prediction_heatmaps(\n",
    "    training_results, source_bins, target_bins, \n",
    "    save_dir=str(results_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined heatmap grid\n",
    "print(\"Creating combined heatmap grid...\")\n",
    "visualizer.create_combined_heatmap_grid(\n",
    "    training_results, source_bins, target_bins,\n",
    "    figsize=(20, 5), save_path=results_dir / 'combined_heatmaps.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary analysis\nprint(\"=\" * 80)\nprint(\"MODEL COMPARISON SUMMARY\")\nprint(\"=\" * 80)\n\nprint(f\"\\nDataset:\")\nprint(f\"  Edge file: {EDGE_FILE_PATH.name}\")\nprint(f\"  Total samples: {len(features):,}\")\nprint(f\"  Test samples: {len(X_test):,}\")\nprint(f\"  Positive ratio: {np.mean(labels):.1%}\")\n\nprint(f\"\\nBest Performing Models:\")\nbest_auc_model = comparison_df.iloc[0]['Model']\nbest_auc_score = comparison_df.iloc[0]['AUC']\nprint(f\"  Overall Best (AUC): {best_auc_model} (AUC = {best_auc_score:.4f})\")\n\nfor metric in ['Accuracy', 'F1 Score', 'RMSE', 'Correlation']:\n    if metric in ['RMSE']:\n        best_idx = comparison_df[metric].idxmin()\n        best_value = comparison_df[metric].min()\n        direction = \"(lower is better)\"\n    else:\n        best_idx = comparison_df[metric].idxmax()\n        best_value = comparison_df[metric].max()\n        direction = \"(higher is better)\"\n    \n    best_model = comparison_df.loc[best_idx, 'Model']\n    print(f\"  Best {metric}: {best_model} ({metric} = {best_value:.4f}) {direction}\")\n\n# Add test vs empirical comparison results if available\nif 'test_empirical_comparison' in locals() and test_empirical_comparison is not None:\n    print(f\"\\nTest Predictions vs Empirical Frequencies:\")\n    best_empirical_corr_model = test_empirical_df.iloc[0]['Model']\n    best_empirical_corr = test_empirical_df.iloc[0]['Correlation vs Empirical']\n    print(f\"  Best Correlation with Empirical: {best_empirical_corr_model} (r = {best_empirical_corr:.6f})\")\n    \n    best_empirical_mae_idx = test_empirical_df['MAE vs Empirical'].idxmin()\n    best_empirical_mae_model = test_empirical_df.loc[best_empirical_mae_idx, 'Model']\n    best_empirical_mae = test_empirical_df.loc[best_empirical_mae_idx, 'MAE vs Empirical']\n    print(f\"  Best MAE vs Empirical: {best_empirical_mae_model} (MAE = {best_empirical_mae:.6f})\")\n    \n    # Report matching statistics\n    avg_match_ratio = test_empirical_df['Match Ratio'].mean()\n    print(f\"  Average match ratio: {avg_match_ratio:.1%} of test samples matched with empirical data\")\n\nprint(f\"\\nKey Insights:\")\n\n# Performance analysis\nauc_scores = comparison_df['AUC'].values\nif np.max(auc_scores) - np.min(auc_scores) < 0.05:\n    print(f\"  - All models show similar AUC performance (range: {np.min(auc_scores):.3f} - {np.max(auc_scores):.3f})\")\nelse:\n    print(f\"  - Significant performance differences observed (AUC range: {np.min(auc_scores):.3f} - {np.max(auc_scores):.3f})\")\n\n# Model complexity vs performance\nnn_auc = comparison_df[comparison_df['Model'] == 'Simple NN']['AUC'].values[0]\nlogistic_auc = comparison_df[comparison_df['Model'] == 'Logistic Regression']['AUC'].values[0]\n\nif nn_auc - logistic_auc > 0.05:\n    print(f\"  - Neural Network shows substantial improvement over Logistic Regression\")\nelif abs(nn_auc - logistic_auc) < 0.02:\n    print(f\"  - Neural Network and Logistic Regression show similar performance\")\nelse:\n    print(f\"  - Neural Network shows modest improvement over Logistic Regression\")\n\n# Correlation analysis\ncorrelations = comparison_df['Correlation'].values\nbest_corr = np.max(correlations)\nif best_corr > 0.8:\n    print(f\"  - Strong correlation between predictions and true probabilities (max: {best_corr:.3f})\")\nelif best_corr > 0.5:\n    print(f\"  - Moderate correlation between predictions and true probabilities (max: {best_corr:.3f})\")\nelse:\n    print(f\"  - Weak correlation between predictions and true probabilities (max: {best_corr:.3f})\")\n\n# Test vs empirical comparison insights\nif 'test_empirical_comparison' in locals() and test_empirical_comparison is not None:\n    empirical_correlations = test_empirical_df['Correlation vs Empirical'].values\n    best_empirical_corr_val = np.max(empirical_correlations)\n    if best_empirical_corr_val > 0.8:\n        print(f\"  - Strong correlation between test predictions and empirical frequencies (max: {best_empirical_corr_val:.3f})\")\n    elif best_empirical_corr_val > 0.5:\n        print(f\"  - Moderate correlation between test predictions and empirical frequencies (max: {best_empirical_corr_val:.3f})\")\n    else:\n        print(f\"  - Weak correlation between test predictions and empirical frequencies (max: {best_empirical_corr_val:.3f})\")\n        \n    print(f\"  - Gap between test performance and empirical accuracy reveals generalization challenges\")\n\nprint(f\"\\nFiles Generated:\")\ngenerated_files = list(results_dir.glob('*'))\nfor file_path in sorted(generated_files):\n    print(f\"  - {file_path.name}\")\n\nprint(f\"\\nAll results saved to: {results_dir}\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendation\n",
    "print(\"\\nRECOMMENDATION:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "best_overall = comparison_df.iloc[0]\n",
    "print(f\"For edge probability prediction on this dataset, the {best_overall['Model']} \"\n",
    "      f\"performs best overall with:\")\n",
    "print(f\"  - AUC: {best_overall['AUC']:.4f}\")\n",
    "print(f\"  - Accuracy: {best_overall['Accuracy']:.4f}\")\n",
    "print(f\"  - F1 Score: {best_overall['F1 Score']:.4f}\")\n",
    "print(f\"  - RMSE: {best_overall['RMSE']:.4f}\")\n",
    "\n",
    "# Training time consideration\n",
    "print(f\"\\nTraining time considerations:\")\n",
    "for model_name, result in training_results.items():\n",
    "    if model_name != 'data_splits':\n",
    "        training_time = result['training_result']['training_time']\n",
    "        print(f\"  - {model_name}: {training_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nFor production use, consider the trade-off between model performance and training time.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}